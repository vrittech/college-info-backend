{
    "sourceFile": "resultmanagement/viewsets/file_viewsets.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 68,
            "patches": [
                {
                    "date": 1749100306060,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1749100311743,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,12 +6,12 @@\n from ..utilities.importbase import *\n \n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n-    # permission_classes = [resultmanagementPermission]\n+    permission_classes = [resultmanagementPermission]\n     # authentication_classes = [JWTAuthentication]\n-    #pagination_class = MyPageNumberPagination\n-    queryset = File.objects.all().order_by()\n+    pagination_class = MyPageNumberPagination\n+    queryset = File.objects.all().order_by('-id')\n \n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id']\n     ordering_fields = ['id']\n"
                },
                {
                    "date": 1749100335172,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,11 +15,11 @@\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id']\n     ordering_fields = ['id']\n \n-    # filterset_fields = {\n-    #     'id': ['exact'],\n-    # }\n+    filterset_fields = {\n+        'id': ['exact'],\n+    }\n \n     def get_queryset(self):\n         queryset = super().get_queryset()\n         #return queryset.filter(user_id=self.request.user.id)\n"
                },
                {
                    "date": 1749100343421,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,8 +17,9 @@\n     ordering_fields = ['id']\n \n     filterset_fields = {\n         'id': ['exact'],\n+        'is_active': ['exact'],\n     }\n \n     def get_queryset(self):\n         queryset = super().get_queryset()\n"
                },
                {
                    "date": 1749100353155,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,8 +18,9 @@\n \n     filterset_fields = {\n         'id': ['exact'],\n         'is_active': ['exact'],\n+        \n     }\n \n     def get_queryset(self):\n         queryset = super().get_queryset()\n"
                },
                {
                    "date": 1749100361517,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,9 +18,9 @@\n \n     filterset_fields = {\n         'id': ['exact'],\n         'is_active': ['exact'],\n-        \n+        'created_date': ['exact', 'gte', 'lte'],\n     }\n \n     def get_queryset(self):\n         queryset = super().get_queryset()\n"
                },
                {
                    "date": 1749100368459,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,13 +19,14 @@\n     filterset_fields = {\n         'id': ['exact'],\n         'is_active': ['exact'],\n         'created_date': ['exact', 'gte', 'lte'],\n+        'updated_date': ['exact', 'gte', 'lte'],\n     }\n \n     def get_queryset(self):\n         queryset = super().get_queryset()\n-        #return queryset.filter(user_id=self.request.user.id)\n+        return queryset\n \n     def get_serializer_class(self):\n         if self.action in ['create', 'update', 'partial_update']:\n             return FileWriteSerializers\n"
                },
                {
                    "date": 1749100385264,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -36,5 +36,7 @@\n \n     # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n     # def action_name(self, request, *args, **kwargs):\n     #     return super().list(request, *args, **kwargs)\n+    \n+    \n \n"
                },
                {
                    "date": 1749100390430,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,6 +37,6 @@\n     # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n     # def action_name(self, request, *args, **kwargs):\n     #     return super().list(request, *args, **kwargs)\n     \n-    \n+    def create\n \n"
                },
                {
                    "date": 1749100428408,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,6 +37,179 @@\n     # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n     # def action_name(self, request, *args, **kwargs):\n     #     return super().list(request, *args, **kwargs)\n     \n-    def create\n+    def create(self, request, *args, **kwargs):\n+        uploaded_file = request.FILES.get('file')\n+        is_file_organised = request.data.get('is_file_organised', 'true').lower() == 'true'\n+        email_column = None\n \n+        if not uploaded_file:\n+            return Response({\"error\": \"No file uploaded\"}, status=400)\n+\n+        file_name = uploaded_file.name.lower()\n+        file_type = 'csv' if file_name.endswith('.csv') else 'xlsx' if file_name.endswith(('.xlsx', '.xls')) else None\n+        if not file_type:\n+            return Response({\"error\": \"Unsupported file format\"}, status=400)\n+\n+        temp_file_path = None\n+        try:\n+            with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_type}\") as temp_file:\n+                for chunk in uploaded_file.chunks():\n+                    temp_file.write(chunk)\n+                temp_file_path = temp_file.name\n+\n+            if file_type == 'csv':\n+                with open(temp_file_path, 'rb') as f:\n+                    raw = f.read(500_000)\n+                encoding = chardet.detect(raw)['encoding'] or 'utf-8'\n+                df_sample = pd.read_csv(temp_file_path, encoding=encoding, dtype=str, keep_default_na=False)\n+            else:\n+                df_sample = pd.read_excel(temp_file_path, dtype=str, keep_default_na=False)\n+\n+            original_columns = list(df_sample.columns)\n+\n+            # ------------------ UNORGANIZED EMAIL FILE HANDLING ------------------\n+            if not is_file_organised:\n+                result = extract_emails_unorganized_optimized(temp_file_path, file_type=file_type)\n+                emails = result['emails']\n+                stats = {\n+                    'total_processed': result.get('total_processed', len(emails)),\n+                    'total_valid': result.get('total_valid', len(emails)),\n+                    'unique_emails': result.get('unique_emails', len(emails)),\n+                    'duplicates': result.get('duplicates', 0),\n+                    'dropped_emails': result.get('dropped_emails', 0),\n+                    'email_domains': result.get('domains', {})\n+                }\n+\n+                if not emails or (stats.get('unique_emails', 0) == 0 and stats.get('duplicates', 0) == 0):\n+                    return Response({\n+                        \"error\": \"No email addresses found\",\n+                        \"is_first_time\": True,\n+                        \"available_columns\": original_columns,\n+                        \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n+                    }, status=200)\n+\n+                csv_buffer = io.StringIO()\n+                writer = csv.writer(csv_buffer)\n+                writer.writerow(['email'])\n+                for email in sorted(emails):\n+                    writer.writerow([email])\n+                collected_emails_file = ContentFile(csv_buffer.getvalue().encode('utf-8'), name=\"collected_emails.csv\")\n+\n+                # Save uploaded file record including collected_emails_file\n+                with transaction.atomic():\n+                    file_upload = FilesUpload.objects.create(\n+                        file=uploaded_file,\n+                        name=request.data.get('name', uploaded_file.name),\n+                        file_name=uploaded_file.name,\n+                        file_size=str(uploaded_file.size),\n+                        file_tag=request.data.get('file_tag', 'validate'),\n+                        workspace=getattr(request.user, 'active_workspace', None),\n+                        total_email=stats['total_valid'],\n+                        billable_email=stats['unique_emails'],\n+                        duplicate_email=stats.get('duplicates', 0),\n+                        dropped_email=stats.get('dropped_emails', 0),\n+                        clean_email_count=stats['unique_emails'],\n+                        total_processed=stats.get('total_processed', len(emails)),\n+                        email_column=\"email\",\n+                        user=request.user if request.user.is_authenticated else None,\n+                        is_file_organised=False,\n+                        collected_emails_file=collected_emails_file\n+                    )\n+\n+                return Response({\n+                    \"id\": file_upload.id,\n+                    \"name\": file_upload.name,\n+                    \"file\": request.build_absolute_uri(file_upload.file.url),\n+                    \"collected_emails_file\": request.build_absolute_uri(file_upload.collected_emails_file.url),\n+                    \"status\": file_upload.status,\n+                    \"file_tag\": file_upload.file_tag,\n+                    \"file_name\": file_upload.file_name,\n+                    \"file_size\": file_upload.file_size,\n+                    \"stats\": {\n+                        \"total_processed\": stats.get('total_processed', len(emails)),\n+                        \"total_emails\": stats.get('total_valid', len(emails)),\n+                        \"unique_emails\": stats.get('unique_emails', 0),\n+                        \"duplicate_emails\": stats.get('duplicates', 0),\n+                        \"dropped_emails\": stats.get('dropped_emails', 0),\n+                        \"email_columns_used\": list(result.get('column_wise_email_counts', {}).keys()),\n+                        \"email_domains\": stats.get('email_domains', {})\n+                    },\n+                    \"clean_emails_available\": stats.get('unique_emails', 0) > 0,\n+                    \"workspace\": file_upload.workspace.name if file_upload.workspace else None\n+                }, status=201)\n+\n+            # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n+            def normalize(name):\n+                return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n+\n+            normalized_map = {normalize(c): c for c in original_columns}\n+            email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n+            if not email_column:\n+                for c in original_columns:\n+                    if normalize(c) == 'email':\n+                        email_column = c\n+                        break\n+            if not email_column:\n+                return Response({\n+                    \"error\": \"No email columns found\",\n+                    \"is_first_time\": True,\n+                    \"available_columns\": original_columns,\n+                    \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n+                }, status=200)\n+\n+            stats = process_organized_email_file(temp_file_path, email_column, file_type)\n+            total_email = stats.get('unique_emails', 0) + stats.get('duplicates', 0)\n+\n+            if stats.get('unique_emails', 0) == 0 and stats.get('duplicates', 0) == 0:\n+                return Response({\n+                    \"error\": \"No email columns found\",\n+                    \"is_first_time\": False,\n+                    \"available_columns\": original_columns,\n+                    \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n+                }, status=200)\n+\n+            with transaction.atomic():\n+                file_upload = FilesUpload.objects.create(\n+                    file=uploaded_file,\n+                    name=request.data.get('name', uploaded_file.name),\n+                    file_name=uploaded_file.name,\n+                    file_size=str(uploaded_file.size),\n+                    file_tag=request.data.get('file_tag', 'validate'),\n+                    workspace=getattr(request.user, 'active_workspace', None),\n+                    user=request.user if request.user.is_authenticated else None,\n+                    is_file_organised=True,\n+                    email_column=email_column,\n+                    total_processed=stats['total_processed'],\n+                    total_email=total_email,\n+                    billable_email=stats['unique_emails'],\n+                    duplicate_email=stats['duplicates'],\n+                    dropped_email=stats['dropped_emails'],\n+                    clean_email_count=stats['unique_emails'],\n+                )\n+\n+            return Response({\n+                \"id\": file_upload.id,\n+                \"name\": file_upload.name,\n+                \"file\": request.build_absolute_uri(file_upload.file.url),\n+                \"status\": file_upload.status,\n+                \"file_tag\": file_upload.file_tag,\n+                \"file_name\": file_upload.file_name,\n+                \"file_size\": file_upload.file_size,\n+                \"stats\": {\n+                    \"total_processed\": stats['total_processed'],\n+                    \"total_emails\": total_email,\n+                    \"unique_emails\": stats['unique_emails'],\n+                    \"duplicate_emails\": stats['duplicates'],\n+                    \"dropped_emails\": stats['dropped_emails'],\n+                    \"email_columns_used\": [email_column],\n+                    \"email_domains\": stats['email_domains']\n+                },\n+                \"clean_emails_available\": stats.get('unique_emails', 0) > 0,\n+                \"workspace\": file_upload.workspace.name if file_upload.workspace else None\n+            }, status=201)\n+\n+        finally:\n+            if temp_file_path and os.path.exists(temp_file_path):\n+                os.unlink(temp_file_path)\n+\n"
                },
                {
                    "date": 1749100439451,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -187,28 +187,28 @@\n                     dropped_email=stats['dropped_emails'],\n                     clean_email_count=stats['unique_emails'],\n                 )\n \n-            return Response({\n-                \"id\": file_upload.id,\n-                \"name\": file_upload.name,\n-                \"file\": request.build_absolute_uri(file_upload.file.url),\n-                \"status\": file_upload.status,\n-                \"file_tag\": file_upload.file_tag,\n-                \"file_name\": file_upload.file_name,\n-                \"file_size\": file_upload.file_size,\n-                \"stats\": {\n-                    \"total_processed\": stats['total_processed'],\n-                    \"total_emails\": total_email,\n-                    \"unique_emails\": stats['unique_emails'],\n-                    \"duplicate_emails\": stats['duplicates'],\n-                    \"dropped_emails\": stats['dropped_emails'],\n-                    \"email_columns_used\": [email_column],\n-                    \"email_domains\": stats['email_domains']\n-                },\n-                \"clean_emails_available\": stats.get('unique_emails', 0) > 0,\n-                \"workspace\": file_upload.workspace.name if file_upload.workspace else None\n-            }, status=201)\n+            # return Response({\n+            #     \"id\": file_upload.id,\n+            #     \"name\": file_upload.name,\n+            #     \"file\": request.build_absolute_uri(file_upload.file.url),\n+            #     \"status\": file_upload.status,\n+            #     \"file_tag\": file_upload.file_tag,\n+            #     \"file_name\": file_upload.file_name,\n+            #     \"file_size\": file_upload.file_size,\n+            #     \"stats\": {\n+            #         \"total_processed\": stats['total_processed'],\n+            #         \"total_emails\": total_email,\n+            #         \"unique_emails\": stats['unique_emails'],\n+            #         \"duplicate_emails\": stats['duplicates'],\n+            #         \"dropped_emails\": stats['dropped_emails'],\n+            #         \"email_columns_used\": [email_column],\n+            #         \"email_domains\": stats['email_domains']\n+            #     },\n+            #     \"clean_emails_available\": stats.get('unique_emails', 0) > 0,\n+            #     \"workspace\": file_upload.workspace.name if file_upload.workspace else None\n+            # }, status=201)\n \n         finally:\n             if temp_file_path and os.path.exists(temp_file_path):\n                 os.unlink(temp_file_path)\n"
                },
                {
                    "date": 1749100456750,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,9 +39,8 @@\n     #     return super().list(request, *args, **kwargs)\n     \n     def create(self, request, *args, **kwargs):\n         uploaded_file = request.FILES.get('file')\n-        is_file_organised = request.data.get('is_file_organised', 'true').lower() == 'true'\n         email_column = None\n \n         if not uploaded_file:\n             return Response({\"error\": \"No file uploaded\"}, status=400)\n@@ -208,8 +207,8 @@\n             #     \"clean_emails_available\": stats.get('unique_emails', 0) > 0,\n             #     \"workspace\": file_upload.workspace.name if file_upload.workspace else None\n             # }, status=201)\n \n-        finally:\n-            if temp_file_path and os.path.exists(temp_file_path):\n-                os.unlink(temp_file_path)\n+        # finally:\n+        #     if temp_file_path and os.path.exists(temp_file_path):\n+        #         os.unlink(temp_file_path)\n \n"
                },
                {
                    "date": 1749100508715,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,11 @@\n from django_filters.rest_framework import DjangoFilterBackend\n from ..models import File\n from ..serializers.file_serializers import FileListSerializers, FileRetrieveSerializers, FileWriteSerializers\n from ..utilities.importbase import *\n+from rest_framework.response import Response\n \n+\n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n     permission_classes = [resultmanagementPermission]\n     # authentication_classes = [JWTAuthentication]\n@@ -39,9 +41,9 @@\n     #     return super().list(request, *args, **kwargs)\n     \n     def create(self, request, *args, **kwargs):\n         uploaded_file = request.FILES.get('file')\n-        email_column = None\n+        \n \n         if not uploaded_file:\n             return Response({\"error\": \"No file uploaded\"}, status=400)\n \n"
                },
                {
                    "date": 1749100532527,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,9 +51,9 @@\n         file_type = 'csv' if file_name.endswith('.csv') else 'xlsx' if file_name.endswith(('.xlsx', '.xls')) else None\n         if not file_type:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n \n-        temp_file_path = None\n+        \n         try:\n             with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_type}\") as temp_file:\n                 for chunk in uploaded_file.chunks():\n                     temp_file.write(chunk)\n"
                },
                {
                    "date": 1749100586297,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,96 +51,8 @@\n         file_type = 'csv' if file_name.endswith('.csv') else 'xlsx' if file_name.endswith(('.xlsx', '.xls')) else None\n         if not file_type:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n \n-        \n-        try:\n-            with tempfile.NamedTemporaryFile(delete=False, suffix=f\".{file_type}\") as temp_file:\n-                for chunk in uploaded_file.chunks():\n-                    temp_file.write(chunk)\n-                temp_file_path = temp_file.name\n-\n-            if file_type == 'csv':\n-                with open(temp_file_path, 'rb') as f:\n-                    raw = f.read(500_000)\n-                encoding = chardet.detect(raw)['encoding'] or 'utf-8'\n-                df_sample = pd.read_csv(temp_file_path, encoding=encoding, dtype=str, keep_default_na=False)\n-            else:\n-                df_sample = pd.read_excel(temp_file_path, dtype=str, keep_default_na=False)\n-\n-            original_columns = list(df_sample.columns)\n-\n-            # ------------------ UNORGANIZED EMAIL FILE HANDLING ------------------\n-            if not is_file_organised:\n-                result = extract_emails_unorganized_optimized(temp_file_path, file_type=file_type)\n-                emails = result['emails']\n-                stats = {\n-                    'total_processed': result.get('total_processed', len(emails)),\n-                    'total_valid': result.get('total_valid', len(emails)),\n-                    'unique_emails': result.get('unique_emails', len(emails)),\n-                    'duplicates': result.get('duplicates', 0),\n-                    'dropped_emails': result.get('dropped_emails', 0),\n-                    'email_domains': result.get('domains', {})\n-                }\n-\n-                if not emails or (stats.get('unique_emails', 0) == 0 and stats.get('duplicates', 0) == 0):\n-                    return Response({\n-                        \"error\": \"No email addresses found\",\n-                        \"is_first_time\": True,\n-                        \"available_columns\": original_columns,\n-                        \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n-                    }, status=200)\n-\n-                csv_buffer = io.StringIO()\n-                writer = csv.writer(csv_buffer)\n-                writer.writerow(['email'])\n-                for email in sorted(emails):\n-                    writer.writerow([email])\n-                collected_emails_file = ContentFile(csv_buffer.getvalue().encode('utf-8'), name=\"collected_emails.csv\")\n-\n-                # Save uploaded file record including collected_emails_file\n-                with transaction.atomic():\n-                    file_upload = FilesUpload.objects.create(\n-                        file=uploaded_file,\n-                        name=request.data.get('name', uploaded_file.name),\n-                        file_name=uploaded_file.name,\n-                        file_size=str(uploaded_file.size),\n-                        file_tag=request.data.get('file_tag', 'validate'),\n-                        workspace=getattr(request.user, 'active_workspace', None),\n-                        total_email=stats['total_valid'],\n-                        billable_email=stats['unique_emails'],\n-                        duplicate_email=stats.get('duplicates', 0),\n-                        dropped_email=stats.get('dropped_emails', 0),\n-                        clean_email_count=stats['unique_emails'],\n-                        total_processed=stats.get('total_processed', len(emails)),\n-                        email_column=\"email\",\n-                        user=request.user if request.user.is_authenticated else None,\n-                        is_file_organised=False,\n-                        collected_emails_file=collected_emails_file\n-                    )\n-\n-                return Response({\n-                    \"id\": file_upload.id,\n-                    \"name\": file_upload.name,\n-                    \"file\": request.build_absolute_uri(file_upload.file.url),\n-                    \"collected_emails_file\": request.build_absolute_uri(file_upload.collected_emails_file.url),\n-                    \"status\": file_upload.status,\n-                    \"file_tag\": file_upload.file_tag,\n-                    \"file_name\": file_upload.file_name,\n-                    \"file_size\": file_upload.file_size,\n-                    \"stats\": {\n-                        \"total_processed\": stats.get('total_processed', len(emails)),\n-                        \"total_emails\": stats.get('total_valid', len(emails)),\n-                        \"unique_emails\": stats.get('unique_emails', 0),\n-                        \"duplicate_emails\": stats.get('duplicates', 0),\n-                        \"dropped_emails\": stats.get('dropped_emails', 0),\n-                        \"email_columns_used\": list(result.get('column_wise_email_counts', {}).keys()),\n-                        \"email_domains\": stats.get('email_domains', {})\n-                    },\n-                    \"clean_emails_available\": stats.get('unique_emails', 0) > 0,\n-                    \"workspace\": file_upload.workspace.name if file_upload.workspace else None\n-                }, status=201)\n-\n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n             def normalize(name):\n                 return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n \n"
                },
                {
                    "date": 1749100597116,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,11 +51,9 @@\n         file_type = 'csv' if file_name.endswith('.csv') else 'xlsx' if file_name.endswith(('.xlsx', '.xls')) else None\n         if not file_type:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n \n-            # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n-            def normalize(name):\n-                return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n+    \n \n             normalized_map = {normalize(c): c for c in original_columns}\n             email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n             if not email_column:\n"
                },
                {
                    "date": 1749100609580,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,9 +53,8 @@\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n \n     \n \n-            normalized_map = {normalize(c): c for c in original_columns}\n             email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n             if not email_column:\n                 for c in original_columns:\n                     if normalize(c) == 'email':\n"
                },
                {
                    "date": 1749100631516,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,75 +51,72 @@\n         file_type = 'csv' if file_name.endswith('.csv') else 'xlsx' if file_name.endswith(('.xlsx', '.xls')) else None\n         if not file_type:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n \n-    \n+            # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n+        def normalize(name):\n+            return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n+        normalized_map = {normalize(c): c for c in original_columns}\n+        email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n+        if not email_column:\n+            for c in original_columns:\n+                if normalize(c) == 'email':\n+                    email_column = c\n+                    break\n+        if not email_column:\n+            return Response({\n+                \"error\": \"No email columns found\",\n+                \"is_first_time\": True,\n+                \"available_columns\": original_columns,\n+                \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n+            }, status=200)\n+        stats = process_organized_email_file(temp_file_path, email_column, file_type)\n+        total_email = stats.get('unique_emails', 0) + stats.get('duplicates', 0)\n+        if stats.get('unique_emails', 0) == 0 and stats.get('duplicates', 0) == 0:\n+            return Response({\n+                \"error\": \"No email columns found\",\n+                \"is_first_time\": False,\n+                \"available_columns\": original_columns,\n+                \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n+            }, status=200)\n+        with transaction.atomic():\n+            file_upload = FilesUpload.objects.create(\n+                file=uploaded_file,\n+                name=request.data.get('name', uploaded_file.name),\n+                file_name=uploaded_file.name,\n+                file_size=str(uploaded_file.size),\n+                file_tag=request.data.get('file_tag', 'validate'),\n+                workspace=getattr(request.user, 'active_workspace', None),\n+                user=request.user if request.user.is_authenticated else None,\n+                is_file_organised=True,\n+                email_column=email_column,\n+                total_processed=stats['total_processed'],\n+                total_email=total_email,\n+                billable_email=stats['unique_emails'],\n+                duplicate_email=stats['duplicates'],\n+                dropped_email=stats['dropped_emails'],\n+                clean_email_count=stats['unique_emails'],\n+            )\n+        # return Response({\n+        #     \"id\": file_upload.id,\n+        #     \"name\": file_upload.name,\n+        #     \"file\": request.build_absolute_uri(file_upload.file.url),\n+        #     \"status\": file_upload.status,\n+        #     \"file_tag\": file_upload.file_tag,\n+        #     \"file_name\": file_upload.file_name,\n+        #     \"file_size\": file_upload.file_size,\n+        #     \"stats\": {\n+        #         \"total_processed\": stats['total_processed'],\n+        #         \"total_emails\": total_email,\n+        #         \"unique_emails\": stats['unique_emails'],\n+        #         \"duplicate_emails\": stats['duplicates'],\n+        #         \"dropped_emails\": stats['dropped_emails'],\n+        #         \"email_columns_used\": [email_column],\n+        #         \"email_domains\": stats['email_domains']\n+        #     },\n+        #     \"clean_emails_available\": stats.get('unique_emails', 0) > 0,\n+        #     \"workspace\": file_upload.workspace.name if file_upload.workspace else None\n+        # }, status=201)\n+    # finally:\n+    #     if temp_file_path and os.path.exists(temp_file_path):\n+    #         os.unlink(temp_file_path)\n \n-            email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n-            if not email_column:\n-                for c in original_columns:\n-                    if normalize(c) == 'email':\n-                        email_column = c\n-                        break\n-            if not email_column:\n-                return Response({\n-                    \"error\": \"No email columns found\",\n-                    \"is_first_time\": True,\n-                    \"available_columns\": original_columns,\n-                    \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n-                }, status=200)\n-\n-            stats = process_organized_email_file(temp_file_path, email_column, file_type)\n-            total_email = stats.get('unique_emails', 0) + stats.get('duplicates', 0)\n-\n-            if stats.get('unique_emails', 0) == 0 and stats.get('duplicates', 0) == 0:\n-                return Response({\n-                    \"error\": \"No email columns found\",\n-                    \"is_first_time\": False,\n-                    \"available_columns\": original_columns,\n-                    \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n-                }, status=200)\n-\n-            with transaction.atomic():\n-                file_upload = FilesUpload.objects.create(\n-                    file=uploaded_file,\n-                    name=request.data.get('name', uploaded_file.name),\n-                    file_name=uploaded_file.name,\n-                    file_size=str(uploaded_file.size),\n-                    file_tag=request.data.get('file_tag', 'validate'),\n-                    workspace=getattr(request.user, 'active_workspace', None),\n-                    user=request.user if request.user.is_authenticated else None,\n-                    is_file_organised=True,\n-                    email_column=email_column,\n-                    total_processed=stats['total_processed'],\n-                    total_email=total_email,\n-                    billable_email=stats['unique_emails'],\n-                    duplicate_email=stats['duplicates'],\n-                    dropped_email=stats['dropped_emails'],\n-                    clean_email_count=stats['unique_emails'],\n-                )\n-\n-            # return Response({\n-            #     \"id\": file_upload.id,\n-            #     \"name\": file_upload.name,\n-            #     \"file\": request.build_absolute_uri(file_upload.file.url),\n-            #     \"status\": file_upload.status,\n-            #     \"file_tag\": file_upload.file_tag,\n-            #     \"file_name\": file_upload.file_name,\n-            #     \"file_size\": file_upload.file_size,\n-            #     \"stats\": {\n-            #         \"total_processed\": stats['total_processed'],\n-            #         \"total_emails\": total_email,\n-            #         \"unique_emails\": stats['unique_emails'],\n-            #         \"duplicate_emails\": stats['duplicates'],\n-            #         \"dropped_emails\": stats['dropped_emails'],\n-            #         \"email_columns_used\": [email_column],\n-            #         \"email_domains\": stats['email_domains']\n-            #     },\n-            #     \"clean_emails_available\": stats.get('unique_emails', 0) > 0,\n-            #     \"workspace\": file_upload.workspace.name if file_upload.workspace else None\n-            # }, status=201)\n-\n-        # finally:\n-        #     if temp_file_path and os.path.exists(temp_file_path):\n-        #         os.unlink(temp_file_path)\n-\n"
                },
                {
                    "date": 1749100645226,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,8 +54,9 @@\n \n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n+        \n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n             for c in original_columns:\n"
                },
                {
                    "date": 1749100651970,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,9 +54,9 @@\n \n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n-        \n+        original_\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n             for c in original_columns:\n"
                },
                {
                    "date": 1749100657801,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,9 +54,10 @@\n \n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n-        original_\n+        original_columns = []\n+        \n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n             for c in original_columns:\n"
                },
                {
                    "date": 1749100664676,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -55,9 +55,9 @@\n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n-        \n+        original_columns = pa\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n             for c in original_columns:\n"
                },
                {
                    "date": 1749100676127,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,10 +4,10 @@\n from ..models import File\n from ..serializers.file_serializers import FileListSerializers, FileRetrieveSerializers, FileWriteSerializers\n from ..utilities.importbase import *\n from rest_framework.response import Response\n+import \n \n-\n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n     permission_classes = [resultmanagementPermission]\n     # authentication_classes = [JWTAuthentication]\n@@ -55,9 +55,9 @@\n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n-        original_columns = pa\n+        original_columns = pandas.read_csv(uploaded_file, nrows=1).columns\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n             for c in original_columns:\n"
                },
                {
                    "date": 1749100688868,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,9 +4,9 @@\n from ..models import File\n from ..serializers.file_serializers import FileListSerializers, FileRetrieveSerializers, FileWriteSerializers\n from ..utilities.importbase import *\n from rest_framework.response import Response\n-import \n+import pandas as pd\n \n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n     permission_classes = [resultmanagementPermission]\n@@ -55,9 +55,9 @@\n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n-        original_columns = pandas.read_csv(uploaded_file, nrows=1).columns\n+        original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n             for c in original_columns:\n"
                },
                {
                    "date": 1749100696915,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -56,8 +56,9 @@\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n         original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n+        df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n             for c in original_columns:\n"
                },
                {
                    "date": 1749100719189,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -55,8 +55,9 @@\n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n+        \n         original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n"
                },
                {
                    "date": 1749100724594,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -55,9 +55,9 @@\n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n-        \n+        if file_type\n         original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n"
                },
                {
                    "date": 1749100731593,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -55,10 +55,11 @@\n             # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n-        if file_type\n-        original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n+        if file_type == 'csv':\n+            \n+            original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n"
                },
                {
                    "date": 1749100737387,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -56,10 +56,10 @@\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n         if file_type == 'csv':\n-            \n             original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n+        \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n"
                },
                {
                    "date": 1749100744357,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,9 +57,11 @@\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n         if file_type == 'csv':\n             original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n-        \n+        elif file_type == 'xlsx':\n+            original_columns = pd.read_excel(uploaded_file, nrows=1).columns\n+        else\n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n"
                },
                {
                    "date": 1749100750243,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,9 +59,16 @@\n         if file_type == 'csv':\n             original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n         elif file_type == 'xlsx':\n             original_columns = pd.read_excel(uploaded_file, nrows=1).columns\n-        else\n+        else:\n+            return Response({\"error\": \"Unsupported file format\"}, status=400)\n+        \n+        \n+        temp_file_path = os.path.join(settings.BASE_DIR, 'temp', uploaded_file.name)\n+        with open(temp_file_path, 'wb+') as destination:\n+            for chunk in uploaded_file.chunks():\n+                destination.write(chunk)\n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n"
                },
                {
                    "date": 1749100756631,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,11 +61,9 @@\n         elif file_type == 'xlsx':\n             original_columns = pd.read_excel(uploaded_file, nrows=1).columns\n         else:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n-        \n-        \n-        temp_file_path = os.path.join(settings.BASE_DIR, 'temp', uploaded_file.name)\n+        \\\n         with open(temp_file_path, 'wb+') as destination:\n             for chunk in uploaded_file.chunks():\n                 destination.write(chunk)\n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n"
                },
                {
                    "date": 1749100767020,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,12 +61,8 @@\n         elif file_type == 'xlsx':\n             original_columns = pd.read_excel(uploaded_file, nrows=1).columns\n         else:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n-        \\\n-        with open(temp_file_path, 'wb+') as destination:\n-            for chunk in uploaded_file.chunks():\n-                destination.write(chunk)\n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n         if not email_column:\n"
                },
                {
                    "date": 1749100812099,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -61,11 +61,13 @@\n         elif file_type == 'xlsx':\n             original_columns = pd.read_excel(uploaded_file, nrows=1).columns\n         else:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n+        \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n+        \n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n                     email_column = c\n"
                },
                {
                    "date": 1749100821267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,10 +64,10 @@\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n-        email_column = normalized_map.get(normalize(request.data.get('email_column', '')))\n-        \n+        name = normalized_map.get(normalize(request.data.get('email_column', '')))\n+        ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n                     email_column = c\n"
                },
                {
                    "date": 1749100834280,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,13 @@\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n-        name = normalized_map.get(normalize(request.data.get('email_column', '')))\n+        name = normalized_map.get(normalize(request.data.get('name', '')))\n+        name = normalized_map.get(normalize(request.data.get('name', '')))\n+        name = normalized_map.get(normalize(request.data.get('name', '')))\n+        name = normalized_map.get(normalize(request.data.get('name', '')))\n+        name = normalized_map.get(normalize(request.data.get('name', '')))\n         ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n"
                },
                {
                    "date": 1749100840114,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,12 +65,12 @@\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n+        file = normalized_map.get(normalize(request.data.get('name', '')))\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n-        name = normalized_map.get(normalize(request.data.get('name', '')))\n         ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n"
                },
                {
                    "date": 1749100845593,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,12 +65,12 @@\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n-        file = normalized_map.get(normalize(request.data.get('name', '')))\n+        file = normalized_map.get(normalize(request.data.get('file', '')))\n+        symbol_no = normalized_map.get(normalize(request.data.get('name', '')))\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n-        name = normalized_map.get(normalize(request.data.get('name', '')))\n         ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n"
                },
                {
                    "date": 1749100855043,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -66,11 +66,11 @@\n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n         file = normalized_map.get(normalize(request.data.get('file', '')))\n-        symbol_no = normalized_map.get(normalize(request.data.get('name', '')))\n+        symbol_no = normalized_map.get(normalize(request.data.get('symbol_no', '')))\n+        dateofbirth = normalized_map.get(normalize(request.data.get('name', '')))\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n-        name = normalized_map.get(normalize(request.data.get('name', '')))\n         ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n"
                },
                {
                    "date": 1749100862596,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,10 +67,10 @@\n         normalized_map = {normalize(c): c for c in original_columns}\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n         file = normalized_map.get(normalize(request.data.get('file', '')))\n         symbol_no = normalized_map.get(normalize(request.data.get('symbol_no', '')))\n-        dateofbirth = normalized_map.get(normalize(request.data.get('name', '')))\n-        name = normalized_map.get(normalize(request.data.get('name', '')))\n+        dateofbirth = normalized_map.get(normalize(request.data.get('dateofbirth', '')))\n+        cgpa = normalized_map.get(normalize(request.data.get('name', '')))\n         ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n"
                },
                {
                    "date": 1749100868996,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -68,9 +68,10 @@\n         name = normalized_map.get(normalize(request.data.get('name', '')))\n         file = normalized_map.get(normalize(request.data.get('file', '')))\n         symbol_no = normalized_map.get(normalize(request.data.get('symbol_no', '')))\n         dateofbirth = normalized_map.get(normalize(request.data.get('dateofbirth', '')))\n-        cgpa = normalized_map.get(normalize(request.data.get('name', '')))\n+        cgpa = normalized_map.get(normalize(request.data.get('cgpa', '')))\n+        \n         ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n"
                },
                {
                    "date": 1749100874520,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -69,9 +69,9 @@\n         file = normalized_map.get(normalize(request.data.get('file', '')))\n         symbol_no = normalized_map.get(normalize(request.data.get('symbol_no', '')))\n         dateofbirth = normalized_map.get(normalize(request.data.get('dateofbirth', '')))\n         cgpa = normalized_map.get(normalize(request.data.get('cgpa', '')))\n-        \n+        remarks = normalized_map.get(normalize(request.data.get('remarks', '')))\n         ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n"
                },
                {
                    "date": 1749100879799,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,8 +70,9 @@\n         symbol_no = normalized_map.get(normalize(request.data.get('symbol_no', '')))\n         dateofbirth = normalized_map.get(normalize(request.data.get('dateofbirth', '')))\n         cgpa = normalized_map.get(normalize(request.data.get('cgpa', '')))\n         remarks = normalized_map.get(normalize(request.data.get('remarks', '')))\n+        \n         ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n"
                },
                {
                    "date": 1749100911751,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,9 +71,9 @@\n         dateofbirth = normalized_map.get(normalize(request.data.get('dateofbirth', '')))\n         cgpa = normalized_map.get(normalize(request.data.get('cgpa', '')))\n         remarks = normalized_map.get(normalize(request.data.get('remarks', '')))\n         \n-        ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n+        # ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n         if not email_column:\n             for c in original_columns:\n                 if normalize(c) == 'email':\n                     email_column = c\n"
                },
                {
                    "date": 1749100920406,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,14 +72,10 @@\n         cgpa = normalized_map.get(normalize(request.data.get('cgpa', '')))\n         remarks = normalized_map.get(normalize(request.data.get('remarks', '')))\n         \n         # ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n+\n         if not email_column:\n-            for c in original_columns:\n-                if normalize(c) == 'email':\n-                    email_column = c\n-                    break\n-        if not email_column:\n             return Response({\n                 \"error\": \"No email columns found\",\n                 \"is_first_time\": True,\n                 \"available_columns\": original_columns,\n"
                },
                {
                    "date": 1749100925997,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,9 +73,8 @@\n         remarks = normalized_map.get(normalize(request.data.get('remarks', '')))\n         \n         # ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n \n-        if not email_column:\n             return Response({\n                 \"error\": \"No email columns found\",\n                 \"is_first_time\": True,\n                 \"available_columns\": original_columns,\n"
                },
                {
                    "date": 1749100933326,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -73,61 +73,11 @@\n         remarks = normalized_map.get(normalize(request.data.get('remarks', '')))\n         \n         # ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n \n-            return Response({\n+        return Response({\n                 \"error\": \"No email columns found\",\n                 \"is_first_time\": True,\n                 \"available_columns\": original_columns,\n                 \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n             }, status=200)\n-        stats = process_organized_email_file(temp_file_path, email_column, file_type)\n-        total_email = stats.get('unique_emails', 0) + stats.get('duplicates', 0)\n-        if stats.get('unique_emails', 0) == 0 and stats.get('duplicates', 0) == 0:\n-            return Response({\n-                \"error\": \"No email columns found\",\n-                \"is_first_time\": False,\n-                \"available_columns\": original_columns,\n-                \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n-            }, status=200)\n-        with transaction.atomic():\n-            file_upload = FilesUpload.objects.create(\n-                file=uploaded_file,\n-                name=request.data.get('name', uploaded_file.name),\n-                file_name=uploaded_file.name,\n-                file_size=str(uploaded_file.size),\n-                file_tag=request.data.get('file_tag', 'validate'),\n-                workspace=getattr(request.user, 'active_workspace', None),\n-                user=request.user if request.user.is_authenticated else None,\n-                is_file_organised=True,\n-                email_column=email_column,\n-                total_processed=stats['total_processed'],\n-                total_email=total_email,\n-                billable_email=stats['unique_emails'],\n-                duplicate_email=stats['duplicates'],\n-                dropped_email=stats['dropped_emails'],\n-                clean_email_count=stats['unique_emails'],\n-            )\n-        # return Response({\n-        #     \"id\": file_upload.id,\n-        #     \"name\": file_upload.name,\n-        #     \"file\": request.build_absolute_uri(file_upload.file.url),\n-        #     \"status\": file_upload.status,\n-        #     \"file_tag\": file_upload.file_tag,\n-        #     \"file_name\": file_upload.file_name,\n-        #     \"file_size\": file_upload.file_size,\n-        #     \"stats\": {\n-        #         \"total_processed\": stats['total_processed'],\n-        #         \"total_emails\": total_email,\n-        #         \"unique_emails\": stats['unique_emails'],\n-        #         \"duplicate_emails\": stats['duplicates'],\n-        #         \"dropped_emails\": stats['dropped_emails'],\n-        #         \"email_columns_used\": [email_column],\n-        #         \"email_domains\": stats['email_domains']\n-        #     },\n-        #     \"clean_emails_available\": stats.get('unique_emails', 0) > 0,\n-        #     \"workspace\": file_upload.workspace.name if file_upload.workspace else None\n-        # }, status=201)\n-    # finally:\n-    #     if temp_file_path and os.path.exists(temp_file_path):\n-    #         os.unlink(temp_file_path)\n-\n+       \n"
                },
                {
                    "date": 1749100949626,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,15 +64,10 @@\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n         normalized_map = {normalize(c): c for c in original_columns}\n-        name = normalized_map.get(normalize(request.data.get('name', '')))\n-        file = normalized_map.get(normalize(request.data.get('file', '')))\n-        symbol_no = normalized_map.get(normalize(request.data.get('symbol_no', '')))\n-        dateofbirth = normalized_map.get(normalize(request.data.get('dateofbirth', '')))\n-        cgpa = normalized_map.get(normalize(request.data.get('cgpa', '')))\n-        remarks = normalized_map.get(normalize(request.data.get('remarks', '')))\n         \n+        \n         # ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n \n         return Response({\n                 \"error\": \"No email columns found\",\n"
                },
                {
                    "date": 1749100957638,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,11 +63,10 @@\n         else:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n-        normalized_map = {normalize(c): c for c in original_columns}\n+       \n         \n-        \n         # ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n \n         return Response({\n                 \"error\": \"No email columns found\",\n"
                },
                {
                    "date": 1749100966836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -50,10 +50,8 @@\n         file_name = uploaded_file.name.lower()\n         file_type = 'csv' if file_name.endswith('.csv') else 'xlsx' if file_name.endswith(('.xlsx', '.xls')) else None\n         if not file_type:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n-\n-            # ------------------ ORGANIZED EMAIL FILE HANDLING ------------------\n         def normalize(name):\n             return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n         original_columns = []\n         if file_type == 'csv':\n@@ -63,11 +61,8 @@\n         else:\n             return Response({\"error\": \"Unsupported file format\"}, status=400)\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n-       \n-        \n-        # ('name', 'file', 'symbol_no', 'dateofbirth', 'cgpa', 'remarks', )\n \n         return Response({\n                 \"error\": \"No email columns found\",\n                 \"is_first_time\": True,\n"
                },
                {
                    "date": 1749100976908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n \n         return Response({\n-                \"error\": \"No email columns found\",\n+                \"message\": \"No email columns found\",\n                 \"is_first_time\": True,\n                 \"available_columns\": original_columns,\n                 \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n             }, status=200)\n"
                },
                {
                    "date": 1749100988706,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n \n         return Response({\n-                \"message\": \"No email columns found\",\n+                \"message\": \"Please select the adjacent \",\n                 \"is_first_time\": True,\n                 \"available_columns\": original_columns,\n                 \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n             }, status=200)\n"
                },
                {
                    "date": 1749101000313,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n \n         return Response({\n-                \"message\": \"Please select the adjacent \",\n+                \"message\": \"Please select the adjacent columns to import data from the file\",\n                 \"is_first_time\": True,\n                 \"available_columns\": original_columns,\n                 \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n             }, status=200)\n"
                },
                {
                    "date": 1749101009409,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,10 +63,9 @@\n         \n         df_sample = pd.read_csv(uploaded_file, nrows=3)\n \n         return Response({\n-                \"message\": \"Please select the adjacent columns to import data from the file\",\n-                \"is_first_time\": True,\n+                \"message\": \"Please select the adjacent columns to import data from the file.\",\n                 \"available_columns\": original_columns,\n                 \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n             }, status=200)\n        \n"
                },
                {
                    "date": 1749101175719,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,33 +39,97 @@\n     # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n     # def action_name(self, request, *args, **kwargs):\n     #     return super().list(request, *args, **kwargs)\n     \n+    from rest_framework.response import Response\n+from rest_framework import status\n+from rest_framework.views import APIView\n+import pandas as pd\n+import io\n+\n+class MyBulkImportView(APIView):\n     def create(self, request, *args, **kwargs):\n-        uploaded_file = request.FILES.get('file')\n-        \n+        uploaded_file = request.FILES.get('file', None)\n \n-        if not uploaded_file:\n-            return Response({\"error\": \"No file uploaded\"}, status=400)\n+        # 1) Validate that a file was uploaded\n+        if uploaded_file is None:\n+            return Response(\n+                {\"error\": \"No file was uploaded. Please attach a CSV or Excel file.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n \n+        # 2) Determine file extension (lowercased)\n         file_name = uploaded_file.name.lower()\n-        file_type = 'csv' if file_name.endswith('.csv') else 'xlsx' if file_name.endswith(('.xlsx', '.xls')) else None\n-        if not file_type:\n-            return Response({\"error\": \"Unsupported file format\"}, status=400)\n-        def normalize(name):\n-            return str(name).strip().lower().replace(' ', '').replace('_', '').replace('-', '')\n-        original_columns = []\n-        if file_type == 'csv':\n-            original_columns = pd.read_csv(uploaded_file, nrows=1).columns\n-        elif file_type == 'xlsx':\n-            original_columns = pd.read_excel(uploaded_file, nrows=1).columns\n+        if file_name.endswith('.csv'):\n+            file_type = 'csv'\n+        elif file_name.endswith(('.xlsx', '.xls')):\n+            file_type = 'excel'\n         else:\n-            return Response({\"error\": \"Unsupported file format\"}, status=400)\n-        \n-        df_sample = pd.read_csv(uploaded_file, nrows=3)\n+            return Response(\n+                {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx are allowed.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n \n-        return Response({\n-                \"message\": \"Please select the adjacent columns to import data from the file.\",\n+        # 3) Helper to normalize column names (if you need it later)\n+        def normalize_column_name(name):\n+            return (\n+                str(name)\n+                .strip()\n+                .lower()\n+                .replace(' ', '')\n+                .replace('_', '')\n+                .replace('-', '')\n+            )\n+\n+        # 4) Attempt to read just the first row to get column names\n+        try:\n+            # We need to rewind the file pointer in case it was read already.\n+            uploaded_file.seek(0)\n+            if file_type == 'csv':\n+                df_header = pd.read_csv(uploaded_file, nrows=1)\n+            else:  # file_type == 'excel'\n+                # You could pass sheet_name explicitly (e.g. sheet_name=0) if you want to force the first sheet.\n+                df_header = pd.read_excel(uploaded_file, nrows=1)\n+\n+        except Exception as e:\n+            return Response(\n+                {\"error\": f\"Error parsing the uploaded file to extract columns: {str(e)}\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        original_columns = list(df_header.columns)\n+\n+        # 5) Read a 33 sample of the data\n+        try:\n+            # Rewind again before sampling\n+            uploaded_file.seek(0)\n+\n+            if file_type == 'csv':\n+                df_sample = pd.read_csv(uploaded_file, nrows=3)\n+            else:\n+                # For Excel, explicitly specify engine if needed, or sheet_name\n+                df_sample = pd.read_excel(uploaded_file, nrows=3)\n+\n+        except Exception as e:\n+            return Response(\n+                {\"error\": f\"Error parsing the uploaded file to get sample rows: {str(e)}\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 6) Safely slice to the first 3 columns (if they exist)\n+        # Pandas automatically returns fewer columns if there arent three.\n+        sample_sliced = df_sample.iloc[:, :3]  # first three columns, all rows up to nrows\n+        sample_preview = sample_sliced.iloc[:, :3]  # explicitly ensure  3 rows  3 columns\n+\n+        # 7) Convert the preview to a JSON-serializable structure\n+        preview_records = sample_preview.to_dict(orient='records')\n+\n+        return Response(\n+            {\n+                \"message\": \"Please select which columns you want to import from the file.\",\n                 \"available_columns\": original_columns,\n-                \"sample_first_3_rows\": df_sample.iloc[:3, :3].to_dict(orient='records')\n-            }, status=200)\n+                \"sample_first_3_rows\": preview_records\n+            },\n+            status=status.HTTP_200_OK\n+        )\n+\n        \n"
                },
                {
                    "date": 1749101188024,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -124,9 +124,9 @@\n         preview_records = sample_preview.to_dict(orient='records')\n \n         return Response(\n             {\n-                \"message\": \"Please select which columns you want to import from the file.\",\n+                \"message\": \"Please select adj columns you want to import from the file.\",\n                 \"available_columns\": original_columns,\n                 \"sample_first_3_rows\": preview_records\n             },\n             status=status.HTTP_200_OK\n"
                },
                {
                    "date": 1749101194292,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -124,9 +124,9 @@\n         preview_records = sample_preview.to_dict(orient='records')\n \n         return Response(\n             {\n-                \"message\": \"Please select adj columns you want to import from the file.\",\n+                \"message\": \"Please select adjacent columns and it fi you want to import from the file.\",\n                 \"available_columns\": original_columns,\n                 \"sample_first_3_rows\": preview_records\n             },\n             status=status.HTTP_200_OK\n"
                },
                {
                    "date": 1749102543411,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -124,9 +124,9 @@\n         preview_records = sample_preview.to_dict(orient='records')\n \n         return Response(\n             {\n-                \"message\": \"Please select adjacent columns and it fi you want to import from the file.\",\n+                \"message\": \"Please select adjacent columns and it fields you want to import from the file.\",\n                 \"available_columns\": original_columns,\n                 \"sample_first_3_rows\": preview_records\n             },\n             status=status.HTTP_200_OK\n"
                },
                {
                    "date": 1749102625516,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,90 +46,5 @@\n import pandas as pd\n import io\n \n class MyBulkImportView(APIView):\n-    def create(self, request, *args, **kwargs):\n-        uploaded_file = request.FILES.get('file', None)\n-\n-        # 1) Validate that a file was uploaded\n-        if uploaded_file is None:\n-            return Response(\n-                {\"error\": \"No file was uploaded. Please attach a CSV or Excel file.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n-\n-        # 2) Determine file extension (lowercased)\n-        file_name = uploaded_file.name.lower()\n-        if file_name.endswith('.csv'):\n-            file_type = 'csv'\n-        elif file_name.endswith(('.xlsx', '.xls')):\n-            file_type = 'excel'\n-        else:\n-            return Response(\n-                {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx are allowed.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n-\n-        # 3) Helper to normalize column names (if you need it later)\n-        def normalize_column_name(name):\n-            return (\n-                str(name)\n-                .strip()\n-                .lower()\n-                .replace(' ', '')\n-                .replace('_', '')\n-                .replace('-', '')\n-            )\n-\n-        # 4) Attempt to read just the first row to get column names\n-        try:\n-            # We need to rewind the file pointer in case it was read already.\n-            uploaded_file.seek(0)\n-            if file_type == 'csv':\n-                df_header = pd.read_csv(uploaded_file, nrows=1)\n-            else:  # file_type == 'excel'\n-                # You could pass sheet_name explicitly (e.g. sheet_name=0) if you want to force the first sheet.\n-                df_header = pd.read_excel(uploaded_file, nrows=1)\n-\n-        except Exception as e:\n-            return Response(\n-                {\"error\": f\"Error parsing the uploaded file to extract columns: {str(e)}\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n-\n-        original_columns = list(df_header.columns)\n-\n-        # 5) Read a 33 sample of the data\n-        try:\n-            # Rewind again before sampling\n-            uploaded_file.seek(0)\n-\n-            if file_type == 'csv':\n-                df_sample = pd.read_csv(uploaded_file, nrows=3)\n-            else:\n-                # For Excel, explicitly specify engine if needed, or sheet_name\n-                df_sample = pd.read_excel(uploaded_file, nrows=3)\n-\n-        except Exception as e:\n-            return Response(\n-                {\"error\": f\"Error parsing the uploaded file to get sample rows: {str(e)}\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n-\n-        # 6) Safely slice to the first 3 columns (if they exist)\n-        # Pandas automatically returns fewer columns if there arent three.\n-        sample_sliced = df_sample.iloc[:, :3]  # first three columns, all rows up to nrows\n-        sample_preview = sample_sliced.iloc[:, :3]  # explicitly ensure  3 rows  3 columns\n-\n-        # 7) Convert the preview to a JSON-serializable structure\n-        preview_records = sample_preview.to_dict(orient='records')\n-\n-        return Response(\n-            {\n-                \"message\": \"Please select adjacent columns and it fields you want to import from the file.\",\n-                \"available_columns\": original_columns,\n-                \"sample_first_3_rows\": preview_records\n-            },\n-            status=status.HTTP_200_OK\n-        )\n-\n-       \n+    \n\\ No newline at end of file\n"
                },
                {
                    "date": 1749102630936,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,14 +37,5 @@\n         return super().get_serializer_class()\n \n     # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n     # def action_name(self, request, *args, **kwargs):\n-    #     return super().list(request, *args, **kwargs)\n-    \n-    from rest_framework.response import Response\n-from rest_framework import status\n-from rest_framework.views import APIView\n-import pandas as pd\n-import io\n-\n-class MyBulkImportView(APIView):\n-    \n\\ No newline at end of file\n+    #     return super().list(request, *args, **kwargs)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1749102639116,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,8 +34,96 @@\n             return FileWriteSerializers\n         elif self.action == 'retrieve':\n             return FileRetrieveSerializers\n         return super().get_serializer_class()\n+    \n+     def create(self, request, *args, **kwargs):\n+        uploaded_file = request.FILES.get('file', None)\n \n+        # 1) Validate that a file was uploaded\n+        if uploaded_file is None:\n+            return Response(\n+                {\"error\": \"No file was uploaded. Please attach a CSV or Excel file.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 2) Determine file extension (lowercased)\n+        file_name = uploaded_file.name.lower()\n+        if file_name.endswith('.csv'):\n+            file_type = 'csv'\n+        elif file_name.endswith(('.xlsx', '.xls')):\n+            file_type = 'excel'\n+        else:\n+            return Response(\n+                {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx are allowed.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 3) Helper to normalize column names (if you need it later)\n+        def normalize_column_name(name):\n+            return (\n+                str(name)\n+                .strip()\n+                .lower()\n+                .replace(' ', '')\n+                .replace('_', '')\n+                .replace('-', '')\n+            )\n+\n+        # 4) Attempt to read just the first row to get column names\n+        try:\n+            # We need to rewind the file pointer in case it was read already.\n+            uploaded_file.seek(0)\n+            if file_type == 'csv':\n+                df_header = pd.read_csv(uploaded_file, nrows=1)\n+            else:  # file_type == 'excel'\n+                # You could pass sheet_name explicitly (e.g. sheet_name=0) if you want to force the first sheet.\n+                df_header = pd.read_excel(uploaded_file, nrows=1)\n+\n+        except Exception as e:\n+            return Response(\n+                {\"error\": f\"Error parsing the uploaded file to extract columns: {str(e)}\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        original_columns = list(df_header.columns)\n+\n+        # 5) Read a 33 sample of the data\n+        try:\n+            # Rewind again before sampling\n+            uploaded_file.seek(0)\n+\n+            if file_type == 'csv':\n+                df_sample = pd.read_csv(uploaded_file, nrows=3)\n+            else:\n+                # For Excel, explicitly specify engine if needed, or sheet_name\n+                df_sample = pd.read_excel(uploaded_file, nrows=3)\n+\n+        except Exception as e:\n+            return Response(\n+                {\"error\": f\"Error parsing the uploaded file to get sample rows: {str(e)}\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 6) Safely slice to the first 3 columns (if they exist)\n+        # Pandas automatically returns fewer columns if there arent three.\n+        sample_sliced = df_sample.iloc[:, :3]  # first three columns, all rows up to nrows\n+        sample_preview = sample_sliced.iloc[:, :3]  # explicitly ensure  3 rows  3 columns\n+\n+        # 7) Convert the preview to a JSON-serializable structure\n+        preview_records = sample_preview.to_dict(orient='records')\n+\n+        return Response(\n+            {\n+                \"message\": \"Please select adjacent columns and it fields you want to import from the file.\",\n+                \"available_columns\": original_columns,\n+                \"sample_first_3_rows\": preview_records\n+            },\n+            status=status.HTTP_200_OK\n+        )\n+\n+       \n+\n+\n     # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n     # def action_name(self, request, *args, **kwargs):\n     #     return super().list(request, *args, **kwargs)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1749102647624,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n from django_filters.rest_framework import DjangoFilterBackend\n from ..models import File\n from ..serializers.file_serializers import FileListSerializers, FileRetrieveSerializers, FileWriteSerializers\n from ..utilities.importbase import *\n-from rest_framework.response import Response\n+from rest_framework.response import Response,\n import pandas as pd\n \n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n@@ -35,9 +35,9 @@\n         elif self.action == 'retrieve':\n             return FileRetrieveSerializers\n         return super().get_serializer_class()\n     \n-     def create(self, request, *args, **kwargs):\n+    def create(self, request, *args, **kwargs):\n         uploaded_file = request.FILES.get('file', None)\n \n         # 1) Validate that a file was uploaded\n         if uploaded_file is None:\n"
                },
                {
                    "date": 1749104892927,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,10 @@\n from django_filters.rest_framework import DjangoFilterBackend\n from ..models import File\n from ..serializers.file_serializers import FileListSerializers, FileRetrieveSerializers, FileWriteSerializers\n from ..utilities.importbase import *\n-from rest_framework.response import Response,\n+from rest_framework.response import Response\n+from rest_framework import status\n import pandas as pd\n \n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n@@ -36,92 +37,87 @@\n             return FileRetrieveSerializers\n         return super().get_serializer_class()\n     \n     def create(self, request, *args, **kwargs):\n+        \"\"\"\n+        1) Save the uploaded file into the File model.\n+        2) Read the first row to extract column headers.\n+        3) Read a 33 preview.\n+        4) Return { file_id, available_columns, sample_first_3_rows }.\n+        \"\"\"\n         uploaded_file = request.FILES.get('file', None)\n \n         # 1) Validate that a file was uploaded\n         if uploaded_file is None:\n             return Response(\n-                {\"error\": \"No file was uploaded. Please attach a CSV or Excel file.\"},\n+                {\"error\": \"No file was uploaded. Please attach a CSV or Excel file under key 'file'.\"},\n                 status=status.HTTP_400_BAD_REQUEST\n             )\n \n         # 2) Determine file extension (lowercased)\n-        file_name = uploaded_file.name.lower()\n-        if file_name.endswith('.csv'):\n+        filename = uploaded_file.name.lower()\n+        if filename.endswith('.csv'):\n             file_type = 'csv'\n-        elif file_name.endswith(('.xlsx', '.xls')):\n+        elif filename.endswith(('.xlsx', '.xls')):\n             file_type = 'excel'\n         else:\n             return Response(\n                 {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx are allowed.\"},\n                 status=status.HTTP_400_BAD_REQUEST\n             )\n \n-        # 3) Helper to normalize column names (if you need it later)\n-        def normalize_column_name(name):\n-            return (\n-                str(name)\n-                .strip()\n-                .lower()\n-                .replace(' ', '')\n-                .replace('_', '')\n-                .replace('-', '')\n-            )\n+        # 3) Save the File instance\n+        file_obj = File.objects.create(\n+            file=uploaded_file,\n+            is_active=False  # or True, depending on your logic\n+        )\n \n-        # 4) Attempt to read just the first row to get column names\n+        # 4) Read the first row to get column names\n         try:\n-            # We need to rewind the file pointer in case it was read already.\n+            # rewind in case Django has read some bytes already\n             uploaded_file.seek(0)\n             if file_type == 'csv':\n                 df_header = pd.read_csv(uploaded_file, nrows=1)\n-            else:  # file_type == 'excel'\n-                # You could pass sheet_name explicitly (e.g. sheet_name=0) if you want to force the first sheet.\n+            else:\n                 df_header = pd.read_excel(uploaded_file, nrows=1)\n-\n         except Exception as e:\n+            # If parsing fails, delete the File record so no orphan is left\n+            file_obj.delete()\n             return Response(\n                 {\"error\": f\"Error parsing the uploaded file to extract columns: {str(e)}\"},\n                 status=status.HTTP_400_BAD_REQUEST\n             )\n \n-        original_columns = list(df_header.columns)\n+        original_columns = [str(col) for col in df_header.columns]\n \n         # 5) Read a 33 sample of the data\n         try:\n-            # Rewind again before sampling\n             uploaded_file.seek(0)\n-\n             if file_type == 'csv':\n                 df_sample = pd.read_csv(uploaded_file, nrows=3)\n             else:\n-                # For Excel, explicitly specify engine if needed, or sheet_name\n                 df_sample = pd.read_excel(uploaded_file, nrows=3)\n-\n         except Exception as e:\n+            file_obj.delete()\n             return Response(\n                 {\"error\": f\"Error parsing the uploaded file to get sample rows: {str(e)}\"},\n                 status=status.HTTP_400_BAD_REQUEST\n             )\n \n-        # 6) Safely slice to the first 3 columns (if they exist)\n-        # Pandas automatically returns fewer columns if there arent three.\n-        sample_sliced = df_sample.iloc[:, :3]  # first three columns, all rows up to nrows\n-        sample_preview = sample_sliced.iloc[:, :3]  # explicitly ensure  3 rows  3 columns\n-\n-        # 7) Convert the preview to a JSON-serializable structure\n+        sample_sliced = df_sample.iloc[:, :3]\n+        sample_preview = sample_sliced.iloc[:3, :3]\n         preview_records = sample_preview.to_dict(orient='records')\n \n+        # 6) Return file_id + column headers + preview\n         return Response(\n             {\n-                \"message\": \"Please select adjacent columns and it fields you want to import from the file.\",\n+                \"file_id\": file_obj.pk,\n+                \"message\": \"Please select which columns to import.\",\n                 \"available_columns\": original_columns,\n                 \"sample_first_3_rows\": preview_records\n             },\n-            status=status.HTTP_200_OK\n+            status=status.HTTP_201_CREATED\n         )\n-\n        \n \n \n     # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n"
                },
                {
                    "date": 1749107409036,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,9 +9,9 @@\n import pandas as pd\n \n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n-    permission_classes = [resultmanagementPermission]\n+    permission_classes = []\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = File.objects.all().order_by('-id')\n \n"
                },
                {
                    "date": 1749107415341,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,13 +5,14 @@\n from ..serializers.file_serializers import FileListSerializers, FileRetrieveSerializers, FileWriteSerializers\n from ..utilities.importbase import *\n from rest_framework.response import Response\n from rest_framework import status\n+from \n import pandas as pd\n \n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n-    permission_classes = []\n+    permission_classes = [DynamicModelPermission]\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = File.objects.all().order_by('-id')\n \n"
                },
                {
                    "date": 1749557222690,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,10 +5,14 @@\n from ..serializers.file_serializers import FileListSerializers, FileRetrieveSerializers, FileWriteSerializers\n from ..utilities.importbase import *\n from rest_framework.response import Response\n from rest_framework import status\n-from \n+from mainproj.permissions import DynamicModelPermission\n import pandas as pd\n+import numpy as np\n+from rest_framework.response import Response\n+from rest_framework import status\n+from .models import File\n \n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n     permission_classes = [DynamicModelPermission]\n"
                },
                {
                    "date": 1749557234361,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,11 +8,8 @@\n from rest_framework import status\n from mainproj.permissions import DynamicModelPermission\n import pandas as pd\n import numpy as np\n-from rest_framework.response import Response\n-from rest_framework import status\n-from .models import File\n \n class fileViewsets(viewsets.ModelViewSet):\n     serializer_class = FileListSerializers\n     permission_classes = [DynamicModelPermission]\n"
                },
                {
                    "date": 1749557251741,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,87 +39,90 @@\n             return FileRetrieveSerializers\n         return super().get_serializer_class()\n     \n     def create(self, request, *args, **kwargs):\n-        \"\"\"\n-        1) Save the uploaded file into the File model.\n-        2) Read the first row to extract column headers.\n-        3) Read a 33 preview.\n-        4) Return { file_id, available_columns, sample_first_3_rows }.\n-        \"\"\"\n-        uploaded_file = request.FILES.get('file', None)\n+    \"\"\"\n+    1) Save the uploaded file into the File model.\n+    2) Read the first row to extract column headers.\n+    3) Read a 33 preview.\n+    4) Return { file_id, available_columns, sample_first_3_rows }.\n+    \"\"\"\n+    uploaded_file = request.FILES.get('file', None)\n \n-        # 1) Validate that a file was uploaded\n-        if uploaded_file is None:\n-            return Response(\n-                {\"error\": \"No file was uploaded. Please attach a CSV or Excel file under key 'file'.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n+    # 1) Validate that a file was uploaded\n+    if uploaded_file is None:\n+        return Response(\n+            {\"error\": \"No file was uploaded. Please attach a CSV or Excel file under key 'file'.\"},\n+            status=status.HTTP_400_BAD_REQUEST\n+        )\n \n-        # 2) Determine file extension (lowercased)\n-        filename = uploaded_file.name.lower()\n-        if filename.endswith('.csv'):\n-            file_type = 'csv'\n-        elif filename.endswith(('.xlsx', '.xls')):\n-            file_type = 'excel'\n+    # 2) Determine file extension (lowercased)\n+    filename = uploaded_file.name.lower()\n+    if filename.endswith('.csv'):\n+        file_type = 'csv'\n+    elif filename.endswith(('.xlsx', '.xls')):\n+        file_type = 'excel'\n+    else:\n+        return Response(\n+            {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx are allowed.\"},\n+            status=status.HTTP_400_BAD_REQUEST\n+        )\n+\n+    # 3) Save the File instance\n+    file_obj = File.objects.create(\n+        file=uploaded_file,\n+        is_active=False  # or True, depending on your logic\n+    )\n+\n+    # 4) Read the first row to get column names\n+    try:\n+        uploaded_file.seek(0)\n+        if file_type == 'csv':\n+            df_header = pd.read_csv(uploaded_file, nrows=1)\n         else:\n-            return Response(\n-                {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx are allowed.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n+            df_header = pd.read_excel(uploaded_file, nrows=1)\n+    except Exception as e:\n+        # If parsing fails, delete the File record so no orphan is left\n+        file_obj.delete()\n+        return Response(\n+            {\"error\": f\"Error parsing the uploaded file to extract columns: {str(e)}\"},\n+            status=status.HTTP_400_BAD_REQUEST\n+        )\n \n-        # 3) Save the File instance\n-        file_obj = File.objects.create(\n-            file=uploaded_file,\n-            is_active=False  # or True, depending on your logic\n+    original_columns = [str(col) for col in df_header.columns]\n+\n+    # 5) Read a 33 sample of the data\n+    try:\n+        uploaded_file.seek(0)\n+        if file_type == 'csv':\n+            df_sample = pd.read_csv(uploaded_file, nrows=3)\n+        else:\n+            df_sample = pd.read_excel(uploaded_file, nrows=3)\n+    except Exception as e:\n+        file_obj.delete()\n+        return Response(\n+            {\"error\": f\"Error parsing the uploaded file to get sample rows: {str(e)}\"},\n+            status=status.HTTP_400_BAD_REQUEST\n         )\n \n-        # 4) Read the first row to get column names\n-        try:\n-            # rewind in case Django has read some bytes already\n-            uploaded_file.seek(0)\n-            if file_type == 'csv':\n-                df_header = pd.read_csv(uploaded_file, nrows=1)\n-            else:\n-                df_header = pd.read_excel(uploaded_file, nrows=1)\n-        except Exception as e:\n-            # If parsing fails, delete the File record so no orphan is left\n-            file_obj.delete()\n-            return Response(\n-                {\"error\": f\"Error parsing the uploaded file to extract columns: {str(e)}\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n+    # Handle edge cases for NaN and infinity values\n+    df_sample.replace([np.nan, np.inf, -np.inf], None, inplace=True)\n \n-        original_columns = [str(col) for col in df_header.columns]\n+    sample_sliced = df_sample.iloc[:, :3]  # Select only the first three columns\n+    sample_preview = sample_sliced.iloc[:3, :3]  # Take a 3x3 sample\n+    preview_records = sample_preview.to_dict(orient='records')\n \n-        # 5) Read a 33 sample of the data\n-        try:\n-            uploaded_file.seek(0)\n-            if file_type == 'csv':\n-                df_sample = pd.read_csv(uploaded_file, nrows=3)\n-            else:\n-                df_sample = pd.read_excel(uploaded_file, nrows=3)\n-        except Exception as e:\n-            file_obj.delete()\n-            return Response(\n-                {\"error\": f\"Error parsing the uploaded file to get sample rows: {str(e)}\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n+    # 6) Return file_id + column headers + preview\n+    return Response(\n+        {\n+            \"file_id\": file_obj.pk,\n+            \"message\": \"Please select which columns to import.\",\n+            \"available_columns\": original_columns,\n+            \"sample_first_3_rows\": preview_records\n+        },\n+        status=status.HTTP_201_CREATED\n+    )\n \n-        sample_sliced = df_sample.iloc[:, :3]\n-        sample_preview = sample_sliced.iloc[:3, :3]\n-        preview_records = sample_preview.to_dict(orient='records')\n-\n-        # 6) Return file_id + column headers + preview\n-        return Response(\n-            {\n-                \"file_id\": file_obj.pk,\n-                \"message\": \"Please select which columns to import.\",\n-                \"available_columns\": original_columns,\n-                \"sample_first_3_rows\": preview_records\n-            },\n-            status=status.HTTP_201_CREATED\n-        )\n        \n \n \n     # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n"
                },
                {
                    "date": 1749557257012,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,90 +39,90 @@\n             return FileRetrieveSerializers\n         return super().get_serializer_class()\n     \n     def create(self, request, *args, **kwargs):\n-    \"\"\"\n-    1) Save the uploaded file into the File model.\n-    2) Read the first row to extract column headers.\n-    3) Read a 33 preview.\n-    4) Return { file_id, available_columns, sample_first_3_rows }.\n-    \"\"\"\n-    uploaded_file = request.FILES.get('file', None)\n+        \"\"\"\n+        1) Save the uploaded file into the File model.\n+        2) Read the first row to extract column headers.\n+        3) Read a 33 preview.\n+        4) Return { file_id, available_columns, sample_first_3_rows }.\n+        \"\"\"\n+        uploaded_file = request.FILES.get('file', None)\n \n-    # 1) Validate that a file was uploaded\n-    if uploaded_file is None:\n-        return Response(\n-            {\"error\": \"No file was uploaded. Please attach a CSV or Excel file under key 'file'.\"},\n-            status=status.HTTP_400_BAD_REQUEST\n-        )\n+        # 1) Validate that a file was uploaded\n+        if uploaded_file is None:\n+            return Response(\n+                {\"error\": \"No file was uploaded. Please attach a CSV or Excel file under key 'file'.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n \n-    # 2) Determine file extension (lowercased)\n-    filename = uploaded_file.name.lower()\n-    if filename.endswith('.csv'):\n-        file_type = 'csv'\n-    elif filename.endswith(('.xlsx', '.xls')):\n-        file_type = 'excel'\n-    else:\n-        return Response(\n-            {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx are allowed.\"},\n-            status=status.HTTP_400_BAD_REQUEST\n+        # 2) Determine file extension (lowercased)\n+        filename = uploaded_file.name.lower()\n+        if filename.endswith('.csv'):\n+            file_type = 'csv'\n+        elif filename.endswith(('.xlsx', '.xls')):\n+            file_type = 'excel'\n+        else:\n+            return Response(\n+                {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx are allowed.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 3) Save the File instance\n+        file_obj = File.objects.create(\n+            file=uploaded_file,\n+            is_active=False  # or True, depending on your logic\n         )\n \n-    # 3) Save the File instance\n-    file_obj = File.objects.create(\n-        file=uploaded_file,\n-        is_active=False  # or True, depending on your logic\n-    )\n+        # 4) Read the first row to get column names\n+        try:\n+            uploaded_file.seek(0)\n+            if file_type == 'csv':\n+                df_header = pd.read_csv(uploaded_file, nrows=1)\n+            else:\n+                df_header = pd.read_excel(uploaded_file, nrows=1)\n+        except Exception as e:\n+            # If parsing fails, delete the File record so no orphan is left\n+            file_obj.delete()\n+            return Response(\n+                {\"error\": f\"Error parsing the uploaded file to extract columns: {str(e)}\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n \n-    # 4) Read the first row to get column names\n-    try:\n-        uploaded_file.seek(0)\n-        if file_type == 'csv':\n-            df_header = pd.read_csv(uploaded_file, nrows=1)\n-        else:\n-            df_header = pd.read_excel(uploaded_file, nrows=1)\n-    except Exception as e:\n-        # If parsing fails, delete the File record so no orphan is left\n-        file_obj.delete()\n-        return Response(\n-            {\"error\": f\"Error parsing the uploaded file to extract columns: {str(e)}\"},\n-            status=status.HTTP_400_BAD_REQUEST\n-        )\n+        original_columns = [str(col) for col in df_header.columns]\n \n-    original_columns = [str(col) for col in df_header.columns]\n+        # 5) Read a 33 sample of the data\n+        try:\n+            uploaded_file.seek(0)\n+            if file_type == 'csv':\n+                df_sample = pd.read_csv(uploaded_file, nrows=3)\n+            else:\n+                df_sample = pd.read_excel(uploaded_file, nrows=3)\n+        except Exception as e:\n+            file_obj.delete()\n+            return Response(\n+                {\"error\": f\"Error parsing the uploaded file to get sample rows: {str(e)}\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n \n-    # 5) Read a 33 sample of the data\n-    try:\n-        uploaded_file.seek(0)\n-        if file_type == 'csv':\n-            df_sample = pd.read_csv(uploaded_file, nrows=3)\n-        else:\n-            df_sample = pd.read_excel(uploaded_file, nrows=3)\n-    except Exception as e:\n-        file_obj.delete()\n+        # Handle edge cases for NaN and infinity values\n+        df_sample.replace([np.nan, np.inf, -np.inf], None, inplace=True)\n+\n+        sample_sliced = df_sample.iloc[:, :3]  # Select only the first three columns\n+        sample_preview = sample_sliced.iloc[:3, :3]  # Take a 3x3 sample\n+        preview_records = sample_preview.to_dict(orient='records')\n+\n+        # 6) Return file_id + column headers + preview\n         return Response(\n-            {\"error\": f\"Error parsing the uploaded file to get sample rows: {str(e)}\"},\n-            status=status.HTTP_400_BAD_REQUEST\n+            {\n+                \"file_id\": file_obj.pk,\n+                \"message\": \"Please select which columns to import.\",\n+                \"available_columns\": original_columns,\n+                \"sample_first_3_rows\": preview_records\n+            },\n+            status=status.HTTP_201_CREATED\n         )\n \n-    # Handle edge cases for NaN and infinity values\n-    df_sample.replace([np.nan, np.inf, -np.inf], None, inplace=True)\n-\n-    sample_sliced = df_sample.iloc[:, :3]  # Select only the first three columns\n-    sample_preview = sample_sliced.iloc[:3, :3]  # Take a 3x3 sample\n-    preview_records = sample_preview.to_dict(orient='records')\n-\n-    # 6) Return file_id + column headers + preview\n-    return Response(\n-        {\n-            \"file_id\": file_obj.pk,\n-            \"message\": \"Please select which columns to import.\",\n-            \"available_columns\": original_columns,\n-            \"sample_first_3_rows\": preview_records\n-        },\n-        status=status.HTTP_201_CREATED\n-    )\n-\n        \n \n \n     # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n"
                }
            ],
            "date": 1749100306060,
            "name": "Commit-0",
            "content": "from rest_framework import viewsets\nfrom rest_framework.filters import SearchFilter, OrderingFilter\nfrom django_filters.rest_framework import DjangoFilterBackend\nfrom ..models import File\nfrom ..serializers.file_serializers import FileListSerializers, FileRetrieveSerializers, FileWriteSerializers\nfrom ..utilities.importbase import *\n\nclass fileViewsets(viewsets.ModelViewSet):\n    serializer_class = FileListSerializers\n    # permission_classes = [resultmanagementPermission]\n    # authentication_classes = [JWTAuthentication]\n    #pagination_class = MyPageNumberPagination\n    queryset = File.objects.all().order_by()\n\n    filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n    search_fields = ['id']\n    ordering_fields = ['id']\n\n    # filterset_fields = {\n    #     'id': ['exact'],\n    # }\n\n    def get_queryset(self):\n        queryset = super().get_queryset()\n        #return queryset.filter(user_id=self.request.user.id)\n\n    def get_serializer_class(self):\n        if self.action in ['create', 'update', 'partial_update']:\n            return FileWriteSerializers\n        elif self.action == 'retrieve':\n            return FileRetrieveSerializers\n        return super().get_serializer_class()\n\n    # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n    # def action_name(self, request, *args, **kwargs):\n    #     return super().list(request, *args, **kwargs)\n\n"
        }
    ]
}