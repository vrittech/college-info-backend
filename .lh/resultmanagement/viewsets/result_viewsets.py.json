{
    "sourceFile": "resultmanagement/viewsets/result_viewsets.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 82,
            "patches": [
                {
                    "date": 1749099839879,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1749099845977,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,9 +9,9 @@\n     serializer_class = ResultListSerializers\n     # permission_classes = [resultmanagementPermission]\n     # authentication_classes = [JWTAuthentication]\n     #pagination_class = MyPageNumberPagination\n-    queryset = Result.objects.all().order\n+    queryset = Result.objects.all().order_by('-id')\n \n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id']\n     ordering_fields = ['id']\n"
                },
                {
                    "date": 1749099857976,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,9 +9,9 @@\n     serializer_class = ResultListSerializers\n     # permission_classes = [resultmanagementPermission]\n     # authentication_classes = [JWTAuthentication]\n     #pagination_class = MyPageNumberPagination\n-    queryset = Result.objects.all().order_by('-id')\n+    queryset = Result.objects.all().order_by('-symbol_no')\n \n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id']\n     ordering_fields = ['id']\n"
                },
                {
                    "date": 1749099864817,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,10 +8,10 @@\n class resultViewsets(viewsets.ModelViewSet):\n     serializer_class = ResultListSerializers\n     # permission_classes = [resultmanagementPermission]\n     # authentication_classes = [JWTAuthentication]\n-    #pagination_class = MyPageNumberPagination\n-    queryset = Result.objects.all().order_by('-symbol_no')\n+    pagination_class = MyPageNumberPagination\n+    queryset = Result.objects.all().order_by('symbol_no')\n \n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id']\n     ordering_fields = ['id']\n"
                },
                {
                    "date": 1749099897564,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,13 +6,13 @@\n from ..utilities.importbase import *\n \n class resultViewsets(viewsets.ModelViewSet):\n     serializer_class = ResultListSerializers\n-    # permission_classes = [resultmanagementPermission]\n+    permission_classes = [resultmanagementPermission]\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n-\n+('name', 'file', 'symbol_no', 'dateofbirth', 'subject', 'cgpa', 'remarks', 'created_date', 'updated_date', )\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id']\n     ordering_fields = ['id']\n \n"
                },
                {
                    "date": 1749099903582,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,9 @@\n     permission_classes = [resultmanagementPermission]\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n-('name', 'file', 'symbol_no', 'dateofbirth', 'subject', 'cgpa', 'remarks', 'created_date', 'updated_date', )\n+( )\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id']\n     ordering_fields = ['id']\n \n"
                },
                {
                    "date": 1749099914585,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,11 +10,10 @@\n     permission_classes = [resultmanagementPermission]\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n-( )\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id']\n+    search_fields = ['id','name', 'file', 'symbol_no', 'dateofbirth', 'subject', 'cgpa', 'remarks', 'created_date', 'updated_date',]\n     ordering_fields = ['id']\n \n     # filterset_fields = {\n     #     'id': ['exact'],\n"
                },
                {
                    "date": 1749099931635,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id','name', 'file', 'symbol_no', 'dateofbirth', 'subject', 'cgpa', 'remarks', 'created_date', 'updated_date',]\n+    search_fields = ['id','name', 'file', 'symbol_no', 'dateofbirth', 'subject', 'cgpa', 'remarks']\n     ordering_fields = ['id']\n \n     # filterset_fields = {\n     #     'id': ['exact'],\n"
                },
                {
                    "date": 1749099939127,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id','name', 'file', 'symbol_no', 'dateofbirth', 'subject', 'cgpa', 'remarks']\n+    search_fields = ['id','name', 'file', 'symbol_no', 'dateofbirth', 'subject', 'cgpa']\n     ordering_fields = ['id']\n \n     # filterset_fields = {\n     #     'id': ['exact'],\n"
                },
                {
                    "date": 1749100019555,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id','name', 'file', 'symbol_no', 'dateofbirth', 'subject', 'cgpa']\n+    search_fields = ['id','name', 'file', 'symbol_no',]\n     ordering_fields = ['id']\n \n     # filterset_fields = {\n     #     'id': ['exact'],\n"
                },
                {
                    "date": 1749100025587,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id','name', 'file', 'symbol_no',]\n+    search_fields = ['id','name', 'file', 'symbol_no']\n     ordering_fields = ['id']\n \n     # filterset_fields = {\n     #     'id': ['exact'],\n"
                },
                {
                    "date": 1749100030942,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id','name', 'file', 'symbol_no']\n+    search_fields = ['id','name', 'symbol_no']\n     ordering_fields = ['id']\n \n     # filterset_fields = {\n     #     'id': ['exact'],\n"
                },
                {
                    "date": 1749100232533,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,10 +11,10 @@\n     # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id','name', 'symbol_no']\n-    ordering_fields = ['id']\n+    search_fields = ['id','name','symbol_no']\n+    ordering_fields = ['id',]\n \n     # filterset_fields = {\n     #     'id': ['exact'],\n     # }\n"
                },
                {
                    "date": 1749100239646,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id','name','symbol_no']\n-    ordering_fields = ['id',]\n+    ordering_fields = ['id','symbol_no']\n \n     # filterset_fields = {\n     #     'id': ['exact'],\n     # }\n"
                },
                {
                    "date": 1749100245185,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,8 +13,9 @@\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id','name','symbol_no']\n     ordering_fields = ['id','symbol_no']\n+    or\n \n     # filterset_fields = {\n     #     'id': ['exact'],\n     # }\n"
                },
                {
                    "date": 1749100251900,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,13 +13,13 @@\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     search_fields = ['id','name','symbol_no']\n     ordering_fields = ['id','symbol_no']\n-    or\n+    ordering = ['symbol_no']\n \n-    # filterset_fields = {\n-    #     'id': ['exact'],\n-    # }\n+    filterset_fields = {\n+        'id': ['exact'],\n+    }\n \n     def get_queryset(self):\n         queryset = super().get_queryset()\n         #return queryset.filter(user_id=self.request.user.id)\n"
                },
                {
                    "date": 1749100260786,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,9 +16,9 @@\n     ordering_fields = ['id','symbol_no']\n     ordering = ['symbol_no']\n \n     filterset_fields = {\n-        'id': ['exact'],\n+        'file': ['exact'],\n     }\n \n     def get_queryset(self):\n         queryset = super().get_queryset()\n"
                },
                {
                    "date": 1749100267106,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,8 +16,9 @@\n     ordering_fields = ['id','symbol_no']\n     ordering = ['symbol_no']\n \n     filterset_fields = {\n+        \n         'file': ['exact'],\n     }\n \n     def get_queryset(self):\n"
                },
                {
                    "date": 1749100279488,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,15 +16,15 @@\n     ordering_fields = ['id','symbol_no']\n     ordering = ['symbol_no']\n \n     filterset_fields = {\n-        \n+        'id': ['exact'],\n         'file': ['exact'],\n     }\n \n     def get_queryset(self):\n         queryset = super().get_queryset()\n-        #return queryset.filter(user_id=self.request.user.id)\n+        #return queryset\n \n     def get_serializer_class(self):\n         if self.action in ['create', 'update', 'partial_update']:\n             return ResultWriteSerializers\n"
                },
                {
                    "date": 1749101907799,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,38 +1,215 @@\n-from rest_framework import viewsets\n-from rest_framework.filters import SearchFilter, OrderingFilter\n-from django_filters.rest_framework import DjangoFilterBackend\n+import csv\n+import io\n+from rest_framework.decorators import action\n+from rest_framework.parsers import MultiPartParser, JSONParser\n+from rest_framework.response import Response\n+from rest_framework import status\n+from django.db import transaction\n+from openpyxl import load_workbook  # for .xlsx support\n from ..models import Result\n from ..serializers.result_serializers import ResultListSerializers, ResultRetrieveSerializers, ResultWriteSerializers\n-from ..utilities.importbase import *\n \n class resultViewsets(viewsets.ModelViewSet):\n     serializer_class = ResultListSerializers\n     permission_classes = [resultmanagementPermission]\n-    # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id','name','symbol_no']\n-    ordering_fields = ['id','symbol_no']\n+    search_fields = ['id', 'name', 'symbol_no']\n+    ordering_fields = ['id', 'symbol_no']\n     ordering = ['symbol_no']\n-\n     filterset_fields = {\n         'id': ['exact'],\n         'file': ['exact'],\n     }\n \n-    def get_queryset(self):\n-        queryset = super().get_queryset()\n-        #return queryset\n+    parser_classes = [MultiPartParser, JSONParser]\n \n     def get_serializer_class(self):\n         if self.action in ['create', 'update', 'partial_update']:\n             return ResultWriteSerializers\n         elif self.action == 'retrieve':\n             return ResultRetrieveSerializers\n         return super().get_serializer_class()\n \n-    # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n-    # def action_name(self, request, *args, **kwargs):\n-    #     return super().list(request, *args, **kwargs)\n+    @action(detail=False, methods=['post'], url_path='bulk-import')\n+    def bulk_import(self, request, *args, **kwargs):\n+        \"\"\"\n+        Expects multipart/form-data: \n+          - 'file': the uploaded CSV or XLSX\n+          - 'mapping': a JSON object, e.g. { \"FileCol1\": \"name\", \"FileCol2\": \"symbol_no\", ... }\n+        Reads the file row by row, maps each column to its corresponding field on Result,\n+        and does a batched bulk_create.\n+        Returns a summary of how many rows were inserted successfully (and any errors).\n+        \"\"\"\n \n+        uploaded_file = request.FILES.get('file', None)\n+        mapping_json = request.data.get('mapping', None)\n+\n+        # 1) Validate presence\n+        if not uploaded_file:\n+            return Response(\n+                {\"error\": \"No file uploaded. Please include the CSV/XLSX under key 'file'.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+        if mapping_json is None:\n+            return Response(\n+                {\"error\": \"No mapping provided. Please send a JSON object under key 'mapping'.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 2) Parse mapping JSON\n+        # mapping_json might be a string if sent as form-data. Ensure it's a Python dict.\n+        if isinstance(mapping_json, str):\n+            try:\n+                import json\n+                mapping = json.loads(mapping_json)\n+            except json.JSONDecodeError as e:\n+                return Response(\n+                    {\"error\": f\"Invalid JSON in mapping field: {str(e)}\"},\n+                    status=status.HTTP_400_BAD_REQUEST\n+                )\n+        elif isinstance(mapping_json, dict):\n+            mapping = mapping_json\n+        else:\n+            return Response(\n+                {\"error\": \"Mapping must be a JSON object, either sent directly or as a JSON-encoded string.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 3) Determine file type\n+        file_name = uploaded_file.name.lower()\n+        if file_name.endswith('.csv'):\n+            file_type = 'csv'\n+        elif file_name.endswith(('.xlsx', '.xls')):\n+            file_type = 'excel'\n+        else:\n+            return Response(\n+                {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx accepted.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 4) Helper to normalize column names if needed\n+        #    (optional—only if you want to normalize key lookups)\n+        def normalize_header(hdr):\n+            return str(hdr).strip()\n+\n+        # 5) We will stream through the file. Start a DB transaction so that either\n+        #    all rows succeed, or none do (optional: you can drop this if you want partial commits).\n+        BATCH_SIZE = 5000\n+        total_inserted = 0\n+        errors = []\n+\n+        try:\n+            with transaction.atomic():\n+                instances_to_create = []\n+\n+                # 6) If CSV: use csv.DictReader on a TextIO wrapper\n+                if file_type == 'csv':\n+                    # CSV can come in as bytes. Wrap it with io.TextIOWrapper\n+                    text_stream = io.TextIOWrapper(uploaded_file.file, encoding='utf-8', newline='')\n+                    reader = csv.DictReader(text_stream)\n+\n+                    # Each row is a dict mapping file‐column → cell‐value\n+                    for row_index, row in enumerate(reader, start=1):\n+                        try:\n+                            kwargs = {}\n+                            for csv_col, model_field in mapping.items():\n+                                # Normalize the CSV column name in case front-end provided a slightly differently cased key\n+                                csv_col_norm = normalize_header(csv_col)\n+                                if csv_col_norm not in row:\n+                                    raise KeyError(f\"Column '{csv_col}' not found in CSV header.\")\n+                                raw_value = row[csv_col_norm]\n+                                kwargs[model_field] = raw_value\n+\n+                            instance = Result(**kwargs)\n+                            instances_to_create.append(instance)\n+\n+                            # Every BATCH_SIZE rows, flush to DB\n+                            if len(instances_to_create) >= BATCH_SIZE:\n+                                Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n+                                total_inserted += len(instances_to_create)\n+                                instances_to_create = []\n+\n+                        except Exception as row_exc:\n+                            # Capture row‐level errors but continue processing\n+                            errors.append({\"row\": row_index, \"error\": str(row_exc)})\n+                            continue\n+\n+                else:\n+                    # 7) If Excel: load with openpyxl and iterate over rows\n+                    #    We assume the first row is the header row\n+                    #    → read header row once, then for each subsequent row build a dict\n+                    wb = load_workbook(filename=uploaded_file.file, read_only=True, data_only=True)\n+                    sheet = wb.active  # or wb[some_sheet_name] if you want a specific sheet\n+\n+                    # 7a) Read header row\n+                    excel_iter = sheet.iter_rows(values_only=True)\n+                    header_row = next(excel_iter)\n+                    # Map header positions to column‐names, so we can do header_row[idx] → cell_value\n+                    header_map = {}\n+                    for idx, cell_val in enumerate(header_row):\n+                        if cell_val is None:\n+                            continue\n+                        hdr = normalize_header(cell_val)\n+                        header_map[hdr] = idx\n+\n+                    # 7b) For each mapping key, see if it’s in header_map\n+                    for required_csv_col in mapping.keys():\n+                        if normalize_header(required_csv_col) not in header_map:\n+                            raise KeyError(f\"Column '{required_csv_col}' not found in Excel headers.\")\n+\n+                    # 7c) Now iterate over all data rows\n+                    for row_index, row_tuple in enumerate(excel_iter, start=2):\n+                        # start=2 because row 1 was header\n+                        try:\n+                            kwargs = {}\n+                            for csv_col, model_field in mapping.items():\n+                                csv_col_norm = normalize_header(csv_col)\n+                                col_index = header_map[csv_col_norm]\n+                                raw_value = row_tuple[col_index]\n+                                kwargs[model_field] = raw_value\n+\n+                            instance = Result(**kwargs)\n+                            instances_to_create.append(instance)\n+\n+                            if len(instances_to_create) >= BATCH_SIZE:\n+                                Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n+                                total_inserted += len(instances_to_create)\n+                                instances_to_create = []\n+\n+                        except Exception as row_exc:\n+                            errors.append({\"row\": row_index, \"error\": str(row_exc)})\n+                            continue\n+\n+                    # End of Excel row loop\n+                    wb.close()\n+\n+                # 8) After iterating all rows, flush any remainder\n+                if instances_to_create:\n+                    Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n+                    total_inserted += len(instances_to_create)\n+                    instances_to_create = []\n+\n+        except KeyError as ke:\n+            # This covers missing column headers or other key‐related errors\n+            return Response(\n+                {\"error\": f\"Mapping error: {str(ke)}\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+        except Exception as e:\n+            # Any other exception in the transaction → rollback everything\n+            return Response(\n+                {\"error\": f\"Server error while importing: {str(e)}\"},\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR\n+            )\n+\n+        # 9) Return a summary\n+        return Response(\n+            {\n+                \"inserted\": total_inserted,\n+                \"errors\": errors,\n+                \"message\": f\"Finished import. Inserted {total_inserted} rows.\"\n+            },\n+            status=status.HTTP_200_OK\n+        )\n"
                },
                {
                    "date": 1749101993456,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,9 @@\n from rest_framework import status\n from django.db import transaction\n from openpyxl import load_workbook  # for .xlsx support\n from ..models import Result\n+from \n from ..serializers.result_serializers import ResultListSerializers, ResultRetrieveSerializers, ResultWriteSerializers\n \n class resultViewsets(viewsets.ModelViewSet):\n     serializer_class = ResultListSerializers\n"
                },
                {
                    "date": 1749101998838,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,216 +1,38 @@\n-import csv\n-import io\n-from rest_framework.decorators import action\n-from rest_framework.parsers import MultiPartParser, JSONParser\n-from rest_framework.response import Response\n-from rest_framework import status\n-from django.db import transaction\n-from openpyxl import load_workbook  # for .xlsx support\n+from rest_framework import viewsets\n+from rest_framework.filters import SearchFilter, OrderingFilter\n+from django_filters.rest_framework import DjangoFilterBackend\n from ..models import Result\n-from \n from ..serializers.result_serializers import ResultListSerializers, ResultRetrieveSerializers, ResultWriteSerializers\n+from ..utilities.importbase import *\n \n class resultViewsets(viewsets.ModelViewSet):\n     serializer_class = ResultListSerializers\n     permission_classes = [resultmanagementPermission]\n+    # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id', 'name', 'symbol_no']\n-    ordering_fields = ['id', 'symbol_no']\n+    search_fields = ['id','name','symbol_no']\n+    ordering_fields = ['id','symbol_no']\n     ordering = ['symbol_no']\n+\n     filterset_fields = {\n         'id': ['exact'],\n         'file': ['exact'],\n     }\n \n-    parser_classes = [MultiPartParser, JSONParser]\n+    def get_queryset(self):\n+        queryset = super().get_queryset()\n+        return queryset\n \n     def get_serializer_class(self):\n         if self.action in ['create', 'update', 'partial_update']:\n             return ResultWriteSerializers\n         elif self.action == 'retrieve':\n             return ResultRetrieveSerializers\n         return super().get_serializer_class()\n \n-    @action(detail=False, methods=['post'], url_path='bulk-import')\n-    def bulk_import(self, request, *args, **kwargs):\n-        \"\"\"\n-        Expects multipart/form-data: \n-          - 'file': the uploaded CSV or XLSX\n-          - 'mapping': a JSON object, e.g. { \"FileCol1\": \"name\", \"FileCol2\": \"symbol_no\", ... }\n-        Reads the file row by row, maps each column to its corresponding field on Result,\n-        and does a batched bulk_create.\n-        Returns a summary of how many rows were inserted successfully (and any errors).\n-        \"\"\"\n+    # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n+    # def action_name(self, request, *args, **kwargs):\n+    #     return super().list(request, *args, **kwargs)\n \n-        uploaded_file = request.FILES.get('file', None)\n-        mapping_json = request.data.get('mapping', None)\n-\n-        # 1) Validate presence\n-        if not uploaded_file:\n-            return Response(\n-                {\"error\": \"No file uploaded. Please include the CSV/XLSX under key 'file'.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n-        if mapping_json is None:\n-            return Response(\n-                {\"error\": \"No mapping provided. Please send a JSON object under key 'mapping'.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n-\n-        # 2) Parse mapping JSON\n-        # mapping_json might be a string if sent as form-data. Ensure it's a Python dict.\n-        if isinstance(mapping_json, str):\n-            try:\n-                import json\n-                mapping = json.loads(mapping_json)\n-            except json.JSONDecodeError as e:\n-                return Response(\n-                    {\"error\": f\"Invalid JSON in mapping field: {str(e)}\"},\n-                    status=status.HTTP_400_BAD_REQUEST\n-                )\n-        elif isinstance(mapping_json, dict):\n-            mapping = mapping_json\n-        else:\n-            return Response(\n-                {\"error\": \"Mapping must be a JSON object, either sent directly or as a JSON-encoded string.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n-\n-        # 3) Determine file type\n-        file_name = uploaded_file.name.lower()\n-        if file_name.endswith('.csv'):\n-            file_type = 'csv'\n-        elif file_name.endswith(('.xlsx', '.xls')):\n-            file_type = 'excel'\n-        else:\n-            return Response(\n-                {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx accepted.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n-\n-        # 4) Helper to normalize column names if needed\n-        #    (optional—only if you want to normalize key lookups)\n-        def normalize_header(hdr):\n-            return str(hdr).strip()\n-\n-        # 5) We will stream through the file. Start a DB transaction so that either\n-        #    all rows succeed, or none do (optional: you can drop this if you want partial commits).\n-        BATCH_SIZE = 5000\n-        total_inserted = 0\n-        errors = []\n-\n-        try:\n-            with transaction.atomic():\n-                instances_to_create = []\n-\n-                # 6) If CSV: use csv.DictReader on a TextIO wrapper\n-                if file_type == 'csv':\n-                    # CSV can come in as bytes. Wrap it with io.TextIOWrapper\n-                    text_stream = io.TextIOWrapper(uploaded_file.file, encoding='utf-8', newline='')\n-                    reader = csv.DictReader(text_stream)\n-\n-                    # Each row is a dict mapping file‐column → cell‐value\n-                    for row_index, row in enumerate(reader, start=1):\n-                        try:\n-                            kwargs = {}\n-                            for csv_col, model_field in mapping.items():\n-                                # Normalize the CSV column name in case front-end provided a slightly differently cased key\n-                                csv_col_norm = normalize_header(csv_col)\n-                                if csv_col_norm not in row:\n-                                    raise KeyError(f\"Column '{csv_col}' not found in CSV header.\")\n-                                raw_value = row[csv_col_norm]\n-                                kwargs[model_field] = raw_value\n-\n-                            instance = Result(**kwargs)\n-                            instances_to_create.append(instance)\n-\n-                            # Every BATCH_SIZE rows, flush to DB\n-                            if len(instances_to_create) >= BATCH_SIZE:\n-                                Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n-                                total_inserted += len(instances_to_create)\n-                                instances_to_create = []\n-\n-                        except Exception as row_exc:\n-                            # Capture row‐level errors but continue processing\n-                            errors.append({\"row\": row_index, \"error\": str(row_exc)})\n-                            continue\n-\n-                else:\n-                    # 7) If Excel: load with openpyxl and iterate over rows\n-                    #    We assume the first row is the header row\n-                    #    → read header row once, then for each subsequent row build a dict\n-                    wb = load_workbook(filename=uploaded_file.file, read_only=True, data_only=True)\n-                    sheet = wb.active  # or wb[some_sheet_name] if you want a specific sheet\n-\n-                    # 7a) Read header row\n-                    excel_iter = sheet.iter_rows(values_only=True)\n-                    header_row = next(excel_iter)\n-                    # Map header positions to column‐names, so we can do header_row[idx] → cell_value\n-                    header_map = {}\n-                    for idx, cell_val in enumerate(header_row):\n-                        if cell_val is None:\n-                            continue\n-                        hdr = normalize_header(cell_val)\n-                        header_map[hdr] = idx\n-\n-                    # 7b) For each mapping key, see if it’s in header_map\n-                    for required_csv_col in mapping.keys():\n-                        if normalize_header(required_csv_col) not in header_map:\n-                            raise KeyError(f\"Column '{required_csv_col}' not found in Excel headers.\")\n-\n-                    # 7c) Now iterate over all data rows\n-                    for row_index, row_tuple in enumerate(excel_iter, start=2):\n-                        # start=2 because row 1 was header\n-                        try:\n-                            kwargs = {}\n-                            for csv_col, model_field in mapping.items():\n-                                csv_col_norm = normalize_header(csv_col)\n-                                col_index = header_map[csv_col_norm]\n-                                raw_value = row_tuple[col_index]\n-                                kwargs[model_field] = raw_value\n-\n-                            instance = Result(**kwargs)\n-                            instances_to_create.append(instance)\n-\n-                            if len(instances_to_create) >= BATCH_SIZE:\n-                                Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n-                                total_inserted += len(instances_to_create)\n-                                instances_to_create = []\n-\n-                        except Exception as row_exc:\n-                            errors.append({\"row\": row_index, \"error\": str(row_exc)})\n-                            continue\n-\n-                    # End of Excel row loop\n-                    wb.close()\n-\n-                # 8) After iterating all rows, flush any remainder\n-                if instances_to_create:\n-                    Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n-                    total_inserted += len(instances_to_create)\n-                    instances_to_create = []\n-\n-        except KeyError as ke:\n-            # This covers missing column headers or other key‐related errors\n-            return Response(\n-                {\"error\": f\"Mapping error: {str(ke)}\"},\n-                status=status.HTTP_400_BAD_REQUEST\n-            )\n-        except Exception as e:\n-            # Any other exception in the transaction → rollback everything\n-            return Response(\n-                {\"error\": f\"Server error while importing: {str(e)}\"},\n-                status=status.HTTP_500_INTERNAL_SERVER_ERROR\n-            )\n-\n-        # 9) Return a summary\n-        return Response(\n-            {\n-                \"inserted\": total_inserted,\n-                \"errors\": errors,\n-                \"message\": f\"Finished import. Inserted {total_inserted} rows.\"\n-            },\n-            status=status.HTTP_200_OK\n-        )\n"
                },
                {
                    "date": 1749102011487,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,32 +7,205 @@\n \n class resultViewsets(viewsets.ModelViewSet):\n     serializer_class = ResultListSerializers\n     permission_classes = [resultmanagementPermission]\n-    # authentication_classes = [JWTAuthentication]\n     pagination_class = MyPageNumberPagination\n     queryset = Result.objects.all().order_by('symbol_no')\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id','name','symbol_no']\n-    ordering_fields = ['id','symbol_no']\n+    search_fields = ['id', 'name', 'symbol_no']\n+    ordering_fields = ['id', 'symbol_no']\n     ordering = ['symbol_no']\n-\n     filterset_fields = {\n         'id': ['exact'],\n         'file': ['exact'],\n     }\n \n-    def get_queryset(self):\n-        queryset = super().get_queryset()\n-        return queryset\n+    parser_classes = [MultiPartParser, JSONParser]\n \n     def get_serializer_class(self):\n         if self.action in ['create', 'update', 'partial_update']:\n             return ResultWriteSerializers\n         elif self.action == 'retrieve':\n             return ResultRetrieveSerializers\n         return super().get_serializer_class()\n \n-    # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n-    # def action_name(self, request, *args, **kwargs):\n-    #     return super().list(request, *args, **kwargs)\n+    @action(detail=False, methods=['post'], url_path='bulk-import')\n+    def bulk_import(self, request, *args, **kwargs):\n+        \"\"\"\n+        Expects multipart/form-data: \n+          - 'file': the uploaded CSV or XLSX\n+          - 'mapping': a JSON object, e.g. { \"FileCol1\": \"name\", \"FileCol2\": \"symbol_no\", ... }\n+        Reads the file row by row, maps each column to its corresponding field on Result,\n+        and does a batched bulk_create.\n+        Returns a summary of how many rows were inserted successfully (and any errors).\n+        \"\"\"\n \n+        uploaded_file = request.FILES.get('file', None)\n+        mapping_json = request.data.get('mapping', None)\n+\n+        # 1) Validate presence\n+        if not uploaded_file:\n+            return Response(\n+                {\"error\": \"No file uploaded. Please include the CSV/XLSX under key 'file'.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+        if mapping_json is None:\n+            return Response(\n+                {\"error\": \"No mapping provided. Please send a JSON object under key 'mapping'.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 2) Parse mapping JSON\n+        # mapping_json might be a string if sent as form-data. Ensure it's a Python dict.\n+        if isinstance(mapping_json, str):\n+            try:\n+                import json\n+                mapping = json.loads(mapping_json)\n+            except json.JSONDecodeError as e:\n+                return Response(\n+                    {\"error\": f\"Invalid JSON in mapping field: {str(e)}\"},\n+                    status=status.HTTP_400_BAD_REQUEST\n+                )\n+        elif isinstance(mapping_json, dict):\n+            mapping = mapping_json\n+        else:\n+            return Response(\n+                {\"error\": \"Mapping must be a JSON object, either sent directly or as a JSON-encoded string.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 3) Determine file type\n+        file_name = uploaded_file.name.lower()\n+        if file_name.endswith('.csv'):\n+            file_type = 'csv'\n+        elif file_name.endswith(('.xlsx', '.xls')):\n+            file_type = 'excel'\n+        else:\n+            return Response(\n+                {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx accepted.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # 4) Helper to normalize column names if needed\n+        #    (optional—only if you want to normalize key lookups)\n+        def normalize_header(hdr):\n+            return str(hdr).strip()\n+\n+        # 5) We will stream through the file. Start a DB transaction so that either\n+        #    all rows succeed, or none do (optional: you can drop this if you want partial commits).\n+        BATCH_SIZE = 5000\n+        total_inserted = 0\n+        errors = []\n+\n+        try:\n+            with transaction.atomic():\n+                instances_to_create = []\n+\n+                # 6) If CSV: use csv.DictReader on a TextIO wrapper\n+                if file_type == 'csv':\n+                    # CSV can come in as bytes. Wrap it with io.TextIOWrapper\n+                    text_stream = io.TextIOWrapper(uploaded_file.file, encoding='utf-8', newline='')\n+                    reader = csv.DictReader(text_stream)\n+\n+                    # Each row is a dict mapping file‐column → cell‐value\n+                    for row_index, row in enumerate(reader, start=1):\n+                        try:\n+                            kwargs = {}\n+                            for csv_col, model_field in mapping.items():\n+                                # Normalize the CSV column name in case front-end provided a slightly differently cased key\n+                                csv_col_norm = normalize_header(csv_col)\n+                                if csv_col_norm not in row:\n+                                    raise KeyError(f\"Column '{csv_col}' not found in CSV header.\")\n+                                raw_value = row[csv_col_norm]\n+                                kwargs[model_field] = raw_value\n+\n+                            instance = Result(**kwargs)\n+                            instances_to_create.append(instance)\n+\n+                            # Every BATCH_SIZE rows, flush to DB\n+                            if len(instances_to_create) >= BATCH_SIZE:\n+                                Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n+                                total_inserted += len(instances_to_create)\n+                                instances_to_create = []\n+\n+                        except Exception as row_exc:\n+                            # Capture row‐level errors but continue processing\n+                            errors.append({\"row\": row_index, \"error\": str(row_exc)})\n+                            continue\n+\n+                else:\n+                    # 7) If Excel: load with openpyxl and iterate over rows\n+                    #    We assume the first row is the header row\n+                    #    → read header row once, then for each subsequent row build a dict\n+                    wb = load_workbook(filename=uploaded_file.file, read_only=True, data_only=True)\n+                    sheet = wb.active  # or wb[some_sheet_name] if you want a specific sheet\n+\n+                    # 7a) Read header row\n+                    excel_iter = sheet.iter_rows(values_only=True)\n+                    header_row = next(excel_iter)\n+                    # Map header positions to column‐names, so we can do header_row[idx] → cell_value\n+                    header_map = {}\n+                    for idx, cell_val in enumerate(header_row):\n+                        if cell_val is None:\n+                            continue\n+                        hdr = normalize_header(cell_val)\n+                        header_map[hdr] = idx\n+\n+                    # 7b) For each mapping key, see if it’s in header_map\n+                    for required_csv_col in mapping.keys():\n+                        if normalize_header(required_csv_col) not in header_map:\n+                            raise KeyError(f\"Column '{required_csv_col}' not found in Excel headers.\")\n+\n+                    # 7c) Now iterate over all data rows\n+                    for row_index, row_tuple in enumerate(excel_iter, start=2):\n+                        # start=2 because row 1 was header\n+                        try:\n+                            kwargs = {}\n+                            for csv_col, model_field in mapping.items():\n+                                csv_col_norm = normalize_header(csv_col)\n+                                col_index = header_map[csv_col_norm]\n+                                raw_value = row_tuple[col_index]\n+                                kwargs[model_field] = raw_value\n+\n+                            instance = Result(**kwargs)\n+                            instances_to_create.append(instance)\n+\n+                            if len(instances_to_create) >= BATCH_SIZE:\n+                                Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n+                                total_inserted += len(instances_to_create)\n+                                instances_to_create = []\n+\n+                        except Exception as row_exc:\n+                            errors.append({\"row\": row_index, \"error\": str(row_exc)})\n+                            continue\n+\n+                    # End of Excel row loop\n+                    wb.close()\n+\n+                # 8) After iterating all rows, flush any remainder\n+                if instances_to_create:\n+                    Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n+                    total_inserted += len(instances_to_create)\n+                    instances_to_create = []\n+\n+        except KeyError as ke:\n+            # This covers missing column headers or other key‐related errors\n+            return Response(\n+                {\"error\": f\"Mapping error: {str(ke)}\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+        except Exception as e:\n+            # Any other exception in the transaction → rollback everything\n+            return Response(\n+                {\"error\": f\"Server error while importing: {str(e)}\"},\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR\n+            )\n+\n+        # 9) Return a summary\n+        return Response(\n+            {\n+                \"inserted\": total_inserted,\n+                \"errors\": errors,\n+                \"message\": f\"Finished import. Inserted {total_inserted} rows.\"\n+            },\n+            status=status.HTTP_200_OK\n+        )\n\\ No newline at end of file\n"
                },
                {
                    "date": 1749102029960,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,17 @@\n from django_filters.rest_framework import DjangoFilterBackend\n from ..models import Result\n from ..serializers.result_serializers import ResultListSerializers, ResultRetrieveSerializers, ResultWriteSerializers\n from ..utilities.importbase import *\n-\n+import csv\n+import io\n+from rest_framework.decorators import action\n+from rest_framework.parsers import MultiPartParser, JSONParser\n+from rest_framework.response import Response\n+from rest_framework import status\n+from django.db import transaction\n+from openpyxl import load_workbook  # for .xlsx support\n+from ..models import Result\n class resultViewsets(viewsets.ModelViewSet):\n     serializer_class = ResultListSerializers\n     permission_classes = [resultmanagementPermission]\n     pagination_class = MyPageNumberPagination\n"
                },
                {
                    "date": 1749102352851,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,219 +1,257 @@\n-from rest_framework import viewsets\n-from rest_framework.filters import SearchFilter, OrderingFilter\n-from django_filters.rest_framework import DjangoFilterBackend\n-from ..models import Result\n-from ..serializers.result_serializers import ResultListSerializers, ResultRetrieveSerializers, ResultWriteSerializers\n-from ..utilities.importbase import *\n import csv\n import io\n+import json\n+\n+from rest_framework import viewsets, status\n from rest_framework.decorators import action\n from rest_framework.parsers import MultiPartParser, JSONParser\n from rest_framework.response import Response\n-from rest_framework import status\n from django.db import transaction\n-from openpyxl import load_workbook  # for .xlsx support\n-from ..models import Result\n-class resultViewsets(viewsets.ModelViewSet):\n+from openpyxl import load_workbook  # for .xlsx/.xls support\n+\n+from .models import File, Result\n+from .serializers import (\n+    ResultListSerializers,\n+    ResultRetrieveSerializers,\n+    ResultWriteSerializers\n+)\n+from .permissions import resultmanagementPermission\n+from .pagination import MyPageNumberPagination\n+\n+\n+class ResultViewSet(viewsets.ModelViewSet):\n+    \"\"\"\n+    Exposes /api/results/ for standard list/retrieve/create/update/delete,\n+    plus a custom POST /api/results/bulk-import/ for batch‐importing a CSV/Excel file.\n+    \"\"\"\n+    queryset = Result.objects.all().order_by(\"symbol_no\")\n     serializer_class = ResultListSerializers\n     permission_classes = [resultmanagementPermission]\n     pagination_class = MyPageNumberPagination\n-    queryset = Result.objects.all().order_by('symbol_no')\n-    filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    search_fields = ['id', 'name', 'symbol_no']\n-    ordering_fields = ['id', 'symbol_no']\n-    ordering = ['symbol_no']\n-    filterset_fields = {\n-        'id': ['exact'],\n-        'file': ['exact'],\n-    }\n-\n     parser_classes = [MultiPartParser, JSONParser]\n \n     def get_serializer_class(self):\n-        if self.action in ['create', 'update', 'partial_update']:\n+        if self.action in [\"create\", \"update\", \"partial_update\"]:\n             return ResultWriteSerializers\n-        elif self.action == 'retrieve':\n+        elif self.action == \"retrieve\":\n             return ResultRetrieveSerializers\n         return super().get_serializer_class()\n \n-    @action(detail=False, methods=['post'], url_path='bulk-import')\n+    @action(detail=False, methods=[\"post\"], url_path=\"bulk-import\")\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n-        Expects multipart/form-data: \n-          - 'file': the uploaded CSV or XLSX\n-          - 'mapping': a JSON object, e.g. { \"FileCol1\": \"name\", \"FileCol2\": \"symbol_no\", ... }\n-        Reads the file row by row, maps each column to its corresponding field on Result,\n-        and does a batched bulk_create.\n-        Returns a summary of how many rows were inserted successfully (and any errors).\n+        1) Expects multipart/form-data with:\n+           - 'file': an uploaded CSV or XLSX/XLS\n+           - 'name': a string (the File Name, e.g. \"SEE Result 2081\")\n+           - 'mapping': JSON-encoded string, e.g.\n+               {\n+                 \"symbol_no\": \"RollNo\",\n+                 \"dateofbirth\": \"DOB\",\n+                 \"cgpa\": \"CGPA\",\n+                 \"remarks\": \"Remarks\"\n+               }\n+\n+        2) Saves File(...) → file_obj\n+        3) Streams row-by-row, doing bulk_create([...]) in batches of 5 000\n+        4) Returns only {\"message\": \"File processed and data saved to database.\"} on success.\n         \"\"\"\n \n-        uploaded_file = request.FILES.get('file', None)\n-        mapping_json = request.data.get('mapping', None)\n+        # ─── 1) Validate Inputs ───────────────────────────────────────────\n+        uploaded_file = request.FILES.get(\"file\", None)\n+        name_value   = request.data.get(\"name\", \"\").strip()\n+        mapping_json = request.data.get(\"mapping\", None)\n \n-        # 1) Validate presence\n         if not uploaded_file:\n             return Response(\n-                {\"error\": \"No file uploaded. Please include the CSV/XLSX under key 'file'.\"},\n+                {\"error\": \"No file was uploaded. Please include your CSV/XLSX under key 'file'.\"},\n                 status=status.HTTP_400_BAD_REQUEST\n             )\n+\n+        if not name_value:\n+            return Response(\n+                {\"error\": \"Missing 'name'. Please send a nonempty File Name.\"},\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n         if mapping_json is None:\n             return Response(\n-                {\"error\": \"No mapping provided. Please send a JSON object under key 'mapping'.\"},\n+                {\"error\": \"Missing 'mapping'. Please send a JSON object under key 'mapping'.\"},\n                 status=status.HTTP_400_BAD_REQUEST\n             )\n \n-        # 2) Parse mapping JSON\n-        # mapping_json might be a string if sent as form-data. Ensure it's a Python dict.\n+        # ─── 2) Parse mapping JSON (it might be a string) ───────────────\n         if isinstance(mapping_json, str):\n             try:\n-                import json\n                 mapping = json.loads(mapping_json)\n             except json.JSONDecodeError as e:\n                 return Response(\n-                    {\"error\": f\"Invalid JSON in mapping field: {str(e)}\"},\n+                    {\"error\": f\"Invalid JSON in 'mapping': {str(e)}\"},\n                     status=status.HTTP_400_BAD_REQUEST\n                 )\n         elif isinstance(mapping_json, dict):\n             mapping = mapping_json\n         else:\n             return Response(\n-                {\"error\": \"Mapping must be a JSON object, either sent directly or as a JSON-encoded string.\"},\n+                {\"error\": \"'mapping' must be a JSON‐encoded string or JSON object.\"},\n                 status=status.HTTP_400_BAD_REQUEST\n             )\n \n-        # 3) Determine file type\n-        file_name = uploaded_file.name.lower()\n-        if file_name.endswith('.csv'):\n-            file_type = 'csv'\n-        elif file_name.endswith(('.xlsx', '.xls')):\n-            file_type = 'excel'\n+        # ─── 3) Ensure exactly the four keys we expect ───────────────────\n+        required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        if set(mapping.keys()) != required_keys:\n+            return Response(\n+                {\n+                    \"error\": (\n+                        \"'mapping' must contain exactly these keys: \"\n+                        \"symbol_no, dateofbirth, cgpa, remarks\"\n+                    )\n+                },\n+                status=status.HTTP_400_BAD_REQUEST\n+            )\n+\n+        # ─── 4) Detect file type by extension ────────────────────────────\n+        filename_lower = uploaded_file.name.lower()\n+        if filename_lower.endswith(\".csv\"):\n+            file_type = \"csv\"\n+        elif filename_lower.endswith((\".xlsx\", \".xls\")):\n+            file_type = \"excel\"\n         else:\n             return Response(\n-                {\"error\": \"Unsupported file format. Only .csv, .xls, or .xlsx accepted.\"},\n+                {\"error\": \"Unsupported file format. Only .csv, .xlsx, .xls are allowed.\"},\n                 status=status.HTTP_400_BAD_REQUEST\n             )\n \n-        # 4) Helper to normalize column names if needed\n-        #    (optional—only if you want to normalize key lookups)\n-        def normalize_header(hdr):\n-            return str(hdr).strip()\n+        # ─── 5) Save a new File(...) record ─────────────────────────────\n+        file_obj = File.objects.create(\n+            file=uploaded_file,\n+            is_active=True\n+        )\n \n-        # 5) We will stream through the file. Start a DB transaction so that either\n-        #    all rows succeed, or none do (optional: you can drop this if you want partial commits).\n+        # ─── 6) Stream + Bulk-Insert into Result ─────────────────────────\n         BATCH_SIZE = 5000\n-        total_inserted = 0\n-        errors = []\n+        instances_to_create = []\n \n+        def normalize_header(hdr: str) -> str:\n+            return str(hdr).strip()\n+\n         try:\n             with transaction.atomic():\n-                instances_to_create = []\n-\n-                # 6) If CSV: use csv.DictReader on a TextIO wrapper\n-                if file_type == 'csv':\n-                    # CSV can come in as bytes. Wrap it with io.TextIOWrapper\n-                    text_stream = io.TextIOWrapper(uploaded_file.file, encoding='utf-8', newline='')\n+                if file_type == \"csv\":\n+                    # a) Wrap the underlying file so csv.DictReader can read it\n+                    text_stream = io.TextIOWrapper(uploaded_file.file, encoding=\"utf-8\", newline=\"\")\n                     reader = csv.DictReader(text_stream)\n \n-                    # Each row is a dict mapping file‐column → cell‐value\n-                    for row_index, row in enumerate(reader, start=1):\n+                    # b) Verify that each mapping value is indeed a header in the CSV\n+                    csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n+                    for field_key, col_name in mapping.items():\n+                        if normalize_header(col_name) not in csv_headers:\n+                            raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n+\n+                    # c) Iterate every row in the CSV\n+                    for row in reader:\n                         try:\n-                            kwargs = {}\n-                            for csv_col, model_field in mapping.items():\n-                                # Normalize the CSV column name in case front-end provided a slightly differently cased key\n-                                csv_col_norm = normalize_header(csv_col)\n-                                if csv_col_norm not in row:\n\\ No newline at end of file\n-                                    raise KeyError(f\"Column '{csv_col}' not found in CSV header.\")\n-                                raw_value = row[csv_col_norm]\n-                                kwargs[model_field] = raw_value\n+                            # Build the exact four data‐fields plus name/file/mapped_json\n+                            instances_to_create.append(\n+                                Result(\n+                                    name=name_value,\n+                                    mapped_json=mapping,\n+                                    file=file_obj,\n+                                    symbol_no=row[ normalize_header(mapping[\"symbol_no\"]) ].strip(),\n+                                    dateofbirth=row[ normalize_header(mapping[\"dateofbirth\"]) ].strip(),\n+                                    cgpa=row[ normalize_header(mapping[\"cgpa\"]) ].strip(),\n+                                    remarks=row[ normalize_header(mapping[\"remarks\"]) ].strip(),\n+                                )\n+                            )\n+                        except Exception:\n+                            # Skip any malformed row—do not abort the whole import\n+                            continue\n \n-                            instance = Result(**kwargs)\n-                            instances_to_create.append(instance)\n+                        # d) Once we have BATCH_SIZE items, flush them in one bulk_create\n+                        if len(instances_to_create) >= BATCH_SIZE:\n+                            Result.objects.bulk_create(instances_to_create)\n+                            instances_to_create = []\n \n-                            # Every BATCH_SIZE rows, flush to DB\n-                            if len(instances_to_create) >= BATCH_SIZE:\n-                                Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n-                                total_inserted += len(instances_to_create)\n-                                instances_to_create = []\n+                    # e) Flush any leftover rows\n+                    if instances_to_create:\n+                        Result.objects.bulk_create(instances_to_create)\n+                        instances_to_create = []\n \n-                        except Exception as row_exc:\n-                            # Capture row‐level errors but continue processing\n-                            errors.append({\"row\": row_index, \"error\": str(row_exc)})\n-                            continue\n-\n                 else:\n-                    # 7) If Excel: load with openpyxl and iterate over rows\n-                    #    We assume the first row is the header row\n-                    #    → read header row once, then for each subsequent row build a dict\n+                    # 7) Excel path: use openpyxl in read_only mode\n                     wb = load_workbook(filename=uploaded_file.file, read_only=True, data_only=True)\n-                    sheet = wb.active  # or wb[some_sheet_name] if you want a specific sheet\n-\n-                    # 7a) Read header row\n+                    sheet = wb.active  # or wb[some_sheet_name] if you need a specific sheet\n                     excel_iter = sheet.iter_rows(values_only=True)\n-                    header_row = next(excel_iter)\n-                    # Map header positions to column‐names, so we can do header_row[idx] → cell_value\n-                    header_map = {}\n-                    for idx, cell_val in enumerate(header_row):\n-                        if cell_val is None:\n-                            continue\n-                        hdr = normalize_header(cell_val)\n-                        header_map[hdr] = idx\n \n-                    # 7b) For each mapping key, see if it’s in header_map\n-                    for required_csv_col in mapping.keys():\n-                        if normalize_header(required_csv_col) not in header_map:\n-                            raise KeyError(f\"Column '{required_csv_col}' not found in Excel headers.\")\n+                    # a) Read the header row (first row)\n+                    header_row = next(excel_iter, None)\n+                    if not header_row:\n+                        raise KeyError(\"Excel file appears to be empty (no header row).\")\n \n-                    # 7c) Now iterate over all data rows\n+                    # b) Build a map { header_text : column_index }\n+                    header_map = {\n+                        normalize_header(cell_val): idx\n+                        for idx, cell_val in enumerate(header_row)\n+                        if cell_val is not None\n+                    }\n+\n+                    # c) Verify that the user’s mapping values exist in that header_map\n+                    for field_key, col_name in mapping.items():\n+                        if normalize_header(col_name) not in header_map:\n+                            raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n+\n+                    # d) Iterate each subsequent row (row_index starts at 2)\n                     for row_index, row_tuple in enumerate(excel_iter, start=2):\n-                        # start=2 because row 1 was header\n                         try:\n-                            kwargs = {}\n-                            for csv_col, model_field in mapping.items():\n-                                csv_col_norm = normalize_header(csv_col)\n-                                col_index = header_map[csv_col_norm]\n-                                raw_value = row_tuple[col_index]\n-                                kwargs[model_field] = raw_value\n+                            sn_idx   = header_map[ normalize_header(mapping[\"symbol_no\"]) ]\n+                            dob_idx  = header_map[ normalize_header(mapping[\"dateofbirth\"]) ]\n+                            cgpa_idx = header_map[ normalize_header(mapping[\"cgpa\"]) ]\n+                            rm_idx   = header_map[ normalize_header(mapping[\"remarks\"]) ]\n \n-                            instance = Result(**kwargs)\n-                            instances_to_create.append(instance)\n+                            def get_cell(r, i):\n+                                return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n \n-                            if len(instances_to_create) >= BATCH_SIZE:\n-                                Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n-                                total_inserted += len(instances_to_create)\n-                                instances_to_create = []\n-\n-                        except Exception as row_exc:\n-                            errors.append({\"row\": row_index, \"error\": str(row_exc)})\n+                            instances_to_create.append(\n+                                Result(\n+                                    name=name_value,\n+                                    mapped_json=mapping,\n+                                    file=file_obj,\n+                                    symbol_no   = get_cell(row_tuple, sn_idx),\n+                                    dateofbirth = get_cell(row_tuple, dob_idx),\n+                                    cgpa        = get_cell(row_tuple, cgpa_idx),\n+                                    remarks     = get_cell(row_tuple, rm_idx),\n+                                )\n+                            )\n+                        except Exception:\n+                            # Skip this row if anything goes wrong (e.g. index error, type error)\n                             continue\n \n-                    # End of Excel row loop\n+                        if len(instances_to_create) >= BATCH_SIZE:\n+                            Result.objects.bulk_create(instances_to_create)\n+                            instances_to_create = []\n+\n+                    # e) Flush any leftover rows\n+                    if instances_to_create:\n+                        Result.objects.bulk_create(instances_to_create)\n+                        instances_to_create = []\n+\n                     wb.close()\n \n-                # 8) After iterating all rows, flush any remainder\n-                if instances_to_create:\n-                    Result.objects.bulk_create(instances_to_create, ignore_conflicts=False)\n-                    total_inserted += len(instances_to_create)\n-                    instances_to_create = []\n-\n         except KeyError as ke:\n-            # This covers missing column headers or other key‐related errors\n+            # A required column was missing: abort entire import, roll back\n             return Response(\n                 {\"error\": f\"Mapping error: {str(ke)}\"},\n                 status=status.HTTP_400_BAD_REQUEST\n             )\n+\n         except Exception as e:\n-            # Any other exception in the transaction → rollback everything\n+            # Any other unexpected error: roll back + 500\n             return Response(\n                 {\"error\": f\"Server error while importing: {str(e)}\"},\n                 status=status.HTTP_500_INTERNAL_SERVER_ERROR\n             )\n \n-        # 9) Return a summary\n+        # ─── 8) On success, return only a minimal message ─────────────────\n         return Response(\n-            {\n-                \"inserted\": total_inserted,\n-                \"errors\": errors,\n-                \"message\": f\"Finished import. Inserted {total_inserted} rows.\"\n-            },\n+            {\"message\": \"File processed and data saved to database.\"},\n             status=status.HTTP_200_OK\n-        )\n+        )\n"
                },
                {
                    "date": 1749102361609,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n from rest_framework.response import Response\n from django.db import transaction\n from openpyxl import load_workbook  # for .xlsx/.xls support\n \n-from .models import File, Result\n+from ..models import File, Result\n from .serializers import (\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n"
                },
                {
                    "date": 1749102375018,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,10 @@\n+from rest_framework import viewsets\n+from rest_framework.filters import SearchFilter, OrderingFilter\n+from django_filters.rest_framework import DjangoFilterBackend\n+from ..models import Result\n+from ..serializers.result_serializers import ResultListSerializers, ResultRetrieveSerializers, ResultWriteSerializers\n+from ..utilities.importbase import *\n import csv\n import io\n import json\n \n@@ -8,9 +14,9 @@\n from rest_framework.response import Response\n from django.db import transaction\n from openpyxl import load_workbook  # for .xlsx/.xls support\n \n-from ..models import File, Result\n+from .models import File, Result\n from .serializers import (\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n"
                },
                {
                    "date": 1749102383786,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,9 +14,9 @@\n from rest_framework.response import Response\n from django.db import transaction\n from openpyxl import load_workbook  # for .xlsx/.xls support\n \n-from .models import File, Result\n+from .models.models import File, Result\n from .serializers import (\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n"
                },
                {
                    "date": 1749102389416,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,10 +14,10 @@\n from rest_framework.response import Response\n from django.db import transaction\n from openpyxl import load_workbook  # for .xlsx/.xls support\n \n-from .models.models import File, Result\n-from .serializers import (\n+from ..models import File, Result\n+from ..serializers import (\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n )\n"
                },
                {
                    "date": 1749102398604,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n )\n-from .permissions import resultmanagementPermission\n+from ..permissions import resultmanagementPermission\n from .pagination import MyPageNumberPagination\n \n \n class ResultViewSet(viewsets.ModelViewSet):\n"
                },
                {
                    "date": 1749102412823,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n )\n-from ..permissions import resultmanagementPermission\n+from ..utilities.permissions import resultmanagementPermission\n from .pagination import MyPageNumberPagination\n \n \n class ResultViewSet(viewsets.ModelViewSet):\n"
                },
                {
                    "date": 1749102421924,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,9 +21,9 @@\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n )\n from ..utilities.permissions import resultmanagementPermission\n-from .pagination import MyPageNumberPagination\n+from ..utilities. .pagination import MyPageNumberPagination\n \n \n class ResultViewSet(viewsets.ModelViewSet):\n     \"\"\"\n"
                },
                {
                    "date": 1749102816183,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -21,9 +21,10 @@\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n )\n from ..utilities.permissions import resultmanagementPermission\n-from ..utilities. .pagination import MyPageNumberPagination\n+from ..utilities.pagination import MyPageNumberPagination\n+CACHE_TTL = 60 * 5  # cache for 5 minutes (300 seconds)\n \n \n class ResultViewSet(viewsets.ModelViewSet):\n     \"\"\"\n"
                },
                {
                    "date": 1749102871949,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,8 +22,9 @@\n     ResultWriteSerializers\n )\n from ..utilities.permissions import resultmanagementPermission\n from ..utilities.pagination import MyPageNumberPagination\n+\n CACHE_TTL = 60 * 5  # cache for 5 minutes (300 seconds)\n \n \n class ResultViewSet(viewsets.ModelViewSet):\n@@ -42,9 +43,28 @@\n             return ResultWriteSerializers\n         elif self.action == \"retrieve\":\n             return ResultRetrieveSerializers\n         return super().get_serializer_class()\n+    \n+    # ─── Cache the list() action ─────────────────────────────────────────\n+    @method_decorator(cache_page(CACHE_TTL), name=\"list\")\n+    def list(self, request, *args, **kwargs):\n+        \"\"\"\n+        GET /api/results/\n+        Cached for 5 minutes. Subsequent requests within 5m will return\n+        the cached response (no repeated DB queries or serialization).\n+        \"\"\"\n+        return super().list(request, *args, **kwargs)\n \n+    # ─── Cache the retrieve() action ──────────────────────────────────────\n+    @method_decorator(cache_page(CACHE_TTL), name=\"retrieve\")\n+    def retrieve(self, request, *args, **kwargs):\n+        \"\"\"\n+        GET /api/results/{pk}/\n+        Cached for 5 minutes per unique URL (i.e. per PK).\n+        \"\"\"\n+        return super().retrieve(request, *args, **kwargs)\n+\n     @action(detail=False, methods=[\"post\"], url_path=\"bulk-import\")\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         1) Expects multipart/form-data with:\n"
                },
                {
                    "date": 1749102883608,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,8 +7,10 @@\n import csv\n import io\n import json\n \n+from django.utils.decorators import method_decorator\n+from django.views.decorators.cache import cache_page\n from rest_framework import viewsets, status\n from rest_framework.decorators import action\n from rest_framework.parsers import MultiPartParser, JSONParser\n from rest_framework.response import Response\n"
                },
                {
                    "date": 1749102889725,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,9 +65,9 @@\n         Cached for 5 minutes per unique URL (i.e. per PK).\n         \"\"\"\n         return super().retrieve(request, *args, **kwargs)\n \n-    @action(detail=False, methods=[\"post\"], url_path=\"bulk-import\")\n+    @action(detail=False, methods=[\"post\"], url_path=\"bulk-import\",perm)\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         1) Expects multipart/form-data with:\n            - 'file': an uploaded CSV or XLSX/XLS\n"
                },
                {
                    "date": 1749102896830,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -65,9 +65,9 @@\n         Cached for 5 minutes per unique URL (i.e. per PK).\n         \"\"\"\n         return super().retrieve(request, *args, **kwargs)\n \n-    @action(detail=False, methods=[\"post\"], url_path=\"bulk-import\",perm)\n+    @action(detail=False, methods=[\"post\"], url_path=\"bulk-import\",permission_classes=[I])\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         1) Expects multipart/form-data with:\n            - 'file': an uploaded CSV or XLSX/XLS\n"
                },
                {
                    "date": 1749102904480,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n from ..utilities.importbase import *\n import csv\n import io\n import json\n-\n+from rest\n from django.utils.decorators import method_decorator\n from django.views.decorators.cache import cache_page\n from rest_framework import viewsets, status\n from rest_framework.decorators import action\n@@ -65,9 +65,9 @@\n         Cached for 5 minutes per unique URL (i.e. per PK).\n         \"\"\"\n         return super().retrieve(request, *args, **kwargs)\n \n-    @action(detail=False, methods=[\"post\"], url_path=\"bulk-import\",permission_classes=[I])\n+    @action(detail=False, methods=[\"post\"], url_path=\"bulk-import\",permission_classes=[IsAuthenticated])\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         1) Expects multipart/form-data with:\n            - 'file': an uploaded CSV or XLSX/XLS\n"
                },
                {
                    "date": 1749103215152,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n from ..utilities.importbase import *\n import csv\n import io\n import json\n-from rest\n+from rest_framework.permissions import IsAuthenticated\n from django.utils.decorators import method_decorator\n from django.views.decorators.cache import cache_page\n from rest_framework import viewsets, status\n from rest_framework.decorators import action\n@@ -25,9 +25,9 @@\n )\n from ..utilities.permissions import resultmanagementPermission\n from ..utilities.pagination import MyPageNumberPagination\n \n-CACHE_TTL = 60 * 5  # cache for 5 minutes (300 seconds)\n+CACHE_TTL = 60 * 60 * 24 * 5   # cache for 5 minutes (300 seconds)\n \n \n class ResultViewSet(viewsets.ModelViewSet):\n     \"\"\"\n"
                },
                {
                    "date": 1749103223107,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,9 +25,9 @@\n )\n from ..utilities.permissions import resultmanagementPermission\n from ..utilities.pagination import MyPageNumberPagination\n \n-CACHE_TTL = 60 * 60 * 24 * 5   # cache for 5 minutes (300 seconds)\n+CACHE_TTL = 60 * 60 * 24 * 5   # cache for 5 days (300 seconds)\n \n \n class ResultViewSet(viewsets.ModelViewSet):\n     \"\"\"\n"
                },
                {
                    "date": 1749103243157,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,9 +15,9 @@\n from rest_framework.parsers import MultiPartParser, JSONParser\n from rest_framework.response import Response\n from django.db import transaction\n from openpyxl import load_workbook  # for .xlsx/.xls support\n-\n+from main\n from ..models import File, Result\n from ..serializers import (\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n@@ -25,9 +25,9 @@\n )\n from ..utilities.permissions import resultmanagementPermission\n from ..utilities.pagination import MyPageNumberPagination\n \n-CACHE_TTL = 60 * 60 * 24 * 5   # cache for 5 days (300 seconds)\n+CACHE_TTL = 60 * 60 * 24 * 5   # cache for 5 days \n \n \n class ResultViewSet(viewsets.ModelViewSet):\n     \"\"\"\n"
                },
                {
                    "date": 1749103264543,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,9 +15,9 @@\n from rest_framework.parsers import MultiPartParser, JSONParser\n from rest_framework.response import Response\n from django.db import transaction\n from openpyxl import load_workbook  # for .xlsx/.xls support\n-from main\n+from mainproj.permissions import DynamicModelPermission\n from ..models import File, Result\n from ..serializers import (\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n@@ -35,9 +35,9 @@\n     plus a custom POST /api/results/bulk-import/ for batch‐importing a CSV/Excel file.\n     \"\"\"\n     queryset = Result.objects.all().order_by(\"symbol_no\")\n     serializer_class = ResultListSerializers\n-    permission_classes = [resultmanagementPermission]\n+    permission_classes = [DynamicModelPermission]\n     pagination_class = MyPageNumberPagination\n     parser_classes = [MultiPartParser, JSONParser]\n \n     def get_serializer_class(self):\n"
                },
                {
                    "date": 1749103295329,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,9 @@\n \n CACHE_TTL = 60 * 60 * 24 * 5   # cache for 5 days \n \n \n-class ResultViewSet(viewsets.ModelViewSet):\n+class res(viewsets.ModelViewSet):\n     \"\"\"\n     Exposes /api/results/ for standard list/retrieve/create/update/delete,\n     plus a custom POST /api/results/bulk-import/ for batch‐importing a CSV/Excel file.\n     \"\"\"\n"
                },
                {
                    "date": 1749103301257,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,9 @@\n \n CACHE_TTL = 60 * 60 * 24 * 5   # cache for 5 days \n \n \n-class res(viewsets.ModelViewSet):\n+class resultViewsets(viewsets.ModelViewSet):\n     \"\"\"\n     Exposes /api/results/ for standard list/retrieve/create/update/delete,\n     plus a custom POST /api/results/bulk-import/ for batch‐importing a CSV/Excel file.\n     \"\"\"\n"
                },
                {
                    "date": 1749103509380,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n from django.db import transaction\n from openpyxl import load_workbook  # for .xlsx/.xls support\n from mainproj.permissions import DynamicModelPermission\n from ..models import File, Result\n-from ..serializers import (\n+from ..serializers. import (\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n )\n"
                },
                {
                    "date": 1749104356921,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n from django.db import transaction\n from openpyxl import load_workbook  # for .xlsx/.xls support\n from mainproj.permissions import DynamicModelPermission\n from ..models import File, Result\n-from ..serializers. import (\n+from ..serializers.result_serializers import (\n     ResultListSerializers,\n     ResultRetrieveSerializers,\n     ResultWriteSerializers\n )\n@@ -35,9 +35,9 @@\n     plus a custom POST /api/results/bulk-import/ for batch‐importing a CSV/Excel file.\n     \"\"\"\n     queryset = Result.objects.all().order_by(\"symbol_no\")\n     serializer_class = ResultListSerializers\n-    permission_classes = [DynamicModelPermission]\n+    # permission_classes = [DynamicModelPermission]\n     pagination_class = MyPageNumberPagination\n     parser_classes = [MultiPartParser, JSONParser]\n \n     def get_serializer_class(self):\n"
                },
                {
                    "date": 1749104498862,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,84 +46,71 @@\n         elif self.action == \"retrieve\":\n             return ResultRetrieveSerializers\n         return super().get_serializer_class()\n     \n-    # ─── Cache the list() action ─────────────────────────────────────────\n-    @method_decorator(cache_page(CACHE_TTL), name=\"list\")\n+   @method_decorator(cache_page(CACHE_TTL), name=\"list\")\n     def list(self, request, *args, **kwargs):\n-        \"\"\"\n-        GET /api/results/\n-        Cached for 5 minutes. Subsequent requests within 5m will return\n-        the cached response (no repeated DB queries or serialization).\n-        \"\"\"\n         return super().list(request, *args, **kwargs)\n \n-    # ─── Cache the retrieve() action ──────────────────────────────────────\n     @method_decorator(cache_page(CACHE_TTL), name=\"retrieve\")\n     def retrieve(self, request, *args, **kwargs):\n-        \"\"\"\n-        GET /api/results/{pk}/\n-        Cached for 5 minutes per unique URL (i.e. per PK).\n-        \"\"\"\n         return super().retrieve(request, *args, **kwargs)\n \n-    @action(detail=False, methods=[\"post\"], url_path=\"bulk-import\",permission_classes=[IsAuthenticated])\n+    @action(\n+        detail=False,\n+        methods=[\"post\"],\n+        url_path=\"bulk-import\",\n+        permission_classes=[IsAuthenticated],\n+    )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n-        1) Expects multipart/form-data with:\n-           - 'file': an uploaded CSV or XLSX/XLS\n-           - 'name': a string (the File Name, e.g. \"SEE Result 2081\")\n-           - 'mapping': JSON-encoded string, e.g.\n-               {\n-                 \"symbol_no\": \"RollNo\",\n-                 \"dateofbirth\": \"DOB\",\n-                 \"cgpa\": \"CGPA\",\n-                 \"remarks\": \"Remarks\"\n-               }\n+        Now accepts JSON body with:\n+          - file_id: integer (existing File PK)\n+          - mapping: {\"symbol_no\":\"<col>\",\"dateofbirth\":\"<col>\",\"cgpa\":\"<col>\",\"remarks\":\"<col>\"}\n \n-        2) Saves File(...) → file_obj\n-        3) Streams row-by-row, doing bulk_create([...]) in batches of 5 000\n-        4) Returns only {\"message\": \"File processed and data saved to database.\"} on success.\n+        It reads File.objects.get(pk=file_id).file on disk, streams through its rows,\n+        and bulk-creates Result(...) in batches of 5000.\n         \"\"\"\n \n         # ─── 1) Validate Inputs ───────────────────────────────────────────\n-        uploaded_file = request.FILES.get(\"file\", None)\n-        name_value   = request.data.get(\"name\", \"\").strip()\n+        file_id = request.data.get(\"file_id\", None)\n         mapping_json = request.data.get(\"mapping\", None)\n \n-        if not uploaded_file:\n+        if file_id is None:\n             return Response(\n-                {\"error\": \"No file was uploaded. Please include your CSV/XLSX under key 'file'.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n+                {\"error\": \"Missing 'file_id'. Please send the ID of an existing File.\"},\n+                status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        if not name_value:\n+        try:\n+            file_obj = File.objects.get(pk=file_id)\n+        except File.DoesNotExist:\n             return Response(\n-                {\"error\": \"Missing 'name'. Please send a nonempty File Name.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n+                {\"error\": f\"File with ID={file_id} not found.\"},\n+                status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         if mapping_json is None:\n             return Response(\n                 {\"error\": \"Missing 'mapping'. Please send a JSON object under key 'mapping'.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n+                status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 2) Parse mapping JSON (it might be a string) ───────────────\n+        # ─── 2) Parse mapping JSON ─────────────────────────────────────────\n         if isinstance(mapping_json, str):\n             try:\n                 mapping = json.loads(mapping_json)\n             except json.JSONDecodeError as e:\n                 return Response(\n                     {\"error\": f\"Invalid JSON in 'mapping': {str(e)}\"},\n-                    status=status.HTTP_400_BAD_REQUEST\n+                    status=status.HTTP_400_BAD_REQUEST,\n                 )\n         elif isinstance(mapping_json, dict):\n             mapping = mapping_json\n         else:\n             return Response(\n                 {\"error\": \"'mapping' must be a JSON‐encoded string or JSON object.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n+                status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         # ─── 3) Ensure exactly the four keys we expect ───────────────────\n         required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n@@ -134,30 +121,24 @@\n                         \"'mapping' must contain exactly these keys: \"\n                         \"symbol_no, dateofbirth, cgpa, remarks\"\n                     )\n                 },\n-                status=status.HTTP_400_BAD_REQUEST\n+                status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 4) Detect file type by extension ────────────────────────────\n-        filename_lower = uploaded_file.name.lower()\n+        # ─── 4) Determine file type by extension ──────────────────────────\n+        filename_lower = file_obj.file.name.lower()\n         if filename_lower.endswith(\".csv\"):\n             file_type = \"csv\"\n         elif filename_lower.endswith((\".xlsx\", \".xls\")):\n             file_type = \"excel\"\n         else:\n             return Response(\n                 {\"error\": \"Unsupported file format. Only .csv, .xlsx, .xls are allowed.\"},\n-                status=status.HTTP_400_BAD_REQUEST\n+                status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 5) Save a new File(...) record ─────────────────────────────\n-        file_obj = File.objects.create(\n-            file=uploaded_file,\n-            is_active=True\n-        )\n-\n-        # ─── 6) Stream + Bulk-Insert into Result ─────────────────────────\n+        # ─── 5) Stream + Bulk-Insert into Result ─────────────────────────\n         BATCH_SIZE = 5000\n         instances_to_create = []\n \n         def normalize_header(hdr: str) -> str:\n@@ -165,122 +146,114 @@\n \n         try:\n             with transaction.atomic():\n                 if file_type == \"csv\":\n-                    # a) Wrap the underlying file so csv.DictReader can read it\n-                    text_stream = io.TextIOWrapper(uploaded_file.file, encoding=\"utf-8\", newline=\"\")\n-                    reader = csv.DictReader(text_stream)\n+                    # a) Open the stored file on disk for reading as text\n+                    file_path = file_obj.file.path\n+                    with open(file_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n+                        reader = csv.DictReader(f)\n \n-                    # b) Verify that each mapping value is indeed a header in the CSV\n-                    csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n-                    for field_key, col_name in mapping.items():\n-                        if normalize_header(col_name) not in csv_headers:\n-                            raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n+                        # b) Verify that each mapping value is a header in the CSV\n+                        csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n+                        for _, col_name in mapping.items():\n+                            if normalize_header(col_name) not in csv_headers:\n+                                raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n \n-                    # c) Iterate every row in the CSV\n-                    for row in reader:\n-                        try:\n-                            # Build the exact four data‐fields plus name/file/mapped_json\n-                            instances_to_create.append(\n-                                Result(\n-                                    name=name_value,\n-                                    mapped_json=mapping,\n-                                    file=file_obj,\n-                                    symbol_no=row[ normalize_header(mapping[\"symbol_no\"]) ].strip(),\n-                                    dateofbirth=row[ normalize_header(mapping[\"dateofbirth\"]) ].strip(),\n-                                    cgpa=row[ normalize_header(mapping[\"cgpa\"]) ].strip(),\n-                                    remarks=row[ normalize_header(mapping[\"remarks\"]) ].strip(),\n+                        # c) Iterate every row\n+                        for row in reader:\n+                            try:\n+                                instances_to_create.append(\n+                                    Result(\n+                                        name=file_obj.file.name,       # use file name as 'name'\n+                                        mapped_json=mapping,\n+                                        file=file_obj,\n+                                        symbol_no=row[normalize_header(mapping[\"symbol_no\"])].strip(),\n+                                        dateofbirth=row[normalize_header(mapping[\"dateofbirth\"])].strip(),\n+                                        cgpa=row[normalize_header(mapping[\"cgpa\"])].strip(),\n+                                        remarks=row[normalize_header(mapping[\"remarks\"])].strip(),\n+                                    )\n                                 )\n-                            )\n-                        except Exception:\n-                            # Skip any malformed row—do not abort the whole import\n-                            continue\n+                            except Exception:\n+                                continue\n \n-                        # d) Once we have BATCH_SIZE items, flush them in one bulk_create\n-                        if len(instances_to_create) >= BATCH_SIZE:\n+                            if len(instances_to_create) >= BATCH_SIZE:\n+                                Result.objects.bulk_create(instances_to_create)\n+                                instances_to_create = []\n+\n+                        if instances_to_create:\n                             Result.objects.bulk_create(instances_to_create)\n                             instances_to_create = []\n \n-                    # e) Flush any leftover rows\n-                    if instances_to_create:\n-                        Result.objects.bulk_create(instances_to_create)\n-                        instances_to_create = []\n-\n                 else:\n                     # 7) Excel path: use openpyxl in read_only mode\n-                    wb = load_workbook(filename=uploaded_file.file, read_only=True, data_only=True)\n-                    sheet = wb.active  # or wb[some_sheet_name] if you need a specific sheet\n+                    file_path = file_obj.file.path\n+                    wb = load_workbook(filename=file_path, read_only=True, data_only=True)\n+                    sheet = wb.active\n                     excel_iter = sheet.iter_rows(values_only=True)\n \n-                    # a) Read the header row (first row)\n+                    # a) Read header row\n                     header_row = next(excel_iter, None)\n                     if not header_row:\n-                        raise KeyError(\"Excel file appears to be empty (no header row).\")\n+                        raise KeyError(\"Excel file appears to be empty (no header).\")\n \n-                    # b) Build a map { header_text : column_index }\n+                    # b) Build a map { header_text: column_index }\n                     header_map = {\n                         normalize_header(cell_val): idx\n                         for idx, cell_val in enumerate(header_row)\n                         if cell_val is not None\n                     }\n \n-                    # c) Verify that the user’s mapping values exist in that header_map\n-                    for field_key, col_name in mapping.items():\n+                    # c) Verify mapping values exist in header_map\n+                    for _, col_name in mapping.items():\n                         if normalize_header(col_name) not in header_map:\n                             raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n \n-                    # d) Iterate each subsequent row (row_index starts at 2)\n-                    for row_index, row_tuple in enumerate(excel_iter, start=2):\n+                    # d) Iterate subsequent rows\n+                    for row_tuple in excel_iter:\n                         try:\n-                            sn_idx   = header_map[ normalize_header(mapping[\"symbol_no\"]) ]\n-                            dob_idx  = header_map[ normalize_header(mapping[\"dateofbirth\"]) ]\n-                            cgpa_idx = header_map[ normalize_header(mapping[\"cgpa\"]) ]\n-                            rm_idx   = header_map[ normalize_header(mapping[\"remarks\"]) ]\n+                            sn_idx = header_map[normalize_header(mapping[\"symbol_no\"])]\n+                            dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n+                            cgpa_idx = header_map[normalize_header(mapping[\"cgpa\"])]\n+                            rm_idx = header_map[normalize_header(mapping[\"remarks\"])]\n \n                             def get_cell(r, i):\n                                 return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n \n                             instances_to_create.append(\n                                 Result(\n-                                    name=name_value,\n+                                    name=file_obj.file.name,\n                                     mapped_json=mapping,\n                                     file=file_obj,\n-                                    symbol_no   = get_cell(row_tuple, sn_idx),\n-                                    dateofbirth = get_cell(row_tuple, dob_idx),\n-                                    cgpa        = get_cell(row_tuple, cgpa_idx),\n-                                    remarks     = get_cell(row_tuple, rm_idx),\n+                                    symbol_no=get_cell(row_tuple, sn_idx),\n+                                    dateofbirth=get_cell(row_tuple, dob_idx),\n+                                    cgpa=get_cell(row_tuple, cgpa_idx),\n+                                    remarks=get_cell(row_tuple, rm_idx),\n                                 )\n                             )\n                         except Exception:\n-                            # Skip this row if anything goes wrong (e.g. index error, type error)\n                             continue\n \n                         if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n                             instances_to_create = []\n \n-                    # e) Flush any leftover rows\n                     if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n                         instances_to_create = []\n \n                     wb.close()\n \n         except KeyError as ke:\n-            # A required column was missing: abort entire import, roll back\n             return Response(\n                 {\"error\": f\"Mapping error: {str(ke)}\"},\n-                status=status.HTTP_400_BAD_REQUEST\n+                status=status.HTTP_400_BAD_REQUEST,\n             )\n-\n         except Exception as e:\n-            # Any other unexpected error: roll back + 500\n             return Response(\n                 {\"error\": f\"Server error while importing: {str(e)}\"},\n-                status=status.HTTP_500_INTERNAL_SERVER_ERROR\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n             )\n \n-        # ─── 8) On success, return only a minimal message ─────────────────\n         return Response(\n             {\"message\": \"File processed and data saved to database.\"},\n-            status=status.HTTP_200_OK\n-        )\n+            status=status.HTTP_200_OK,\n+        )\n\\ No newline at end of file\n"
                },
                {
                    "date": 1749104504186,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,9 +46,9 @@\n         elif self.action == \"retrieve\":\n             return ResultRetrieveSerializers\n         return super().get_serializer_class()\n     \n-   @method_decorator(cache_page(CACHE_TTL), name=\"list\")\n+    @method_decorator(cache_page(CACHE_TTL), name=\"list\")\n     def list(self, request, *args, **kwargs):\n         return super().list(request, *args, **kwargs)\n \n     @method_decorator(cache_page(CACHE_TTL), name=\"retrieve\")\n"
                },
                {
                    "date": 1749104603802,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -58,9 +58,9 @@\n     @action(\n         detail=False,\n         methods=[\"post\"],\n         url_path=\"bulk-import\",\n-        permission_classes=[IsAuthenticated],\n+        # permission_classes=[IsAuthenticated],\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         Now accepts JSON body with:\n"
                },
                {
                    "date": 1749104674181,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,8 @@\n     queryset = Result.objects.all().order_by(\"symbol_no\")\n     serializer_class = ResultListSerializers\n     # permission_classes = [DynamicModelPermission]\n     pagination_class = MyPageNumberPagination\n-    parser_classes = [MultiPartParser, JSONParser]\n \n     def get_serializer_class(self):\n         if self.action in [\"create\", \"update\", \"partial_update\"]:\n             return ResultWriteSerializers\n"
                },
                {
                    "date": 1749105051972,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,27 +57,32 @@\n     @action(\n         detail=False,\n         methods=[\"post\"],\n         url_path=\"bulk-import\",\n-        # permission_classes=[IsAuthenticated],\n+        permission_classes=[IsAuthenticated],\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n-        Now accepts JSON body with:\n-          - file_id: integer (existing File PK)\n-          - mapping: {\"symbol_no\":\"<col>\",\"dateofbirth\":\"<col>\",\"cgpa\":\"<col>\",\"remarks\":\"<col>\"}\n+        Now expects JSON body:\n+          {\n+            \"file_id\": <existing File PK>,\n+            \"mapping\": {\n+                \"symbol_no\": \"<column_name>\",\n+                \"dateofbirth\": \"<column_name>\",\n+                \"cgpa\": \"<column_name>\",\n+                \"remarks\": \"<column_name>\"\n+            }\n+          }\n \n-        It reads File.objects.get(pk=file_id).file on disk, streams through its rows,\n-        and bulk-creates Result(...) in batches of 5000.\n+        It will open the stored File.file as a file‐like object,\n+        stream through its rows, and bulk_insert into Result.\n         \"\"\"\n+        file_id = request.data.get(\"file_id\")\n+        mapping_json = request.data.get(\"mapping\")\n \n-        # ─── 1) Validate Inputs ───────────────────────────────────────────\n-        file_id = request.data.get(\"file_id\", None)\n-        mapping_json = request.data.get(\"mapping\", None)\n-\n         if file_id is None:\n             return Response(\n-                {\"error\": \"Missing 'file_id'. Please send the ID of an existing File.\"},\n+                {\"error\": \"Missing 'file_id'. Send an existing File ID.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         try:\n@@ -89,13 +94,13 @@\n             )\n \n         if mapping_json is None:\n             return Response(\n-                {\"error\": \"Missing 'mapping'. Please send a JSON object under key 'mapping'.\"},\n+                {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 2) Parse mapping JSON ─────────────────────────────────────────\n+        # Parse mapping JSON\n         if isinstance(mapping_json, str):\n             try:\n                 mapping = json.loads(mapping_json)\n             except json.JSONDecodeError as e:\n@@ -106,13 +111,12 @@\n         elif isinstance(mapping_json, dict):\n             mapping = mapping_json\n         else:\n             return Response(\n-                {\"error\": \"'mapping' must be a JSON‐encoded string or JSON object.\"},\n+                {\"error\": \"'mapping' must be a JSON string or object.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 3) Ensure exactly the four keys we expect ───────────────────\n         required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n@@ -123,21 +127,20 @@\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 4) Determine file type by extension ──────────────────────────\n+        # Determine CSV vs. Excel by the stored filename’s extension\n         filename_lower = file_obj.file.name.lower()\n         if filename_lower.endswith(\".csv\"):\n             file_type = \"csv\"\n         elif filename_lower.endswith((\".xlsx\", \".xls\")):\n             file_type = \"excel\"\n         else:\n             return Response(\n-                {\"error\": \"Unsupported file format. Only .csv, .xlsx, .xls are allowed.\"},\n+                {\"error\": \"Unsupported file format. Must be .csv, .xlsx, or .xls.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 5) Stream + Bulk-Insert into Result ─────────────────────────\n         BATCH_SIZE = 5000\n         instances_to_create = []\n \n         def normalize_header(hdr: str) -> str:\n@@ -145,69 +148,71 @@\n \n         try:\n             with transaction.atomic():\n                 if file_type == \"csv\":\n-                    # a) Open the stored file on disk for reading as text\n-                    file_path = file_obj.file.path\n-                    with open(file_path, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n-                        reader = csv.DictReader(f)\n+                    # ─── CSV path: open the file‐like object for reading\n+                    file_obj.file.open(\"r\")  # ensure the file is opened\n+                    # Wrap in TextIOWrapper so csv.DictReader reads text\n+                    text_stream = io.TextIOWrapper(\n+                        file_obj.file, encoding=\"utf-8\", newline=\"\"\n+                    )\n+                    reader = csv.DictReader(text_stream)\n \n-                        # b) Verify that each mapping value is a header in the CSV\n-                        csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n-                        for _, col_name in mapping.items():\n-                            if normalize_header(col_name) not in csv_headers:\n-                                raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n+                    # Verify header row\n+                    csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n+                    for _, col_name in mapping.items():\n+                        if normalize_header(col_name) not in csv_headers:\n+                            raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n \n-                        # c) Iterate every row\n-                        for row in reader:\n-                            try:\n-                                instances_to_create.append(\n-                                    Result(\n-                                        name=file_obj.file.name,       # use file name as 'name'\n-                                        mapped_json=mapping,\n-                                        file=file_obj,\n-                                        symbol_no=row[normalize_header(mapping[\"symbol_no\"])].strip(),\n-                                        dateofbirth=row[normalize_header(mapping[\"dateofbirth\"])].strip(),\n-                                        cgpa=row[normalize_header(mapping[\"cgpa\"])].strip(),\n-                                        remarks=row[normalize_header(mapping[\"remarks\"])].strip(),\n-                                    )\n+                    # Iterate each row\n+                    for row in reader:\n+                        try:\n+                            instances_to_create.append(\n+                                Result(\n+                                    name=file_obj.file.name,\n+                                    mapped_json=mapping,\n+                                    file=file_obj,\n+                                    symbol_no=row[normalize_header(mapping[\"symbol_no\"])].strip(),\n+                                    dateofbirth=row[normalize_header(mapping[\"dateofbirth\"])].strip(),\n+                                    cgpa=row[normalize_header(mapping[\"cgpa\"])].strip(),\n+                                    remarks=row[normalize_header(mapping[\"remarks\"])].strip(),\n                                 )\n-                            except Exception:\n-                                continue\n+                            )\n+                        except Exception:\n+                            continue\n \n-                            if len(instances_to_create) >= BATCH_SIZE:\n-                                Result.objects.bulk_create(instances_to_create)\n-                                instances_to_create = []\n-\n-                        if instances_to_create:\n+                        if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n                             instances_to_create = []\n \n+                    if instances_to_create:\n+                        Result.objects.bulk_create(instances_to_create)\n+                        instances_to_create = []\n+\n+                    # Close the underlying file\n+                    file_obj.file.close()\n+\n                 else:\n-                    # 7) Excel path: use openpyxl in read_only mode\n-                    file_path = file_obj.file.path\n-                    wb = load_workbook(filename=file_path, read_only=True, data_only=True)\n+                    # ─── Excel path: open the file‐like object, read into load_workbook\n+                    file_obj.file.open(\"rb\")  # open in bytes mode\n+                    wb = load_workbook(filename=file_obj.file, read_only=True, data_only=True)\n                     sheet = wb.active\n                     excel_iter = sheet.iter_rows(values_only=True)\n \n-                    # a) Read header row\n                     header_row = next(excel_iter, None)\n                     if not header_row:\n-                        raise KeyError(\"Excel file appears to be empty (no header).\")\n+                        raise KeyError(\"Excel file appears empty (no header).\")\n \n-                    # b) Build a map { header_text: column_index }\n                     header_map = {\n                         normalize_header(cell_val): idx\n                         for idx, cell_val in enumerate(header_row)\n                         if cell_val is not None\n                     }\n \n-                    # c) Verify mapping values exist in header_map\n                     for _, col_name in mapping.items():\n                         if normalize_header(col_name) not in header_map:\n                             raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n \n-                    # d) Iterate subsequent rows\n                     for row_tuple in excel_iter:\n                         try:\n                             sn_idx = header_map[normalize_header(mapping[\"symbol_no\"])]\n                             dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n@@ -239,8 +244,9 @@\n                         Result.objects.bulk_create(instances_to_create)\n                         instances_to_create = []\n \n                     wb.close()\n+                    file_obj.file.close()\n \n         except KeyError as ke:\n             return Response(\n                 {\"error\": f\"Mapping error: {str(ke)}\"},\n"
                },
                {
                    "date": 1749105137072,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,211 +54,209 @@\n     def retrieve(self, request, *args, **kwargs):\n         return super().retrieve(request, *args, **kwargs)\n \n     @action(\n-        detail=False,\n-        methods=[\"post\"],\n-        url_path=\"bulk-import\",\n-        permission_classes=[IsAuthenticated],\n-    )\n-    def bulk_import(self, request, *args, **kwargs):\n-        \"\"\"\n-        Now expects JSON body:\n-          {\n-            \"file_id\": <existing File PK>,\n-            \"mapping\": {\n-                \"symbol_no\": \"<column_name>\",\n-                \"dateofbirth\": \"<column_name>\",\n-                \"cgpa\": \"<column_name>\",\n-                \"remarks\": \"<column_name>\"\n-            }\n-          }\n+    detail=False,\n+    methods=[\"post\"],\n+    url_path=\"bulk-import\",\n+    permission_classes=[IsAuthenticated],\n+)\n+def bulk_import(self, request, *args, **kwargs):\n+    \"\"\"\n+    Expects JSON body with:\n+      {\n+        \"file_id\": <existing File PK>,\n+        \"mapping\": {\n+            \"symbol_no\": \"<column_name>\",\n+            \"dateofbirth\": \"<column_name>\",\n+            \"cgpa\": \"<column_name>\",\n+            \"remarks\": \"<column_name>\"\n+        }\n+      }\n \n-        It will open the stored File.file as a file‐like object,\n-        stream through its rows, and bulk_insert into Result.\n-        \"\"\"\n-        file_id = request.data.get(\"file_id\")\n-        mapping_json = request.data.get(\"mapping\")\n+    Streams through the stored File.file (CSV or Excel) and bulk-creates Result rows.\n+    \"\"\"\n+    file_id = request.data.get(\"file_id\")\n+    mapping_json = request.data.get(\"mapping\")\n \n-        if file_id is None:\n-            return Response(\n-                {\"error\": \"Missing 'file_id'. Send an existing File ID.\"},\n-                status=status.HTTP_400_BAD_REQUEST,\n-            )\n+    # 1) Validate file_id\n+    if file_id is None:\n+        return Response(\n+            {\"error\": \"Missing 'file_id'. Please send an existing File ID.\"},\n+            status=status.HTTP_400_BAD_REQUEST,\n+        )\n \n+    try:\n+        file_obj = File.objects.get(pk=file_id)\n+    except File.DoesNotExist:\n+        return Response(\n+            {\"error\": f\"File with ID={file_id} not found.\"},\n+            status=status.HTTP_400_BAD_REQUEST,\n+        )\n+\n+    # 2) Parse mapping\n+    if mapping_json is None:\n+        return Response(\n+            {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n+            status=status.HTTP_400_BAD_REQUEST,\n+        )\n+\n+    if isinstance(mapping_json, str):\n         try:\n-            file_obj = File.objects.get(pk=file_id)\n-        except File.DoesNotExist:\n+            mapping = json.loads(mapping_json)\n+        except json.JSONDecodeError as e:\n             return Response(\n-                {\"error\": f\"File with ID={file_id} not found.\"},\n+                {\"error\": f\"Invalid JSON in 'mapping': {str(e)}\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n+    elif isinstance(mapping_json, dict):\n+        mapping = mapping_json\n+    else:\n+        return Response(\n+            {\"error\": \"'mapping' must be a JSON string or object.\"},\n+            status=status.HTTP_400_BAD_REQUEST,\n+        )\n \n-        if mapping_json is None:\n-            return Response(\n-                {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n-                status=status.HTTP_400_BAD_REQUEST,\n-            )\n-\n-        # Parse mapping JSON\n-        if isinstance(mapping_json, str):\n-            try:\n-                mapping = json.loads(mapping_json)\n-            except json.JSONDecodeError as e:\n-                return Response(\n-                    {\"error\": f\"Invalid JSON in 'mapping': {str(e)}\"},\n-                    status=status.HTTP_400_BAD_REQUEST,\n+    # 3) Ensure exactly four keys\n+    required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+    if set(mapping.keys()) != required_keys:\n+        return Response(\n+            {\n+                \"error\": (\n+                    \"'mapping' must contain exactly these keys: \"\n+                    \"symbol_no, dateofbirth, cgpa, remarks\"\n                 )\n-        elif isinstance(mapping_json, dict):\n-            mapping = mapping_json\n-        else:\n-            return Response(\n-                {\"error\": \"'mapping' must be a JSON string or object.\"},\n-                status=status.HTTP_400_BAD_REQUEST,\n-            )\n+            },\n+            status=status.HTTP_400_BAD_REQUEST,\n+        )\n \n-        required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n-        if set(mapping.keys()) != required_keys:\n-            return Response(\n-                {\n-                    \"error\": (\n-                        \"'mapping' must contain exactly these keys: \"\n-                        \"symbol_no, dateofbirth, cgpa, remarks\"\n-                    )\n-                },\n-                status=status.HTTP_400_BAD_REQUEST,\n-            )\n+    # 4) Determine file type by extension\n+    filename_lower = file_obj.file.name.lower()\n+    if filename_lower.endswith(\".csv\"):\n+        file_type = \"csv\"\n+    elif filename_lower.endswith((\".xlsx\", \".xls\")):\n+        file_type = \"excel\"\n+    else:\n+        return Response(\n+            {\"error\": \"Unsupported file format. Must be .csv, .xlsx, or .xls.\"},\n+            status=status.HTTP_400_BAD_REQUEST,\n+        )\n \n-        # Determine CSV vs. Excel by the stored filename’s extension\n-        filename_lower = file_obj.file.name.lower()\n-        if filename_lower.endswith(\".csv\"):\n-            file_type = \"csv\"\n-        elif filename_lower.endswith((\".xlsx\", \".xls\")):\n-            file_type = \"excel\"\n-        else:\n-            return Response(\n-                {\"error\": \"Unsupported file format. Must be .csv, .xlsx, or .xls.\"},\n-                status=status.HTTP_400_BAD_REQUEST,\n-            )\n+    BATCH_SIZE = 5000\n+    instances_to_create = []\n \n-        BATCH_SIZE = 5000\n-        instances_to_create = []\n+    def normalize_header(hdr: str) -> str:\n+        return str(hdr).strip()\n \n-        def normalize_header(hdr: str) -> str:\n-            return str(hdr).strip()\n+    try:\n+        with transaction.atomic():\n+            if file_type == \"csv\":\n+                # ─── CSV path: open in binary then wrap for text\n+                file_obj.file.open(\"rb\")\n+                text_stream = io.TextIOWrapper(file_obj.file, encoding=\"utf-8\", newline=\"\")\n+                reader = csv.DictReader(text_stream)\n \n-        try:\n-            with transaction.atomic():\n-                if file_type == \"csv\":\n-                    # ─── CSV path: open the file‐like object for reading\n-                    file_obj.file.open(\"r\")  # ensure the file is opened\n-                    # Wrap in TextIOWrapper so csv.DictReader reads text\n-                    text_stream = io.TextIOWrapper(\n-                        file_obj.file, encoding=\"utf-8\", newline=\"\"\n-                    )\n-                    reader = csv.DictReader(text_stream)\n+                # Verify header row\n+                csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n+                for _, col_name in mapping.items():\n+                    if normalize_header(col_name) not in csv_headers:\n+                        raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n \n-                    # Verify header row\n-                    csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n-                    for _, col_name in mapping.items():\n-                        if normalize_header(col_name) not in csv_headers:\n-                            raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n-\n-                    # Iterate each row\n-                    for row in reader:\n-                        try:\n-                            instances_to_create.append(\n-                                Result(\n-                                    name=file_obj.file.name,\n-                                    mapped_json=mapping,\n-                                    file=file_obj,\n\\ No newline at end of file\n-                                    symbol_no=row[normalize_header(mapping[\"symbol_no\"])].strip(),\n-                                    dateofbirth=row[normalize_header(mapping[\"dateofbirth\"])].strip(),\n-                                    cgpa=row[normalize_header(mapping[\"cgpa\"])].strip(),\n-                                    remarks=row[normalize_header(mapping[\"remarks\"])].strip(),\n-                                )\n+                # Iterate each row\n+                for row in reader:\n+                    try:\n+                        instances_to_create.append(\n+                            Result(\n+                                name=file_obj.file.name,\n+                                mapped_json=mapping,\n+                                file=file_obj,\n+                                symbol_no=row[normalize_header(mapping[\"symbol_no\"])].strip(),\n+                                dateofbirth=row[normalize_header(mapping[\"dateofbirth\"])].strip(),\n+                                cgpa=row[normalize_header(mapping[\"cgpa\"])].strip(),\n+                                remarks=row[normalize_header(mapping[\"remarks\"])].strip(),\n                             )\n-                        except Exception:\n-                            continue\n+                        )\n+                    except Exception:\n+                        # skip malformed rows\n+                        continue\n \n-                        if len(instances_to_create) >= BATCH_SIZE:\n-                            Result.objects.bulk_create(instances_to_create)\n-                            instances_to_create = []\n-\n-                    if instances_to_create:\n+                    if len(instances_to_create) >= BATCH_SIZE:\n                         Result.objects.bulk_create(instances_to_create)\n                         instances_to_create = []\n \n-                    # Close the underlying file\n-                    file_obj.file.close()\n+                if instances_to_create:\n+                    Result.objects.bulk_create(instances_to_create)\n+                    instances_to_create = []\n \n-                else:\n-                    # ─── Excel path: open the file‐like object, read into load_workbook\n-                    file_obj.file.open(\"rb\")  # open in bytes mode\n-                    wb = load_workbook(filename=file_obj.file, read_only=True, data_only=True)\n-                    sheet = wb.active\n-                    excel_iter = sheet.iter_rows(values_only=True)\n+                file_obj.file.close()\n \n-                    header_row = next(excel_iter, None)\n-                    if not header_row:\n-                        raise KeyError(\"Excel file appears empty (no header).\")\n+            else:\n+                # ─── Excel path: open in binary and pass to load_workbook\n+                file_obj.file.open(\"rb\")\n+                wb = load_workbook(filename=file_obj.file, read_only=True, data_only=True)\n+                sheet = wb.active\n+                excel_iter = sheet.iter_rows(values_only=True)\n \n-                    header_map = {\n-                        normalize_header(cell_val): idx\n-                        for idx, cell_val in enumerate(header_row)\n-                        if cell_val is not None\n-                    }\n+                header_row = next(excel_iter, None)\n+                if not header_row:\n+                    raise KeyError(\"Excel file appears empty (no header).\")\n \n-                    for _, col_name in mapping.items():\n-                        if normalize_header(col_name) not in header_map:\n-                            raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n+                header_map = {\n+                    normalize_header(cell_val): idx\n+                    for idx, cell_val in enumerate(header_row)\n+                    if cell_val is not None\n+                }\n \n-                    for row_tuple in excel_iter:\n-                        try:\n-                            sn_idx = header_map[normalize_header(mapping[\"symbol_no\"])]\n-                            dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n-                            cgpa_idx = header_map[normalize_header(mapping[\"cgpa\"])]\n-                            rm_idx = header_map[normalize_header(mapping[\"remarks\"])]\n+                for _, col_name in mapping.items():\n+                    if normalize_header(col_name) not in header_map:\n+                        raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n \n-                            def get_cell(r, i):\n-                                return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n+                for row_tuple in excel_iter:\n+                    try:\n+                        sn_idx = header_map[normalize_header(mapping[\"symbol_no\"])]\n+                        dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n+                        cgpa_idx = header_map[normalize_header(mapping[\"cgpa\"])]\n+                        rm_idx = header_map[normalize_header(mapping[\"remarks\"])]\n \n-                            instances_to_create.append(\n-                                Result(\n-                                    name=file_obj.file.name,\n-                                    mapped_json=mapping,\n-                                    file=file_obj,\n-                                    symbol_no=get_cell(row_tuple, sn_idx),\n-                                    dateofbirth=get_cell(row_tuple, dob_idx),\n-                                    cgpa=get_cell(row_tuple, cgpa_idx),\n-                                    remarks=get_cell(row_tuple, rm_idx),\n-                                )\n+                        def get_cell(r, i):\n+                            return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n+\n+                        instances_to_create.append(\n+                            Result(\n+                                name=file_obj.file.name,\n+                                mapped_json=mapping,\n+                                file=file_obj,\n+                                symbol_no=get_cell(row_tuple, sn_idx),\n+                                dateofbirth=get_cell(row_tuple, dob_idx),\n+                                cgpa=get_cell(row_tuple, cgpa_idx),\n+                                remarks=get_cell(row_tuple, rm_idx),\n                             )\n-                        except Exception:\n-                            continue\n+                        )\n+                    except Exception:\n+                        continue\n \n-                        if len(instances_to_create) >= BATCH_SIZE:\n-                            Result.objects.bulk_create(instances_to_create)\n-                            instances_to_create = []\n-\n-                    if instances_to_create:\n+                    if len(instances_to_create) >= BATCH_SIZE:\n                         Result.objects.bulk_create(instances_to_create)\n                         instances_to_create = []\n \n-                    wb.close()\n-                    file_obj.file.close()\n+                if instances_to_create:\n+                    Result.objects.bulk_create(instances_to_create)\n+                    instances_to_create = []\n \n-        except KeyError as ke:\n-            return Response(\n-                {\"error\": f\"Mapping error: {str(ke)}\"},\n-                status=status.HTTP_400_BAD_REQUEST,\n-            )\n-        except Exception as e:\n-            return Response(\n-                {\"error\": f\"Server error while importing: {str(e)}\"},\n-                status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n-            )\n+                wb.close()\n+                file_obj.file.close()\n \n+    except KeyError as ke:\n         return Response(\n-            {\"message\": \"File processed and data saved to database.\"},\n-            status=status.HTTP_200_OK,\n-        )\n+            {\"error\": f\"Mapping error: {str(ke)}\"},\n+            status=status.HTTP_400_BAD_REQUEST,\n+        )\n+    except Exception as e:\n+        return Response(\n+            {\"error\": f\"Server error while importing: {str(e)}\"},\n+            status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n+        )\n+\n+    return Response(\n+        {\"message\": \"File processed and data saved to database.\"},\n+        status=status.HTTP_200_OK,\n+    )\n"
                },
                {
                    "date": 1749105148775,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -58,205 +58,205 @@\n     detail=False,\n     methods=[\"post\"],\n     url_path=\"bulk-import\",\n     permission_classes=[IsAuthenticated],\n-)\n-def bulk_import(self, request, *args, **kwargs):\n-    \"\"\"\n-    Expects JSON body with:\n-      {\n-        \"file_id\": <existing File PK>,\n-        \"mapping\": {\n-            \"symbol_no\": \"<column_name>\",\n-            \"dateofbirth\": \"<column_name>\",\n-            \"cgpa\": \"<column_name>\",\n-            \"remarks\": \"<column_name>\"\n+    )\n+    def bulk_import(self, request, *args, **kwargs):\n+        \"\"\"\n+        Expects JSON body with:\n+        {\n+            \"file_id\": <existing File PK>,\n+            \"mapping\": {\n+                \"symbol_no\": \"<column_name>\",\n+                \"dateofbirth\": \"<column_name>\",\n+                \"cgpa\": \"<column_name>\",\n+                \"remarks\": \"<column_name>\"\n+            }\n         }\n-      }\n \n-    Streams through the stored File.file (CSV or Excel) and bulk-creates Result rows.\n-    \"\"\"\n-    file_id = request.data.get(\"file_id\")\n-    mapping_json = request.data.get(\"mapping\")\n+        Streams through the stored File.file (CSV or Excel) and bulk-creates Result rows.\n+        \"\"\"\n+        file_id = request.data.get(\"file_id\")\n+        mapping_json = request.data.get(\"mapping\")\n \n-    # 1) Validate file_id\n-    if file_id is None:\n-        return Response(\n-            {\"error\": \"Missing 'file_id'. Please send an existing File ID.\"},\n-            status=status.HTTP_400_BAD_REQUEST,\n-        )\n+        # 1) Validate file_id\n+        if file_id is None:\n+            return Response(\n+                {\"error\": \"Missing 'file_id'. Please send an existing File ID.\"},\n+                status=status.HTTP_400_BAD_REQUEST,\n+            )\n \n-    try:\n-        file_obj = File.objects.get(pk=file_id)\n-    except File.DoesNotExist:\n-        return Response(\n-            {\"error\": f\"File with ID={file_id} not found.\"},\n-            status=status.HTTP_400_BAD_REQUEST,\n-        )\n-\n-    # 2) Parse mapping\n-    if mapping_json is None:\n-        return Response(\n-            {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n-            status=status.HTTP_400_BAD_REQUEST,\n-        )\n-\n-    if isinstance(mapping_json, str):\n         try:\n-            mapping = json.loads(mapping_json)\n-        except json.JSONDecodeError as e:\n+            file_obj = File.objects.get(pk=file_id)\n+        except File.DoesNotExist:\n             return Response(\n-                {\"error\": f\"Invalid JSON in 'mapping': {str(e)}\"},\n+                {\"error\": f\"File with ID={file_id} not found.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n-    elif isinstance(mapping_json, dict):\n-        mapping = mapping_json\n-    else:\n-        return Response(\n-            {\"error\": \"'mapping' must be a JSON string or object.\"},\n-            status=status.HTTP_400_BAD_REQUEST,\n-        )\n \n-    # 3) Ensure exactly four keys\n-    required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n-    if set(mapping.keys()) != required_keys:\n-        return Response(\n-            {\n-                \"error\": (\n-                    \"'mapping' must contain exactly these keys: \"\n-                    \"symbol_no, dateofbirth, cgpa, remarks\"\n+        # 2) Parse mapping\n+        if mapping_json is None:\n+            return Response(\n+                {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n+                status=status.HTTP_400_BAD_REQUEST,\n+            )\n+\n+        if isinstance(mapping_json, str):\n+            try:\n+                mapping = json.loads(mapping_json)\n+            except json.JSONDecodeError as e:\n+                return Response(\n+                    {\"error\": f\"Invalid JSON in 'mapping': {str(e)}\"},\n+                    status=status.HTTP_400_BAD_REQUEST,\n                 )\n-            },\n-            status=status.HTTP_400_BAD_REQUEST,\n-        )\n+        elif isinstance(mapping_json, dict):\n+            mapping = mapping_json\n+        else:\n+            return Response(\n+                {\"error\": \"'mapping' must be a JSON string or object.\"},\n+                status=status.HTTP_400_BAD_REQUEST,\n+            )\n \n-    # 4) Determine file type by extension\n-    filename_lower = file_obj.file.name.lower()\n-    if filename_lower.endswith(\".csv\"):\n-        file_type = \"csv\"\n-    elif filename_lower.endswith((\".xlsx\", \".xls\")):\n-        file_type = \"excel\"\n-    else:\n-        return Response(\n-            {\"error\": \"Unsupported file format. Must be .csv, .xlsx, or .xls.\"},\n-            status=status.HTTP_400_BAD_REQUEST,\n-        )\n+        # 3) Ensure exactly four keys\n+        required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        if set(mapping.keys()) != required_keys:\n+            return Response(\n+                {\n+                    \"error\": (\n+                        \"'mapping' must contain exactly these keys: \"\n+                        \"symbol_no, dateofbirth, cgpa, remarks\"\n+                    )\n+                },\n+                status=status.HTTP_400_BAD_REQUEST,\n+            )\n \n-    BATCH_SIZE = 5000\n-    instances_to_create = []\n+        # 4) Determine file type by extension\n+        filename_lower = file_obj.file.name.lower()\n+        if filename_lower.endswith(\".csv\"):\n+            file_type = \"csv\"\n+        elif filename_lower.endswith((\".xlsx\", \".xls\")):\n+            file_type = \"excel\"\n+        else:\n+            return Response(\n+                {\"error\": \"Unsupported file format. Must be .csv, .xlsx, or .xls.\"},\n+                status=status.HTTP_400_BAD_REQUEST,\n+            )\n \n-    def normalize_header(hdr: str) -> str:\n-        return str(hdr).strip()\n+        BATCH_SIZE = 5000\n+        instances_to_create = []\n \n-    try:\n-        with transaction.atomic():\n-            if file_type == \"csv\":\n-                # ─── CSV path: open in binary then wrap for text\n-                file_obj.file.open(\"rb\")\n-                text_stream = io.TextIOWrapper(file_obj.file, encoding=\"utf-8\", newline=\"\")\n-                reader = csv.DictReader(text_stream)\n+        def normalize_header(hdr: str) -> str:\n+            return str(hdr).strip()\n \n-                # Verify header row\n-                csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n-                for _, col_name in mapping.items():\n-                    if normalize_header(col_name) not in csv_headers:\n-                        raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n+        try:\n+            with transaction.atomic():\n+                if file_type == \"csv\":\n+                    # ─── CSV path: open in binary then wrap for text\n+                    file_obj.file.open(\"rb\")\n+                    text_stream = io.TextIOWrapper(file_obj.file, encoding=\"utf-8\", newline=\"\")\n+                    reader = csv.DictReader(text_stream)\n \n-                # Iterate each row\n-                for row in reader:\n-                    try:\n-                        instances_to_create.append(\n-                            Result(\n-                                name=file_obj.file.name,\n-                                mapped_json=mapping,\n-                                file=file_obj,\n-                                symbol_no=row[normalize_header(mapping[\"symbol_no\"])].strip(),\n-                                dateofbirth=row[normalize_header(mapping[\"dateofbirth\"])].strip(),\n-                                cgpa=row[normalize_header(mapping[\"cgpa\"])].strip(),\n-                                remarks=row[normalize_header(mapping[\"remarks\"])].strip(),\n+                    # Verify header row\n+                    csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n+                    for _, col_name in mapping.items():\n+                        if normalize_header(col_name) not in csv_headers:\n+                            raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n+\n+                    # Iterate each row\n+                    for row in reader:\n+                        try:\n+                            instances_to_create.append(\n+                                Result(\n+                                    name=file_obj.file.name,\n+                                    mapped_json=mapping,\n+                                    file=file_obj,\n+                                    symbol_no=row[normalize_header(mapping[\"symbol_no\"])].strip(),\n+                                    dateofbirth=row[normalize_header(mapping[\"dateofbirth\"])].strip(),\n+                                    cgpa=row[normalize_header(mapping[\"cgpa\"])].strip(),\n+                                    remarks=row[normalize_header(mapping[\"remarks\"])].strip(),\n+                                )\n                             )\n-                        )\n-                    except Exception:\n-                        # skip malformed rows\n-                        continue\n+                        except Exception:\n+                            # skip malformed rows\n+                            continue\n \n-                    if len(instances_to_create) >= BATCH_SIZE:\n+                        if len(instances_to_create) >= BATCH_SIZE:\n+                            Result.objects.bulk_create(instances_to_create)\n+                            instances_to_create = []\n+\n+                    if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n                         instances_to_create = []\n \n-                if instances_to_create:\n-                    Result.objects.bulk_create(instances_to_create)\n-                    instances_to_create = []\n+                    file_obj.file.close()\n \n-                file_obj.file.close()\n+                else:\n+                    # ─── Excel path: open in binary and pass to load_workbook\n+                    file_obj.file.open(\"rb\")\n+                    wb = load_workbook(filename=file_obj.file, read_only=True, data_only=True)\n+                    sheet = wb.active\n+                    excel_iter = sheet.iter_rows(values_only=True)\n \n-            else:\n-                # ─── Excel path: open in binary and pass to load_workbook\n-                file_obj.file.open(\"rb\")\n-                wb = load_workbook(filename=file_obj.file, read_only=True, data_only=True)\n-                sheet = wb.active\n-                excel_iter = sheet.iter_rows(values_only=True)\n+                    header_row = next(excel_iter, None)\n+                    if not header_row:\n+                        raise KeyError(\"Excel file appears empty (no header).\")\n \n-                header_row = next(excel_iter, None)\n-                if not header_row:\n-                    raise KeyError(\"Excel file appears empty (no header).\")\n+                    header_map = {\n+                        normalize_header(cell_val): idx\n+                        for idx, cell_val in enumerate(header_row)\n+                        if cell_val is not None\n+                    }\n \n-                header_map = {\n-                    normalize_header(cell_val): idx\n-                    for idx, cell_val in enumerate(header_row)\n-                    if cell_val is not None\n-                }\n+                    for _, col_name in mapping.items():\n+                        if normalize_header(col_name) not in header_map:\n+                            raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n \n-                for _, col_name in mapping.items():\n-                    if normalize_header(col_name) not in header_map:\n-                        raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n+                    for row_tuple in excel_iter:\n+                        try:\n+                            sn_idx = header_map[normalize_header(mapping[\"symbol_no\"])]\n+                            dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n+                            cgpa_idx = header_map[normalize_header(mapping[\"cgpa\"])]\n+                            rm_idx = header_map[normalize_header(mapping[\"remarks\"])]\n \n-                for row_tuple in excel_iter:\n-                    try:\n-                        sn_idx = header_map[normalize_header(mapping[\"symbol_no\"])]\n-                        dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n-                        cgpa_idx = header_map[normalize_header(mapping[\"cgpa\"])]\n-                        rm_idx = header_map[normalize_header(mapping[\"remarks\"])]\n+                            def get_cell(r, i):\n+                                return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n \n-                        def get_cell(r, i):\n-                            return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n-\n-                        instances_to_create.append(\n-                            Result(\n-                                name=file_obj.file.name,\n-                                mapped_json=mapping,\n-                                file=file_obj,\n-                                symbol_no=get_cell(row_tuple, sn_idx),\n-                                dateofbirth=get_cell(row_tuple, dob_idx),\n-                                cgpa=get_cell(row_tuple, cgpa_idx),\n-                                remarks=get_cell(row_tuple, rm_idx),\n+                            instances_to_create.append(\n+                                Result(\n+                                    name=file_obj.file.name,\n+                                    mapped_json=mapping,\n+                                    file=file_obj,\n+                                    symbol_no=get_cell(row_tuple, sn_idx),\n+                                    dateofbirth=get_cell(row_tuple, dob_idx),\n+                                    cgpa=get_cell(row_tuple, cgpa_idx),\n+                                    remarks=get_cell(row_tuple, rm_idx),\n+                                )\n                             )\n-                        )\n-                    except Exception:\n-                        continue\n+                        except Exception:\n+                            continue\n \n-                    if len(instances_to_create) >= BATCH_SIZE:\n+                        if len(instances_to_create) >= BATCH_SIZE:\n+                            Result.objects.bulk_create(instances_to_create)\n+                            instances_to_create = []\n+\n+                    if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n                         instances_to_create = []\n \n-                if instances_to_create:\n-                    Result.objects.bulk_create(instances_to_create)\n-                    instances_to_create = []\n+                    wb.close()\n+                    file_obj.file.close()\n \n-                wb.close()\n-                file_obj.file.close()\n+        except KeyError as ke:\n+            return Response(\n+                {\"error\": f\"Mapping error: {str(ke)}\"},\n+                status=status.HTTP_400_BAD_REQUEST,\n+            )\n+        except Exception as e:\n+            return Response(\n+                {\"error\": f\"Server error while importing: {str(e)}\"},\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n+            )\n \n-    except KeyError as ke:\n         return Response(\n-            {\"error\": f\"Mapping error: {str(ke)}\"},\n-            status=status.HTTP_400_BAD_REQUEST,\n+            {\"message\": \"File processed and data saved to database.\"},\n+            status=status.HTTP_200_OK,\n         )\n-    except Exception as e:\n-        return Response(\n-            {\"error\": f\"Server error while importing: {str(e)}\"},\n-            status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n-        )\n-\n-    return Response(\n-        {\"message\": \"File processed and data saved to database.\"},\n-        status=status.HTTP_200_OK,\n-    )\n"
                },
                {
                    "date": 1749105153969,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,9 +57,9 @@\n     @action(\n     detail=False,\n     methods=[\"post\"],\n     url_path=\"bulk-import\",\n-    permission_classes=[IsAuthenticated],\n+    # permission_classes=[IsAuthenticated],\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         Expects JSON body with:\n"
                },
                {
                    "date": 1749105218844,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,27 +54,28 @@\n     def retrieve(self, request, *args, **kwargs):\n         return super().retrieve(request, *args, **kwargs)\n \n     @action(\n-    detail=False,\n-    methods=[\"post\"],\n-    url_path=\"bulk-import\",\n-    # permission_classes=[IsAuthenticated],\n+        detail=False,\n+        methods=[\"post\"],\n+        url_path=\"bulk-import\",\n+        permission_classes=[IsAuthenticated],\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         Expects JSON body with:\n-        {\n+          {\n             \"file_id\": <existing File PK>,\n             \"mapping\": {\n                 \"symbol_no\": \"<column_name>\",\n                 \"dateofbirth\": \"<column_name>\",\n                 \"cgpa\": \"<column_name>\",\n                 \"remarks\": \"<column_name>\"\n             }\n-        }\n+          }\n \n-        Streams through the stored File.file (CSV or Excel) and bulk-creates Result rows.\n+        Streams through the stored File.file (CSV or Excel) and bulk-creates Result rows,\n+        saving only: name, file, symbol_no, dateofbirth, cgpa, remarks.\n         \"\"\"\n         file_id = request.data.get(\"file_id\")\n         mapping_json = request.data.get(\"mapping\")\n \n@@ -151,9 +152,11 @@\n             with transaction.atomic():\n                 if file_type == \"csv\":\n                     # ─── CSV path: open in binary then wrap for text\n                     file_obj.file.open(\"rb\")\n-                    text_stream = io.TextIOWrapper(file_obj.file, encoding=\"utf-8\", newline=\"\")\n+                    text_stream = io.TextIOWrapper(\n+                        file_obj.file, encoding=\"utf-8\", newline=\"\"\n+                    )\n                     reader = csv.DictReader(text_stream)\n \n                     # Verify header row\n                     csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n@@ -166,9 +169,8 @@\n                         try:\n                             instances_to_create.append(\n                                 Result(\n                                     name=file_obj.file.name,\n-                                    mapped_json=mapping,\n                                     file=file_obj,\n                                     symbol_no=row[normalize_header(mapping[\"symbol_no\"])].strip(),\n                                     dateofbirth=row[normalize_header(mapping[\"dateofbirth\"])].strip(),\n                                     cgpa=row[normalize_header(mapping[\"cgpa\"])].strip(),\n@@ -222,9 +224,8 @@\n \n                             instances_to_create.append(\n                                 Result(\n                                     name=file_obj.file.name,\n-                                    mapped_json=mapping,\n                                     file=file_obj,\n                                     symbol_no=get_cell(row_tuple, sn_idx),\n                                     dateofbirth=get_cell(row_tuple, dob_idx),\n                                     cgpa=get_cell(row_tuple, cgpa_idx),\n@@ -258,5 +259,5 @@\n \n         return Response(\n             {\"message\": \"File processed and data saved to database.\"},\n             status=status.HTTP_200_OK,\n-        )\n+        )\n\\ No newline at end of file\n"
                },
                {
                    "date": 1749105453242,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,25 +57,33 @@\n     @action(\n         detail=False,\n         methods=[\"post\"],\n         url_path=\"bulk-import\",\n+        # permission_classes=[IsAuthenticated],\n+    )\n+    @action(\n+        detail=False,\n+        methods=[\"post\"],\n+        url_path=\"bulk-import\",\n         permission_classes=[IsAuthenticated],\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n-        Expects JSON body with:\n-          {\n-            \"file_id\": <existing File PK>,\n-            \"mapping\": {\n-                \"symbol_no\": \"<column_name>\",\n-                \"dateofbirth\": \"<column_name>\",\n-                \"cgpa\": \"<column_name>\",\n-                \"remarks\": \"<column_name>\"\n-            }\n+        Expects JSON body:\n+        {\n+          \"file_id\": <existing File PK>,\n+          \"mapping\": {\n+            \"symbol_no\":    \"<column_name>\",\n+            \"dateofbirth\":  \"<column_name>\",\n+            \"cgpa\":         \"<column_name>\",\n+            \"remarks\":      \"<column_name>\"\n           }\n+        }\n \n-        Streams through the stored File.file (CSV or Excel) and bulk-creates Result rows,\n-        saving only: name, file, symbol_no, dateofbirth, cgpa, remarks.\n+        1) Look up File by ID.\n+        2) Save mapping JSON to File.mapped_json.\n+        3) Stream through the File.file (CSV or Excel).\n+        4) Bulk‐create Result rows with only: name, file, symbol_no, dateofbirth, cgpa, remarks.\n         \"\"\"\n         file_id = request.data.get(\"file_id\")\n         mapping_json = request.data.get(\"mapping\")\n \n@@ -84,18 +92,17 @@\n             return Response(\n                 {\"error\": \"Missing 'file_id'. Please send an existing File ID.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n-\n         try:\n             file_obj = File.objects.get(pk=file_id)\n         except File.DoesNotExist:\n             return Response(\n                 {\"error\": f\"File with ID={file_id} not found.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # 2) Parse mapping\n+        # 2) Parse mapping JSON\n         if mapping_json is None:\n             return Response(\n                 {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n@@ -112,9 +119,9 @@\n         elif isinstance(mapping_json, dict):\n             mapping = mapping_json\n         else:\n             return Response(\n-                {\"error\": \"'mapping' must be a JSON string or object.\"},\n+                {\"error\": \"'mapping' must be a JSON string or JSON object.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         # 3) Ensure exactly four keys\n@@ -129,9 +136,13 @@\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # 4) Determine file type by extension\n+        # 4) Save mapping JSON onto the File model\n+        file_obj.mapped_json = mapping\n+        file_obj.save(update_fields=[\"mapped_json\"])\n+\n+        # 5) Determine file type by extension\n         filename_lower = file_obj.file.name.lower()\n         if filename_lower.endswith(\".csv\"):\n             file_type = \"csv\"\n         elif filename_lower.endswith((\".xlsx\", \".xls\")):\n@@ -157,15 +168,15 @@\n                         file_obj.file, encoding=\"utf-8\", newline=\"\"\n                     )\n                     reader = csv.DictReader(text_stream)\n \n-                    # Verify header row\n+                    # Verify header row contains each mapped column\n                     csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n                     for _, col_name in mapping.items():\n                         if normalize_header(col_name) not in csv_headers:\n                             raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n \n-                    # Iterate each row\n+                    # Stream each row\n                     for row in reader:\n                         try:\n                             instances_to_create.append(\n                                 Result(\n@@ -177,25 +188,29 @@\n                                     remarks=row[normalize_header(mapping[\"remarks\"])].strip(),\n                                 )\n                             )\n                         except Exception:\n-                            # skip malformed rows\n+                            # Skip any malformed row\n                             continue\n \n+                        # Bulk‐insert in batches\n                         if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n                             instances_to_create = []\n \n+                    # Flush any remainder\n                     if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n                         instances_to_create = []\n \n                     file_obj.file.close()\n \n                 else:\n-                    # ─── Excel path: open in binary and pass to load_workbook\n+                    # ─── Excel path: open in binary and pass to openpyxl\n                     file_obj.file.open(\"rb\")\n-                    wb = load_workbook(filename=file_obj.file, read_only=True, data_only=True)\n+                    wb = load_workbook(\n+                        filename=file_obj.file, read_only=True, data_only=True\n+                    )\n                     sheet = wb.active\n                     excel_iter = sheet.iter_rows(values_only=True)\n \n                     header_row = next(excel_iter, None)\n@@ -211,8 +226,9 @@\n                     for _, col_name in mapping.items():\n                         if normalize_header(col_name) not in header_map:\n                             raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n \n+                    # Stream each subsequent row\n                     for row_tuple in excel_iter:\n                         try:\n                             sn_idx = header_map[normalize_header(mapping[\"symbol_no\"])]\n                             dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n"
                },
                {
                    "date": 1749105460477,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n     @action(\n         detail=False,\n         methods=[\"post\"],\n         url_path=\"bulk-import\",\n-        permission_classes=[IsAuthenticated],\n+        # permission_classes=[IsAuthenticated],\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         Expects JSON body:\n"
                },
                {
                    "date": 1749105877708,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,28 +67,31 @@\n         # permission_classes=[IsAuthenticated],\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n-        Expects JSON body:\n+        Expects JSON body with:\n         {\n-          \"file_id\": <existing File PK>,\n+          \"file_id\":    <existing File PK>,\n+          \"name\":       \"<some name for each Result row (e.g. student_name)>\",\n           \"mapping\": {\n-            \"symbol_no\":    \"<column_name>\",\n-            \"dateofbirth\":  \"<column_name>\",\n-            \"cgpa\":         \"<column_name>\",\n-            \"remarks\":      \"<column_name>\"\n+            \"symbol_no\":    \"<column_name_for_symbol_no>\",\n+            \"dateofbirth\":  \"<column_name_for_dateofbirth>\",\n+            \"cgpa\":         \"<column_name_for_cgpa>\",\n+            \"remarks\":      \"<column_name_for_remarks>\"\n           }\n         }\n \n         1) Look up File by ID.\n         2) Save mapping JSON to File.mapped_json.\n         3) Stream through the File.file (CSV or Excel).\n-        4) Bulk‐create Result rows with only: name, file, symbol_no, dateofbirth, cgpa, remarks.\n+        4) Bulk‐create Result rows with: name, file, symbol_no, dateofbirth, cgpa, remarks.\n         \"\"\"\n+        # ─── 1) Read top‐level inputs ─────────────────────────────────────\n         file_id = request.data.get(\"file_id\")\n+        name_value = request.data.get(\"name\", \"\").strip()\n         mapping_json = request.data.get(\"mapping\")\n \n-        # 1) Validate file_id\n+        # Validate file_id\n         if file_id is None:\n             return Response(\n                 {\"error\": \"Missing 'file_id'. Please send an existing File ID.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n@@ -100,15 +103,21 @@\n                 {\"error\": f\"File with ID={file_id} not found.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # 2) Parse mapping JSON\n+        # Validate name\n+        if not name_value:\n+            return Response(\n+                {\"error\": \"Missing 'name'. Please send a non‐empty name string.\"},\n+                status=status.HTTP_400_BAD_REQUEST,\n+            )\n+\n+        # Parse mapping\n         if mapping_json is None:\n             return Response(\n                 {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n-\n         if isinstance(mapping_json, str):\n             try:\n                 mapping = json.loads(mapping_json)\n             except json.JSONDecodeError as e:\n@@ -123,9 +132,9 @@\n                 {\"error\": \"'mapping' must be a JSON string or JSON object.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # 3) Ensure exactly four keys\n+        # Ensure exactly four keys in mapping\n         required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n@@ -136,13 +145,13 @@\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # 4) Save mapping JSON onto the File model\n+        # ─── 2) Save mapping onto the File model ─────────────────────────\n         file_obj.mapped_json = mapping\n         file_obj.save(update_fields=[\"mapped_json\"])\n \n-        # 5) Determine file type by extension\n+        # ─── 3) Determine file type by extension ────────────────────────\n         filename_lower = file_obj.file.name.lower()\n         if filename_lower.endswith(\".csv\"):\n             file_type = \"csv\"\n         elif filename_lower.endswith((\".xlsx\", \".xls\")):\n@@ -161,9 +170,9 @@\n \n         try:\n             with transaction.atomic():\n                 if file_type == \"csv\":\n-                    # ─── CSV path: open in binary then wrap for text\n+                    # ─── CSV path: open in binary then wrap for text ─────────\n                     file_obj.file.open(\"rb\")\n                     text_stream = io.TextIOWrapper(\n                         file_obj.file, encoding=\"utf-8\", newline=\"\"\n                     )\n@@ -179,14 +188,22 @@\n                     for row in reader:\n                         try:\n                             instances_to_create.append(\n                                 Result(\n-                                    name=file_obj.file.name,\n+                                    name=name_value,  # use the provided name here\n                                     file=file_obj,\n-                                    symbol_no=row[normalize_header(mapping[\"symbol_no\"])].strip(),\n-                                    dateofbirth=row[normalize_header(mapping[\"dateofbirth\"])].strip(),\n-                                    cgpa=row[normalize_header(mapping[\"cgpa\"])].strip(),\n-                                    remarks=row[normalize_header(mapping[\"remarks\"])].strip(),\n+                                    symbol_no=row[\n+                                        normalize_header(mapping[\"symbol_no\"])\n+                                    ].strip(),\n+                                    dateofbirth=row[\n+                                        normalize_header(mapping[\"dateofbirth\"])\n+                                    ].strip(),\n+                                    cgpa=row[\n+                                        normalize_header(mapping[\"cgpa\"])\n+                                    ].strip(),\n+                                    remarks=row[\n+                                        normalize_header(mapping[\"remarks\"])\n+                                    ].strip(),\n                                 )\n                             )\n                         except Exception:\n                             # Skip any malformed row\n@@ -204,9 +221,9 @@\n \n                     file_obj.file.close()\n \n                 else:\n-                    # ─── Excel path: open in binary and pass to openpyxl\n+                    # ─── Excel path: open in binary and pass to openpyxl ──────\n                     file_obj.file.open(\"rb\")\n                     wb = load_workbook(\n                         filename=file_obj.file, read_only=True, data_only=True\n                     )\n@@ -239,9 +256,9 @@\n                                 return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n \n                             instances_to_create.append(\n                                 Result(\n-                                    name=file_obj.file.name,\n+                                    name=name_value,  # use the provided name here\n                                     file=file_obj,\n                                     symbol_no=get_cell(row_tuple, sn_idx),\n                                     dateofbirth=get_cell(row_tuple, dob_idx),\n                                     cgpa=get_cell(row_tuple, cgpa_idx),\n"
                },
                {
                    "date": 1749105967817,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n     @action(\n         detail=False,\n         methods=[\"post\"],\n         url_path=\"bulk-import\",\n-        # permission_classes=[IsAuthenticated],\n+        # permission_classes=[IsAuthenticated],  # enable if you need auth\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         Expects JSON body with:\n@@ -79,12 +79,14 @@\n             \"remarks\":      \"<column_name_for_remarks>\"\n           }\n         }\n \n+        Behavior:\n         1) Look up File by ID.\n-        2) Save mapping JSON to File.mapped_json.\n-        3) Stream through the File.file (CSV or Excel).\n-        4) Bulk‐create Result rows with: name, file, symbol_no, dateofbirth, cgpa, remarks.\n+        2) Delete any existing Results linked to that File.\n+        3) Save mapping JSON to File.mapped_json.\n+        4) Stream through the File.file (CSV or Excel).\n+        5) Bulk‐create fresh Result rows with: name, file, symbol_no, dateofbirth, cgpa, remarks.\n         \"\"\"\n         # ─── 1) Read top‐level inputs ─────────────────────────────────────\n         file_id = request.data.get(\"file_id\")\n         name_value = request.data.get(\"name\", \"\").strip()\n@@ -145,13 +147,16 @@\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 2) Save mapping onto the File model ─────────────────────────\n+        # ─── 2) Delete existing Results for this File ────────────────────\n+        Result.objects.filter(file=file_obj).delete()\n+\n+        # ─── 3) Save mapping onto the File model ─────────────────────────\n         file_obj.mapped_json = mapping\n         file_obj.save(update_fields=[\"mapped_json\"])\n \n-        # ─── 3) Determine file type by extension ────────────────────────\n+        # ─── 4) Determine file type by extension ────────────────────────\n         filename_lower = file_obj.file.name.lower()\n         if filename_lower.endswith(\".csv\"):\n             file_type = \"csv\"\n         elif filename_lower.endswith((\".xlsx\", \".xls\")):\n"
                },
                {
                    "date": 1749106174544,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -67,26 +67,27 @@\n         # permission_classes=[IsAuthenticated],  # enable if you need auth\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n-        Expects JSON body with:\n+        Expects JSON body:\n         {\n-          \"file_id\":    <existing File PK>,\n-          \"name\":       \"<some name for each Result row (e.g. student_name)>\",\n+          \"file_id\":      <existing File PK>,\n+          \"name\":         \"<a display name to store on File.name>\",\n           \"mapping\": {\n+            \"student_name\": \"<column_name_for_student_name>\",\n             \"symbol_no\":    \"<column_name_for_symbol_no>\",\n             \"dateofbirth\":  \"<column_name_for_dateofbirth>\",\n             \"cgpa\":         \"<column_name_for_cgpa>\",\n             \"remarks\":      \"<column_name_for_remarks>\"\n           }\n         }\n \n-        Behavior:\n         1) Look up File by ID.\n-        2) Delete any existing Results linked to that File.\n-        3) Save mapping JSON to File.mapped_json.\n-        4) Stream through the File.file (CSV or Excel).\n-        5) Bulk‐create fresh Result rows with: name, file, symbol_no, dateofbirth, cgpa, remarks.\n+        2) Delete any existing Result rows for that File.\n+        3) Save the incoming \"name\" onto File.name.\n+        4) Save mapping JSON onto File.mapped_json.\n+        5) Stream through File.file (CSV or Excel) and bulk‐create Result rows with:\n+             student_name, file, symbol_no, dateofbirth, cgpa, remarks.\n         \"\"\"\n         # ─── 1) Read top‐level inputs ─────────────────────────────────────\n         file_id = request.data.get(\"file_id\")\n         name_value = request.data.get(\"name\", \"\").strip()\n@@ -105,16 +106,16 @@\n                 {\"error\": f\"File with ID={file_id} not found.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # Validate name\n+        # Validate name (which we will write into File.name)\n         if not name_value:\n             return Response(\n                 {\"error\": \"Missing 'name'. Please send a non‐empty name string.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # Parse mapping\n+        # Parse mapping JSON\n         if mapping_json is None:\n             return Response(\n                 {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n@@ -134,29 +135,33 @@\n                 {\"error\": \"'mapping' must be a JSON string or JSON object.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # Ensure exactly four keys in mapping\n-        required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        # Ensure exactly five keys in mapping: student_name, symbol_no, dateofbirth, cgpa, remarks\n+        required_keys = {\"student_name\", \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n                     \"error\": (\n                         \"'mapping' must contain exactly these keys: \"\n-                        \"symbol_no, dateofbirth, cgpa, remarks\"\n+                        \"student_name, symbol_no, dateofbirth, cgpa, remarks\"\n                     )\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 2) Delete existing Results for this File ────────────────────\n+        # ─── 2) Delete any existing Results for this File ────────────────\n         Result.objects.filter(file=file_obj).delete()\n \n-        # ─── 3) Save mapping onto the File model ─────────────────────────\n+        # ─── 3) Save name_value onto File.name ───────────────────────────\n+        file_obj.name = name_value\n+        file_obj.save(update_fields=[\"name\"])\n+\n+        # ─── 4) Save mapping JSON onto File.mapped_json ─────────────────\n         file_obj.mapped_json = mapping\n         file_obj.save(update_fields=[\"mapped_json\"])\n \n-        # ─── 4) Determine file type by extension ────────────────────────\n+        # ─── 5) Determine file type by extension of the stored File.file ─\n         filename_lower = file_obj.file.name.lower()\n         if filename_lower.endswith(\".csv\"):\n             file_type = \"csv\"\n         elif filename_lower.endswith((\".xlsx\", \".xls\")):\n@@ -175,9 +180,9 @@\n \n         try:\n             with transaction.atomic():\n                 if file_type == \"csv\":\n-                    # ─── CSV path: open in binary then wrap for text ─────────\n+                    # ─── CSV path: open in binary then wrap for text ───────────\n                     file_obj.file.open(\"rb\")\n                     text_stream = io.TextIOWrapper(\n                         file_obj.file, encoding=\"utf-8\", newline=\"\"\n                     )\n@@ -188,14 +193,16 @@\n                     for _, col_name in mapping.items():\n                         if normalize_header(col_name) not in csv_headers:\n                             raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n \n-                    # Stream each row\n+                    # Stream each row, building Result objects\n                     for row in reader:\n                         try:\n                             instances_to_create.append(\n                                 Result(\n-                                    name=name_value,  # use the provided name here\n+                                    student_name=row[\n+                                        normalize_header(mapping[\"student_name\"])\n+                                    ].strip(),\n                                     file=file_obj,\n                                     symbol_no=row[\n                                         normalize_header(mapping[\"symbol_no\"])\n                                     ].strip(),\n@@ -210,25 +217,25 @@\n                                     ].strip(),\n                                 )\n                             )\n                         except Exception:\n-                            # Skip any malformed row\n+                            # Skip any malformed or missing‐column rows\n                             continue\n \n-                        # Bulk‐insert in batches\n+                        # Bulk‐insert in batches of BATCH_SIZE\n                         if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n                             instances_to_create = []\n \n-                    # Flush any remainder\n+                    # Flush any remaining rows\n                     if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n                         instances_to_create = []\n \n                     file_obj.file.close()\n \n                 else:\n-                    # ─── Excel path: open in binary and pass to openpyxl ──────\n+                    # ─── Excel path: open in binary and pass to openpyxl ───────\n                     file_obj.file.open(\"rb\")\n                     wb = load_workbook(\n                         filename=file_obj.file, read_only=True, data_only=True\n                     )\n@@ -238,46 +245,51 @@\n                     header_row = next(excel_iter, None)\n                     if not header_row:\n                         raise KeyError(\"Excel file appears empty (no header).\")\n \n+                    # Build header_map: { normalized_header: index }\n                     header_map = {\n                         normalize_header(cell_val): idx\n                         for idx, cell_val in enumerate(header_row)\n                         if cell_val is not None\n                     }\n \n+                    # Verify each column in mapping exists\n                     for _, col_name in mapping.items():\n                         if normalize_header(col_name) not in header_map:\n                             raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n \n-                    # Stream each subsequent row\n+                    # Iterate each subsequent row tuple\n                     for row_tuple in excel_iter:\n                         try:\n                             sn_idx = header_map[normalize_header(mapping[\"symbol_no\"])]\n                             dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n                             cgpa_idx = header_map[normalize_header(mapping[\"cgpa\"])]\n                             rm_idx = header_map[normalize_header(mapping[\"remarks\"])]\n+                            name_idx = header_map[normalize_header(mapping[\"student_name\"])]\n \n                             def get_cell(r, i):\n                                 return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n \n                             instances_to_create.append(\n                                 Result(\n-                                    name=name_value,  # use the provided name here\n+                                    student_name=get_cell(row_tuple, name_idx),\n                                     file=file_obj,\n                                     symbol_no=get_cell(row_tuple, sn_idx),\n                                     dateofbirth=get_cell(row_tuple, dob_idx),\n                                     cgpa=get_cell(row_tuple, cgpa_idx),\n                                     remarks=get_cell(row_tuple, rm_idx),\n                                 )\n                             )\n                         except Exception:\n+                            # Skip rows with missing columns or other errors\n                             continue\n \n                         if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n                             instances_to_create = []\n \n+                    # Flush any remaining rows\n                     if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n                         instances_to_create = []\n \n"
                },
                {
                    "date": 1749106221441,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -136,9 +136,9 @@\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         # Ensure exactly five keys in mapping: student_name, symbol_no, dateofbirth, cgpa, remarks\n-        required_keys = {\"student_name\", \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        required_keys = { \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n                     \"error\": (\n"
                },
                {
                    "date": 1749106238043,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -142,9 +142,9 @@\n             return Response(\n                 {\n                     \"error\": (\n                         \"'mapping' must contain exactly these keys: \"\n-                        \"student_name, symbol_no, dateofbirth, cgpa, remarks\"\n+                        \" symbol_no, dateofbirth, cgpa, remarks\"\n                     )\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n"
                },
                {
                    "date": 1749106389913,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,38 +63,24 @@\n     @action(\n         detail=False,\n         methods=[\"post\"],\n         url_path=\"bulk-import\",\n-        # permission_classes=[IsAuthenticated],  # enable if you need auth\n+        # permission_classes=[IsAuthenticated],  # enable if needed\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n-        Expects JSON body:\n-        {\n-          \"file_id\":      <existing File PK>,\n-          \"name\":         \"<a display name to store on File.name>\",\n-          \"mapping\": {\n-            \"student_name\": \"<column_name_for_student_name>\",\n-            \"symbol_no\":    \"<column_name_for_symbol_no>\",\n-            \"dateofbirth\":  \"<column_name_for_dateofbirth>\",\n-            \"cgpa\":         \"<column_name_for_cgpa>\",\n-            \"remarks\":      \"<column_name_for_remarks>\"\n-          }\n-        }\n-\n         1) Look up File by ID.\n-        2) Delete any existing Result rows for that File.\n-        3) Save the incoming \"name\" onto File.name.\n+        2) Delete existing Results for that File.\n+        3) Save “name” onto File.name.\n         4) Save mapping JSON onto File.mapped_json.\n-        5) Stream through File.file (CSV or Excel) and bulk‐create Result rows with:\n-             student_name, file, symbol_no, dateofbirth, cgpa, remarks.\n+        5) Stream through the CSV/Excel, building Result rows.\n+           Print debug info on rows read and inserted.\n         \"\"\"\n-        # ─── 1) Read top‐level inputs ─────────────────────────────────────\n+        # 1) Read top-level inputs\n         file_id = request.data.get(\"file_id\")\n         name_value = request.data.get(\"name\", \"\").strip()\n         mapping_json = request.data.get(\"mapping\")\n \n-        # Validate file_id\n         if file_id is None:\n             return Response(\n                 {\"error\": \"Missing 'file_id'. Please send an existing File ID.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n@@ -106,16 +92,14 @@\n                 {\"error\": f\"File with ID={file_id} not found.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # Validate name (which we will write into File.name)\n         if not name_value:\n             return Response(\n                 {\"error\": \"Missing 'name'. Please send a non‐empty name string.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # Parse mapping JSON\n         if mapping_json is None:\n             return Response(\n                 {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n@@ -135,33 +119,33 @@\n                 {\"error\": \"'mapping' must be a JSON string or JSON object.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # Ensure exactly five keys in mapping: student_name, symbol_no, dateofbirth, cgpa, remarks\n-        required_keys = { \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        required_keys = {\"student_name\", \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n                     \"error\": (\n                         \"'mapping' must contain exactly these keys: \"\n-                        \" symbol_no, dateofbirth, cgpa, remarks\"\n+                        \"student_name, symbol_no, dateofbirth, cgpa, remarks\"\n                     )\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # ─── 2) Delete any existing Results for this File ────────────────\n-        Result.objects.filter(file=file_obj).delete()\n+        # 2) Delete existing Results\n+        deleted_count, _ = Result.objects.filter(file=file_obj).delete()\n+        print(f\"DEBUG: deleted {deleted_count} existing Result rows for file_id {file_id}\")\n \n-        # ─── 3) Save name_value onto File.name ───────────────────────────\n+        # 3) Save name onto File.name\n         file_obj.name = name_value\n         file_obj.save(update_fields=[\"name\"])\n \n-        # ─── 4) Save mapping JSON onto File.mapped_json ─────────────────\n+        # 4) Save mapping onto File.mapped_json\n         file_obj.mapped_json = mapping\n         file_obj.save(update_fields=[\"mapped_json\"])\n \n-        # ─── 5) Determine file type by extension of the stored File.file ─\n+        # 5) Determine file type by extension\n         filename_lower = file_obj.file.name.lower()\n         if filename_lower.endswith(\".csv\"):\n             file_type = \"csv\"\n         elif filename_lower.endswith((\".xlsx\", \".xls\")):\n@@ -177,65 +161,67 @@\n \n         def normalize_header(hdr: str) -> str:\n             return str(hdr).strip()\n \n+        total_rows_read = 0\n+        total_rows_inserted = 0\n+\n         try:\n             with transaction.atomic():\n                 if file_type == \"csv\":\n-                    # ─── CSV path: open in binary then wrap for text ───────────\n+                    # Open in binary, wrap for text\n                     file_obj.file.open(\"rb\")\n                     text_stream = io.TextIOWrapper(\n                         file_obj.file, encoding=\"utf-8\", newline=\"\"\n                     )\n                     reader = csv.DictReader(text_stream)\n \n-                    # Verify header row contains each mapped column\n-                    csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n-                    for _, col_name in mapping.items():\n-                        if normalize_header(col_name) not in csv_headers:\n-                            raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n+                    # Print the header row we see:\n+                    print(\"DEBUG: CSV header row is:\", reader.fieldnames)\n \n-                    # Stream each row, building Result objects\n-                    for row in reader:\n+                    for row_num, row in enumerate(reader, start=1):\n+                        total_rows_read += 1\n                         try:\n+                            sn = row[ normalize_header(mapping[\"student_name\"]) ].strip()\n+                            so = row[ normalize_header(mapping[\"symbol_no\"]) ].strip()\n+                            dob = row[ normalize_header(mapping[\"dateofbirth\"]) ].strip()\n+                            cg  = row[ normalize_header(mapping[\"cgpa\"]) ].strip()\n+                            rm  = row[ normalize_header(mapping[\"remarks\"]) ].strip()\n+\n                             instances_to_create.append(\n                                 Result(\n-                                    student_name=row[\n-                                        normalize_header(mapping[\"student_name\"])\n-                                    ].strip(),\n-                                    file=file_obj,\n-                                    symbol_no=row[\n-                                        normalize_header(mapping[\"symbol_no\"])\n-                                    ].strip(),\n-                                    dateofbirth=row[\n-                                        normalize_header(mapping[\"dateofbirth\"])\n-                                    ].strip(),\n-                                    cgpa=row[\n-                                        normalize_header(mapping[\"cgpa\"])\n-                                    ].strip(),\n-                                    remarks=row[\n-                                        normalize_header(mapping[\"remarks\"])\n-                                    ].strip(),\n+                                    student_name = sn,\n+                                    file         = file_obj,\n+                                    symbol_no    = so,\n+                                    dateofbirth  = dob,\n+                                    cgpa         = cg,\n+                                    remarks      = rm,\n                                 )\n                             )\n-                        except Exception:\n-                            # Skip any malformed or missing‐column rows\n+                        except KeyError as ke:\n+                            print(f\"DEBUG: Row {row_num} missing column: {ke}\")\n                             continue\n+                        except Exception as e:\n+                            print(f\"DEBUG: Unexpected error on row {row_num}: {str(e)}\")\n+                            continue\n \n-                        # Bulk‐insert in batches of BATCH_SIZE\n                         if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n+                            print(f\"DEBUG: bulk_created {len(instances_to_create)} rows\")\n+                            total_rows_inserted += len(instances_to_create)\n                             instances_to_create = []\n \n-                    # Flush any remaining rows\n+                    # Flush any remainder\n                     if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n+                        print(f\"DEBUG: final bulk_created {len(instances_to_create)} rows\")\n+                        total_rows_inserted += len(instances_to_create)\n                         instances_to_create = []\n \n                     file_obj.file.close()\n \n                 else:\n-                    # ─── Excel path: open in binary and pass to openpyxl ───────\n+                    # Excel path\n                     file_obj.file.open(\"rb\")\n                     wb = load_workbook(\n                         filename=file_obj.file, read_only=True, data_only=True\n                     )\n@@ -244,54 +230,54 @@\n \n                     header_row = next(excel_iter, None)\n                     if not header_row:\n                         raise KeyError(\"Excel file appears empty (no header).\")\n-\n-                    # Build header_map: { normalized_header: index }\n                     header_map = {\n                         normalize_header(cell_val): idx\n                         for idx, cell_val in enumerate(header_row)\n                         if cell_val is not None\n                     }\n+                    print(\"DEBUG: Excel header_map is:\", header_map)\n \n-                    # Verify each column in mapping exists\n-                    for _, col_name in mapping.items():\n-                        if normalize_header(col_name) not in header_map:\n-                            raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n-\n-                    # Iterate each subsequent row tuple\n-                    for row_tuple in excel_iter:\n+                    for row_num, row_tuple in enumerate(excel_iter, start=2):\n+                        total_rows_read += 1\n                         try:\n-                            sn_idx = header_map[normalize_header(mapping[\"symbol_no\"])]\n-                            dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n-                            cgpa_idx = header_map[normalize_header(mapping[\"cgpa\"])]\n-                            rm_idx = header_map[normalize_header(mapping[\"remarks\"])]\n-                            name_idx = header_map[normalize_header(mapping[\"student_name\"])]\n+                            sn_idx  = header_map[ normalize_header(mapping[\"student_name\"]) ]\n+                            so_idx  = header_map[ normalize_header(mapping[\"symbol_no\"]) ]\n+                            dob_idx = header_map[ normalize_header(mapping[\"dateofbirth\"]) ]\n+                            cg_idx  = header_map[ normalize_header(mapping[\"cgpa\"]) ]\n+                            rm_idx  = header_map[ normalize_header(mapping[\"remarks\"]) ]\n \n                             def get_cell(r, i):\n                                 return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n \n                             instances_to_create.append(\n                                 Result(\n-                                    student_name=get_cell(row_tuple, name_idx),\n-                                    file=file_obj,\n-                                    symbol_no=get_cell(row_tuple, sn_idx),\n-                                    dateofbirth=get_cell(row_tuple, dob_idx),\n-                                    cgpa=get_cell(row_tuple, cgpa_idx),\n-                                    remarks=get_cell(row_tuple, rm_idx),\n+                                    student_name = get_cell(row_tuple, sn_idx),\n+                                    file         = file_obj,\n+                                    symbol_no    = get_cell(row_tuple, so_idx),\n+                                    dateofbirth  = get_cell(row_tuple, dob_idx),\n+                                    cgpa         = get_cell(row_tuple, cg_idx),\n+                                    remarks      = get_cell(row_tuple, rm_idx),\n                                 )\n                             )\n-                        except Exception:\n-                            # Skip rows with missing columns or other errors\n+                        except KeyError as ke:\n+                            print(f\"DEBUG: Row {row_num} missing column: {ke}\")\n                             continue\n+                        except Exception as e:\n+                            print(f\"DEBUG: Unexpected error on row {row_num}: {str(e)}\")\n+                            continue\n \n                         if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n+                            print(f\"DEBUG: bulk_created {len(instances_to_create)} rows\")\n+                            total_rows_inserted += len(instances_to_create)\n                             instances_to_create = []\n \n-                    # Flush any remaining rows\n                     if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n+                        print(f\"DEBUG: final bulk_created {len(instances_to_create)} rows\")\n+                        total_rows_inserted += len(instances_to_create)\n                         instances_to_create = []\n \n                     wb.close()\n                     file_obj.file.close()\n@@ -306,8 +292,16 @@\n                 {\"error\": f\"Server error while importing: {str(e)}\"},\n                 status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n             )\n \n+        print(f\"DEBUG SUMMARY: total_rows_read={total_rows_read}, total_rows_inserted={total_rows_inserted}\")\n         return Response(\n-            {\"message\": \"File processed and data saved to database.\"},\n+            {\n\\ No newline at end of file\n+                \"message\": \"File processed and data saved to database.\",\n+                \"debug\": {\n+                    \"deleted_prior_results\": deleted_count,\n+                    \"rows_read\": total_rows_read,\n+                    \"rows_inserted\": total_rows_inserted\n+                }\n+            },\n             status=status.HTTP_200_OK,\n-        )\n+        )\n"
                },
                {
                    "date": 1749106416989,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -125,9 +125,9 @@\n             return Response(\n                 {\n                     \"error\": (\n                         \"'mapping' must contain exactly these keys: \"\n-                        \"student_name, symbol_no, dateofbirth, cgpa, remarks\"\n+                        \", symbol_no, dateofbirth, cgpa, remarks\"\n                     )\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n@@ -303,5 +303,5 @@\n                     \"rows_inserted\": total_rows_inserted\n                 }\n             },\n             status=status.HTTP_200_OK,\n-        )\n\\ No newline at end of file\n+        )\n"
                },
                {
                    "date": 1749106424228,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,15 +119,15 @@\n                 {\"error\": \"'mapping' must be a JSON string or JSON object.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        required_keys = {\"student_name\", \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        required_keys = { \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n                     \"error\": (\n                         \"'mapping' must contain exactly these keys: \"\n-                        \", symbol_no, dateofbirth, cgpa, remarks\"\n+                        \"student_name, symbol_no, dateofbirth, cgpa, remarks\"\n                     )\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n"
                },
                {
                    "date": 1749106544042,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,30 +57,38 @@\n     @action(\n         detail=False,\n         methods=[\"post\"],\n         url_path=\"bulk-import\",\n-        # permission_classes=[IsAuthenticated],\n+        # permission_classes=[IsAuthenticated],  # enable if necessary\n     )\n-    @action(\n-        detail=False,\n-        methods=[\"post\"],\n-        url_path=\"bulk-import\",\n-        # permission_classes=[IsAuthenticated],  # enable if needed\n-    )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n+        Expects JSON body:\n+        {\n+          \"file_id\":      <existing File PK>,\n+          \"name\":         \"<a display name to store on File.name>\",\n+          \"mapping\": {\n+            \"student_name\": \"<column_name_for_student_name>\",\n+            \"symbol_no\":    \"<column_name_for_symbol_no>\",\n+            \"dateofbirth\":  \"<column_name_for_dateofbirth>\",\n+            \"cgpa\":         \"<column_name_for_cgpa>\",\n+            \"remarks\":      \"<column_name_for_remarks>\"\n+          }\n+        }\n+\n+        Behavior:\n         1) Look up File by ID.\n-        2) Delete existing Results for that File.\n-        3) Save “name” onto File.name.\n+        2) If any Result rows already exist for that File, delete them. If none exist, skip deletion.\n+        3) Save the incoming \"name\" onto File.name.\n         4) Save mapping JSON onto File.mapped_json.\n-        5) Stream through the CSV/Excel, building Result rows.\n-           Print debug info on rows read and inserted.\n+        5) Stream through File.file (CSV or Excel), bulk-create new Result rows.\n         \"\"\"\n-        # 1) Read top-level inputs\n+        # ─── 1) Read top‐level inputs ─────────────────────────────────────\n         file_id = request.data.get(\"file_id\")\n         name_value = request.data.get(\"name\", \"\").strip()\n         mapping_json = request.data.get(\"mapping\")\n \n+        # Validate file_id\n         if file_id is None:\n             return Response(\n                 {\"error\": \"Missing 'file_id'. Please send an existing File ID.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n@@ -92,14 +100,16 @@\n                 {\"error\": f\"File with ID={file_id} not found.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n+        # Validate name (to store on File.name)\n         if not name_value:\n             return Response(\n                 {\"error\": \"Missing 'name'. Please send a non‐empty name string.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n+        # Parse mapping JSON\n         if mapping_json is None:\n             return Response(\n                 {\"error\": \"Missing 'mapping'. Please send JSON under key 'mapping'.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n@@ -119,9 +129,10 @@\n                 {\"error\": \"'mapping' must be a JSON string or JSON object.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        required_keys = { \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        # Ensure exactly five keys in mapping: student_name, symbol_no, dateofbirth, cgpa, remarks\n+        required_keys = {\"student_name\", \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n                     \"error\": (\n@@ -131,21 +142,21 @@\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # 2) Delete existing Results\n-        deleted_count, _ = Result.objects.filter(file=file_obj).delete()\n-        print(f\"DEBUG: deleted {deleted_count} existing Result rows for file_id {file_id}\")\n+        # ─── 2) Delete existing Results for this File only if they exist ─\n+        if Result.objects.filter(file=file_obj).exists():\n+            Result.objects.filter(file=file_obj).delete()\n \n-        # 3) Save name onto File.name\n+        # ─── 3) Save name_value onto File.name ───────────────────────────\n         file_obj.name = name_value\n         file_obj.save(update_fields=[\"name\"])\n \n-        # 4) Save mapping onto File.mapped_json\n+        # ─── 4) Save mapping JSON onto File.mapped_json ─────────────────\n         file_obj.mapped_json = mapping\n         file_obj.save(update_fields=[\"mapped_json\"])\n \n-        # 5) Determine file type by extension\n+        # ─── 5) Determine file type by extension of the stored file ─────\n         filename_lower = file_obj.file.name.lower()\n         if filename_lower.endswith(\".csv\"):\n             file_type = \"csv\"\n         elif filename_lower.endswith((\".xlsx\", \".xls\")):\n@@ -161,67 +172,65 @@\n \n         def normalize_header(hdr: str) -> str:\n             return str(hdr).strip()\n \n-        total_rows_read = 0\n-        total_rows_inserted = 0\n-\n         try:\n             with transaction.atomic():\n                 if file_type == \"csv\":\n-                    # Open in binary, wrap for text\n+                    # ─── CSV path: open in binary and wrap for text ───────────\n                     file_obj.file.open(\"rb\")\n                     text_stream = io.TextIOWrapper(\n                         file_obj.file, encoding=\"utf-8\", newline=\"\"\n                     )\n                     reader = csv.DictReader(text_stream)\n \n-                    # Print the header row we see:\n-                    print(\"DEBUG: CSV header row is:\", reader.fieldnames)\n+                    # Check header row\n+                    csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n \n-                    for row_num, row in enumerate(reader, start=1):\n-                        total_rows_read += 1\n+                    # Verify each mapped column exists\n+                    for _, col_name in mapping.items():\n+                        if normalize_header(col_name) not in csv_headers:\n+                            raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n+\n+                    # Stream each row\n+                    for row in reader:\n                         try:\n-                            sn = row[ normalize_header(mapping[\"student_name\"]) ].strip()\n-                            so = row[ normalize_header(mapping[\"symbol_no\"]) ].strip()\n-                            dob = row[ normalize_header(mapping[\"dateofbirth\"]) ].strip()\n-                            cg  = row[ normalize_header(mapping[\"cgpa\"]) ].strip()\n-                            rm  = row[ normalize_header(mapping[\"remarks\"]) ].strip()\n+                            student_name = row[normalize_header(mapping[\"student_name\"])].strip()\n+                            symbol_no    = row[normalize_header(mapping[\"symbol_no\"])].strip()\n+                            dateofbirth  = row[normalize_header(mapping[\"dateofbirth\"])].strip()\n+                            cgpa         = row[normalize_header(mapping[\"cgpa\"])].strip()\n+                            remarks      = row[normalize_header(mapping[\"remarks\"])].strip()\n \n                             instances_to_create.append(\n                                 Result(\n-                                    student_name = sn,\n-                                    file         = file_obj,\n-                                    symbol_no    = so,\n-                                    dateofbirth  = dob,\n-                                    cgpa         = cg,\n-                                    remarks      = rm,\n+                                    student_name=student_name,\n+                                    file=file_obj,\n+                                    symbol_no=symbol_no,\n+                                    dateofbirth=dateofbirth,\n+                                    cgpa=cgpa,\n+                                    remarks=remarks,\n                                 )\n                             )\n                         except KeyError as ke:\n-                            print(f\"DEBUG: Row {row_num} missing column: {ke}\")\n+                            # If a required column is missing in this row, skip the row\n                             continue\n-                        except Exception as e:\n-                            print(f\"DEBUG: Unexpected error on row {row_num}: {str(e)}\")\n+                        except Exception:\n                             continue\n \n+                        # Bulk‐insert in batches\n                         if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n-                            print(f\"DEBUG: bulk_created {len(instances_to_create)} rows\")\n-                            total_rows_inserted += len(instances_to_create)\n                             instances_to_create = []\n \n-                    # Flush any remainder\n+                    # Flush any remaining rows\n                     if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n-                        print(f\"DEBUG: final bulk_created {len(instances_to_create)} rows\")\n-                        total_rows_inserted += len(instances_to_create)\n                         instances_to_create = []\n \n                     file_obj.file.close()\n \n                 else:\n-                    # Excel path\n+                    # ─── Excel path: open in binary and pass to load_workbook ────\n                     file_obj.file.open(\"rb\")\n                     wb = load_workbook(\n                         filename=file_obj.file, read_only=True, data_only=True\n                     )\n@@ -230,54 +239,54 @@\n \n                     header_row = next(excel_iter, None)\n                     if not header_row:\n                         raise KeyError(\"Excel file appears empty (no header).\")\n+\n+                    # Build header_map: { normalized_header: index }\n                     header_map = {\n                         normalize_header(cell_val): idx\n                         for idx, cell_val in enumerate(header_row)\n                         if cell_val is not None\n                     }\n-                    print(\"DEBUG: Excel header_map is:\", header_map)\n \n-                    for row_num, row_tuple in enumerate(excel_iter, start=2):\n-                        total_rows_read += 1\n+                    # Verify each mapped column exists\n+                    for _, col_name in mapping.items():\n+                        if normalize_header(col_name) not in header_map:\n+                            raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n+\n+                    # Stream each subsequent row\n+                    for row_tuple in excel_iter:\n                         try:\n-                            sn_idx  = header_map[ normalize_header(mapping[\"student_name\"]) ]\n-                            so_idx  = header_map[ normalize_header(mapping[\"symbol_no\"]) ]\n-                            dob_idx = header_map[ normalize_header(mapping[\"dateofbirth\"]) ]\n-                            cg_idx  = header_map[ normalize_header(mapping[\"cgpa\"]) ]\n-                            rm_idx  = header_map[ normalize_header(mapping[\"remarks\"]) ]\n+                            name_idx = header_map[normalize_header(mapping[\"student_name\"])]\n+                            sn_idx   = header_map[normalize_header(mapping[\"symbol_no\"])]\n+                            dob_idx  = header_map[normalize_header(mapping[\"dateofbirth\"])]\n+                            cg_idx   = header_map[normalize_header(mapping[\"cgpa\"])]\n+                            rm_idx   = header_map[normalize_header(mapping[\"remarks\"])]\n \n                             def get_cell(r, i):\n                                 return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n \n                             instances_to_create.append(\n                                 Result(\n-                                    student_name = get_cell(row_tuple, sn_idx),\n-                                    file         = file_obj,\n-                                    symbol_no    = get_cell(row_tuple, so_idx),\n-                                    dateofbirth  = get_cell(row_tuple, dob_idx),\n-                                    cgpa         = get_cell(row_tuple, cg_idx),\n-                                    remarks      = get_cell(row_tuple, rm_idx),\n+                                    student_name=get_cell(row_tuple, name_idx),\n+                                    file=file_obj,\n+                                    symbol_no=get_cell(row_tuple, sn_idx),\n+                                    dateofbirth=get_cell(row_tuple, dob_idx),\n+                                    cgpa=get_cell(row_tuple, cg_idx),\n+                                    remarks=get_cell(row_tuple, rm_idx),\n                                 )\n                             )\n-                        except KeyError as ke:\n-                            print(f\"DEBUG: Row {row_num} missing column: {ke}\")\n+                        except KeyError:\n                             continue\n-                        except Exception as e:\n-                            print(f\"DEBUG: Unexpected error on row {row_num}: {str(e)}\")\n+                        except Exception:\n                             continue\n \n                         if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n-                            print(f\"DEBUG: bulk_created {len(instances_to_create)} rows\")\n-                            total_rows_inserted += len(instances_to_create)\n                             instances_to_create = []\n \n                     if instances_to_create:\n                         Result.objects.bulk_create(instances_to_create)\n-                        print(f\"DEBUG: final bulk_created {len(instances_to_create)} rows\")\n-                        total_rows_inserted += len(instances_to_create)\n                         instances_to_create = []\n \n                     wb.close()\n                     file_obj.file.close()\n@@ -292,16 +301,8 @@\n                 {\"error\": f\"Server error while importing: {str(e)}\"},\n                 status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n             )\n \n-        print(f\"DEBUG SUMMARY: total_rows_read={total_rows_read}, total_rows_inserted={total_rows_inserted}\")\n         return Response(\n-            {\n-                \"message\": \"File processed and data saved to database.\",\n-                \"debug\": {\n-                    \"deleted_prior_results\": deleted_count,\n-                    \"rows_read\": total_rows_read,\n-                    \"rows_inserted\": total_rows_inserted\n-                }\n-            },\n+            {\"message\": \"File processed and data saved to database.\"},\n             status=status.HTTP_200_OK,\n-        )\n+        )\n\\ No newline at end of file\n"
                },
                {
                    "date": 1749106574555,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,9 +130,9 @@\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         # Ensure exactly five keys in mapping: student_name, symbol_no, dateofbirth, cgpa, remarks\n-        required_keys = {\"student_name\", \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        required_keys = { \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n                     \"error\": (\n"
                },
                {
                    "date": 1749106768674,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,16 +57,16 @@\n     @action(\n         detail=False,\n         methods=[\"post\"],\n         url_path=\"bulk-import\",\n-        # permission_classes=[IsAuthenticated],  # enable if necessary\n+        # permission_classes=[IsAuthenticated],  # enable if needed\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         Expects JSON body:\n         {\n           \"file_id\":      <existing File PK>,\n-          \"name\":         \"<a display name to store on File.name>\",\n+          \"name\":         \"<display name to store on File.name>\",\n           \"mapping\": {\n             \"student_name\": \"<column_name_for_student_name>\",\n             \"symbol_no\":    \"<column_name_for_symbol_no>\",\n             \"dateofbirth\":  \"<column_name_for_dateofbirth>\",\n@@ -77,11 +77,12 @@\n \n         Behavior:\n         1) Look up File by ID.\n         2) If any Result rows already exist for that File, delete them. If none exist, skip deletion.\n-        3) Save the incoming \"name\" onto File.name.\n-        4) Save mapping JSON onto File.mapped_json.\n-        5) Stream through File.file (CSV or Excel), bulk-create new Result rows.\n+        3) Save incoming \"name\" onto File.name.\n+        4) Save mapping onto File.mapped_json.\n+        5) Stream through File.file (CSV or Excel), bulk-create new Result rows with:\n+             student_name, file, symbol_no, dateofbirth, cgpa, remarks.\n         \"\"\"\n         # ─── 1) Read top‐level inputs ─────────────────────────────────────\n         file_id = request.data.get(\"file_id\")\n         name_value = request.data.get(\"name\", \"\").strip()\n@@ -130,9 +131,9 @@\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         # Ensure exactly five keys in mapping: student_name, symbol_no, dateofbirth, cgpa, remarks\n-        required_keys = { \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        required_keys = {\"student_name\", \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n                     \"error\": (\n@@ -175,9 +176,9 @@\n \n         try:\n             with transaction.atomic():\n                 if file_type == \"csv\":\n-                    # ─── CSV path: open in binary and wrap for text ───────────\n+                    # ─── CSV path: open in binary then wrap for text ───────────\n                     file_obj.file.open(\"rb\")\n                     text_stream = io.TextIOWrapper(\n                         file_obj.file, encoding=\"utf-8\", newline=\"\"\n                     )\n@@ -202,17 +203,17 @@\n \n                             instances_to_create.append(\n                                 Result(\n                                     student_name=student_name,\n-                                    file=file_obj,\n+                                    file=file_obj,            # <-- file FK is set here\n                                     symbol_no=symbol_no,\n                                     dateofbirth=dateofbirth,\n                                     cgpa=cgpa,\n                                     remarks=remarks,\n                                 )\n                             )\n-                        except KeyError as ke:\n-                            # If a required column is missing in this row, skip the row\n+                        except KeyError:\n+                            # Skip rows missing any of the mapped columns\n                             continue\n                         except Exception:\n                             continue\n \n@@ -267,9 +268,9 @@\n \n                             instances_to_create.append(\n                                 Result(\n                                     student_name=get_cell(row_tuple, name_idx),\n-                                    file=file_obj,\n+                                    file=file_obj,           # <-- file FK is set here\n                                     symbol_no=get_cell(row_tuple, sn_idx),\n                                     dateofbirth=get_cell(row_tuple, dob_idx),\n                                     cgpa=get_cell(row_tuple, cg_idx),\n                                     remarks=get_cell(row_tuple, rm_idx),\n"
                },
                {
                    "date": 1749106784199,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -131,9 +131,9 @@\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         # Ensure exactly five keys in mapping: student_name, symbol_no, dateofbirth, cgpa, remarks\n-        required_keys = {\"student_name\", \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        required_keys = { \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n                     \"error\": (\n"
                },
                {
                    "date": 1749106789385,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -131,9 +131,9 @@\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         # Ensure exactly five keys in mapping: student_name, symbol_no, dateofbirth, cgpa, remarks\n-        required_keys = { \"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n+        required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n         if set(mapping.keys()) != required_keys:\n             return Response(\n                 {\n                     \"error\": (\n"
                },
                {
                    "date": 1749107121567,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,26 +63,28 @@\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         Expects JSON body:\n         {\n-          \"file_id\":      <existing File PK>,\n-          \"name\":         \"<display name to store on File.name>\",\n+          \"file_id\": <existing File PK>,\n+          \"name\":    \"<display name to store on File.name>\",\n           \"mapping\": {\n-            \"student_name\": \"<column_name_for_student_name>\",\n+            // Required:\n             \"symbol_no\":    \"<column_name_for_symbol_no>\",\n             \"dateofbirth\":  \"<column_name_for_dateofbirth>\",\n             \"cgpa\":         \"<column_name_for_cgpa>\",\n-            \"remarks\":      \"<column_name_for_remarks>\"\n+            \"remarks\":      \"<column_name_for_remarks>\",\n+            // Optional:\n+            \"student_name\": \"<column_name_for_student_name>\"\n           }\n         }\n \n         Behavior:\n         1) Look up File by ID.\n-        2) If any Result rows already exist for that File, delete them. If none exist, skip deletion.\n-        3) Save incoming \"name\" onto File.name.\n-        4) Save mapping onto File.mapped_json.\n-        5) Stream through File.file (CSV or Excel), bulk-create new Result rows with:\n-             student_name, file, symbol_no, dateofbirth, cgpa, remarks.\n+        2) If any Result rows exist for that File, delete them. Otherwise skip deletion.\n+        3) Save top‐level \"name\" onto File.name.\n+        4) Save mapping JSON onto File.mapped_json.\n+        5) Stream through File.file (CSV or Excel), bulk-create new Result rows:\n+             - If \"student_name\" was in mapping, use that column; otherwise default to name_value.\n         \"\"\"\n         # ─── 1) Read top‐level inputs ─────────────────────────────────────\n         file_id = request.data.get(\"file_id\")\n         name_value = request.data.get(\"name\", \"\").strip()\n@@ -104,9 +106,9 @@\n \n         # Validate name (to store on File.name)\n         if not name_value:\n             return Response(\n-                {\"error\": \"Missing 'name'. Please send a non‐empty name string.\"},\n+                {\"error\": \"Missing 'name'. Please send a non-empty name string.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n         # Parse mapping JSON\n@@ -130,16 +132,16 @@\n                 {\"error\": \"'mapping' must be a JSON string or JSON object.\"},\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n \n-        # Ensure exactly five keys in mapping: student_name, symbol_no, dateofbirth, cgpa, remarks\n+        # Ensure the four required keys are present; \"student_name\" is optional\n         required_keys = {\"symbol_no\", \"dateofbirth\", \"cgpa\", \"remarks\"}\n-        if set(mapping.keys()) != required_keys:\n+        if not required_keys.issubset(set(mapping.keys())):\n             return Response(\n                 {\n                     \"error\": (\n-                        \"'mapping' must contain exactly these keys: \"\n-                        \"student_name, symbol_no, dateofbirth, cgpa, remarks\"\n+                        \"'mapping' must contain at least these keys: \"\n+                        \"symbol_no, dateofbirth, cgpa, remarks\"\n                     )\n                 },\n                 status=status.HTTP_400_BAD_REQUEST,\n             )\n@@ -186,39 +188,53 @@\n \n                     # Check header row\n                     csv_headers = [normalize_header(h) for h in (reader.fieldnames or [])]\n \n-                    # Verify each mapped column exists\n-                    for _, col_name in mapping.items():\n+                    # Verify each required column exists\n+                    for col_key in required_keys:\n+                        col_name = mapping[col_key]\n                         if normalize_header(col_name) not in csv_headers:\n                             raise KeyError(f\"Column '{col_name}' not found in CSV header.\")\n \n+                    # If \"student_name\" was provided, verify it too\n+                    has_name_column = False\n+                    if \"student_name\" in mapping:\n+                        name_col = mapping[\"student_name\"]\n+                        if normalize_header(name_col) not in csv_headers:\n+                            raise KeyError(f\"Column '{name_col}' not found in CSV header.\")\n+                        has_name_column = True\n+\n                     # Stream each row\n                     for row in reader:\n                         try:\n-                            student_name = row[normalize_header(mapping[\"student_name\"])].strip()\n                             symbol_no    = row[normalize_header(mapping[\"symbol_no\"])].strip()\n                             dateofbirth  = row[normalize_header(mapping[\"dateofbirth\"])].strip()\n                             cgpa         = row[normalize_header(mapping[\"cgpa\"])].strip()\n                             remarks      = row[normalize_header(mapping[\"remarks\"])].strip()\n \n+                            if has_name_column:\n+                                student_name = row[normalize_header(mapping[\"student_name\"])].strip()\n+                            else:\n+                                student_name = name_value\n+\n                             instances_to_create.append(\n                                 Result(\n                                     student_name=student_name,\n-                                    file=file_obj,            # <-- file FK is set here\n+                                    file=file_obj,\n                                     symbol_no=symbol_no,\n                                     dateofbirth=dateofbirth,\n                                     cgpa=cgpa,\n                                     remarks=remarks,\n                                 )\n                             )\n                         except KeyError:\n-                            # Skip rows missing any of the mapped columns\n+                            # Skip any row missing required columns\n                             continue\n                         except Exception:\n+                            # Skip any other malformed row\n                             continue\n \n-                        # Bulk‐insert in batches\n+                        # Bulk-insert in batches\n                         if len(instances_to_create) >= BATCH_SIZE:\n                             Result.objects.bulk_create(instances_to_create)\n                             instances_to_create = []\n \n@@ -229,9 +245,9 @@\n \n                     file_obj.file.close()\n \n                 else:\n-                    # ─── Excel path: open in binary and pass to load_workbook ────\n+                    # ─── Excel path: open in binary and pass to openpyxl ────\n                     file_obj.file.open(\"rb\")\n                     wb = load_workbook(\n                         filename=file_obj.file, read_only=True, data_only=True\n                     )\n@@ -248,29 +264,45 @@\n                         for idx, cell_val in enumerate(header_row)\n                         if cell_val is not None\n                     }\n \n-                    # Verify each mapped column exists\n-                    for _, col_name in mapping.items():\n+                    # Verify each required column exists\n+                    for col_key in required_keys:\n+                        col_name = mapping[col_key]\n                         if normalize_header(col_name) not in header_map:\n                             raise KeyError(f\"Column '{col_name}' not found in Excel headers.\")\n \n+                    # If \"student_name\" provided, verify it too\n+                    has_name_column = False\n+                    if \"student_name\" in mapping:\n+                        name_col = mapping[\"student_name\"]\n+                        if normalize_header(name_col) not in header_map:\n+                            raise KeyError(f\"Column '{name_col}' not found in Excel headers.\")\n+                        has_name_column = True\n+\n                     # Stream each subsequent row\n                     for row_tuple in excel_iter:\n                         try:\n-                            name_idx = header_map[normalize_header(mapping[\"student_name\"])]\n-                            sn_idx   = header_map[normalize_header(mapping[\"symbol_no\"])]\n-                            dob_idx  = header_map[normalize_header(mapping[\"dateofbirth\"])]\n-                            cg_idx   = header_map[normalize_header(mapping[\"cgpa\"])]\n-                            rm_idx   = header_map[normalize_header(mapping[\"remarks\"])]\n+                            sn_idx  = header_map[normalize_header(mapping[\"symbol_no\"])]\n+                            dob_idx = header_map[normalize_header(mapping[\"dateofbirth\"])]\n+                            cg_idx  = header_map[normalize_header(mapping[\"cgpa\"])]\n+                            rm_idx  = header_map[normalize_header(mapping[\"remarks\"])]\n \n+                            if has_name_column:\n+                                name_idx = header_map[normalize_header(mapping[\"student_name\"])]\n+\n                             def get_cell(r, i):\n                                 return \"\" if (i >= len(r) or r[i] is None) else str(r[i]).strip()\n \n+                            if has_name_column:\n+                                student_name = get_cell(row_tuple, name_idx)\n+                            else:\n+                                student_name = name_value\n+\n                             instances_to_create.append(\n                                 Result(\n-                                    student_name=get_cell(row_tuple, name_idx),\n-                                    file=file_obj,           # <-- file FK is set here\n+                                    student_name=student_name,\n+                                    file=file_obj,\n                                     symbol_no=get_cell(row_tuple, sn_idx),\n                                     dateofbirth=get_cell(row_tuple, dob_idx),\n                                     cgpa=get_cell(row_tuple, cg_idx),\n                                     remarks=get_cell(row_tuple, rm_idx),\n"
                },
                {
                    "date": 1749107423554,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n     plus a custom POST /api/results/bulk-import/ for batch‐importing a CSV/Excel file.\n     \"\"\"\n     queryset = Result.objects.all().order_by(\"symbol_no\")\n     serializer_class = ResultListSerializers\n-    # permission_classes = [DynamicModelPermission]\n+    permission_classes = [DynamicModelPermission]\n     pagination_class = MyPageNumberPagination\n \n     def get_serializer_class(self):\n         if self.action in [\"create\", \"update\", \"partial_update\"]:\n"
                },
                {
                    "date": 1749107438195,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -57,9 +57,9 @@\n     @action(\n         detail=False,\n         methods=[\"post\"],\n         url_path=\"bulk-import\",\n-        # permission_classes=[IsAuthenticated],  # enable if needed\n+        permission_classes=[IsAuthenticated],  # enable if needed\n     )\n     def bulk_import(self, request, *args, **kwargs):\n         \"\"\"\n         Expects JSON body:\n"
                },
                {
                    "date": 1749365249222,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n         5) Stream through File.file (CSV or Excel), bulk-create new Result rows:\n              - If \"student_name\" was in mapping, use that column; otherwise default to name_value.\n         \"\"\"\n         # ─── 1) Read top‐level inputs ─────────────────────────────────────\n-        file_id = request.data.get(\"file_id\")\n+        file_id = request.data.get(\"file\")\n         name_value = request.data.get(\"name\", \"\").strip()\n         mapping_json = request.data.get(\"mapping\")\n \n         # Validate file_id\n"
                },
                {
                    "date": 1749365323683,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -85,9 +85,9 @@\n         5) Stream through File.file (CSV or Excel), bulk-create new Result rows:\n              - If \"student_name\" was in mapping, use that column; otherwise default to name_value.\n         \"\"\"\n         # ─── 1) Read top‐level inputs ─────────────────────────────────────\n-        file_id = request.data.get(\"file\")\n+        file_id = request.data.get(\"file_id\")\n         name_value = request.data.get(\"name\", \"\").strip()\n         mapping_json = request.data.get(\"mapping\")\n \n         # Validate file_id\n"
                },
                {
                    "date": 1749531168294,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,11 @@\n     queryset = Result.objects.all().order_by(\"symbol_no\")\n     serializer_class = ResultListSerializers\n     permission_classes = [DynamicModelPermission]\n     pagination_class = MyPageNumberPagination\n+        filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n \n+\n     def get_serializer_class(self):\n         if self.action in [\"create\", \"update\", \"partial_update\"]:\n             return ResultWriteSerializers\n         elif self.action == \"retrieve\":\n"
                },
                {
                    "date": 1749531183275,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -37,9 +37,10 @@\n     queryset = Result.objects.all().order_by(\"symbol_no\")\n     serializer_class = ResultListSerializers\n     permission_classes = [DynamicModelPermission]\n     pagination_class = MyPageNumberPagination\n-        filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n+    filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n+    \n \n \n     def get_serializer_class(self):\n         if self.action in [\"create\", \"update\", \"partial_update\"]:\n"
                },
                {
                    "date": 1749531188583,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,9 +38,9 @@\n     serializer_class = ResultListSerializers\n     permission_classes = [DynamicModelPermission]\n     pagination_class = MyPageNumberPagination\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    \n+    filter\n \n \n     def get_serializer_class(self):\n         if self.action in [\"create\", \"update\", \"partial_update\"]:\n"
                },
                {
                    "date": 1749531200324,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,9 +38,14 @@\n     serializer_class = ResultListSerializers\n     permission_classes = [DynamicModelPermission]\n     pagination_class = MyPageNumberPagination\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n-    filter\n+    filterset_fields = {\n+        'id': ['exact'],\n+        'is_active': ['exact'],\n+        'created_date': ['exact', 'gte', 'lte'],\n+        'updated_date': ['exact', 'gte', 'lte'],\n+    }\n \n \n     def get_serializer_class(self):\n         if self.action in [\"create\", \"update\", \"partial_update\"]:\n"
                },
                {
                    "date": 1749531209771,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -40,9 +40,9 @@\n     pagination_class = MyPageNumberPagination\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     filterset_fields = {\n         'id': ['exact'],\n-        'is_active': ['exact'],\n+        'symbol_no': ['exact'],\n         'created_date': ['exact', 'gte', 'lte'],\n         'updated_date': ['exact', 'gte', 'lte'],\n     }\n \n"
                },
                {
                    "date": 1749532397299,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -35,9 +35,9 @@\n     plus a custom POST /api/results/bulk-import/ for batch‐importing a CSV/Excel file.\n     \"\"\"\n     queryset = Result.objects.all().order_by(\"symbol_no\")\n     serializer_class = ResultListSerializers\n-    permission_classes = [DynamicModelPermission]\n+    # permission_classes = [DynamicModelPermission]\n     pagination_class = MyPageNumberPagination\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     filterset_fields = {\n         'id': ['exact'],\n"
                },
                {
                    "date": 1749539819065,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,8 +41,9 @@\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     filterset_fields = {\n         'id': ['exact'],\n         'symbol_no': ['exact'],\n+        \n         'created_date': ['exact', 'gte', 'lte'],\n         'updated_date': ['exact', 'gte', 'lte'],\n     }\n \n"
                },
                {
                    "date": 1749540533016,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,9 +41,9 @@\n     filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n     filterset_fields = {\n         'id': ['exact'],\n         'symbol_no': ['exact'],\n-        \n+        'file'\n         'created_date': ['exact', 'gte', 'lte'],\n         'updated_date': ['exact', 'gte', 'lte'],\n     }\n \n"
                }
            ],
            "date": 1749099839879,
            "name": "Commit-0",
            "content": "from rest_framework import viewsets\nfrom rest_framework.filters import SearchFilter, OrderingFilter\nfrom django_filters.rest_framework import DjangoFilterBackend\nfrom ..models import Result\nfrom ..serializers.result_serializers import ResultListSerializers, ResultRetrieveSerializers, ResultWriteSerializers\nfrom ..utilities.importbase import *\n\nclass resultViewsets(viewsets.ModelViewSet):\n    serializer_class = ResultListSerializers\n    # permission_classes = [resultmanagementPermission]\n    # authentication_classes = [JWTAuthentication]\n    #pagination_class = MyPageNumberPagination\n    queryset = Result.objects.all().order\n\n    filter_backends = [SearchFilter, DjangoFilterBackend, OrderingFilter]\n    search_fields = ['id']\n    ordering_fields = ['id']\n\n    # filterset_fields = {\n    #     'id': ['exact'],\n    # }\n\n    def get_queryset(self):\n        queryset = super().get_queryset()\n        #return queryset.filter(user_id=self.request.user.id)\n\n    def get_serializer_class(self):\n        if self.action in ['create', 'update', 'partial_update']:\n            return ResultWriteSerializers\n        elif self.action == 'retrieve':\n            return ResultRetrieveSerializers\n        return super().get_serializer_class()\n\n    # @action(detail=False, methods=['get'], name=\"action_name\", url_path=\"url_path\")\n    # def action_name(self, request, *args, **kwargs):\n    #     return super().list(request, *args, **kwargs)\n\n"
        }
    ]
}