{
    "sourceFile": "bucketimages/management/commands/sync_r2_bucket.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 47,
            "patches": [
                {
                    "date": 1746683926792,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1746685184866,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,83 +1,88 @@\n-# management/commands/sync_r2_bucket.py\n from django.core.management.base import BaseCommand\n from django.utils import timezone\n from ...models import BucketFile, BucketSyncLog\n+from django.conf import settings\n import boto3\n-from django.conf import settings\n-from datetime import datetime\n import logging\n \n logger = logging.getLogger(__name__)\n \n class Command(BaseCommand):\n     help = 'Synchronizes the database with the R2 bucket contents'\n-    \n+\n     def handle(self, *args, **options):\n         sync_log = BucketSyncLog.objects.create(started_at=timezone.now())\n-        \n+\n         try:\n-            # Initialize R2 client\n+            # Setup S3 client\n             s3_client = boto3.client(\n                 's3',\n                 endpoint_url=settings.AWS_S3_ENDPOINT_URL,\n                 aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n                 aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n                 region_name=settings.AWS_S3_REGION_NAME\n             )\n-            \n-            # Get all files from R2\n+\n             paginator = s3_client.get_paginator('list_objects_v2')\n             page_iterator = paginator.paginate(Bucket=settings.AWS_STORAGE_BUCKET_NAME)\n-            \n-            # Track all keys we find in R2\n+\n             r2_keys = set()\n-            \n-            # Process each page of results\n+            unchanged_files = 0\n+\n             for page in page_iterator:\n                 for obj in page.get('Contents', []):\n                     key = obj['Key']\n                     r2_keys.add(key)\n-                    \n-                    # Update or create the BucketFile record\n+\n+                    # Construct the URL (if you store it in DB)\n+                    file_url = (\n+                        f\"https://{settings.AWS_S3_CUSTOM_DOMAIN}/{key}\"\n+                        if getattr(settings, 'AWS_S3_CUSTOM_DOMAIN', None)\n+                        else f\"{settings.AWS_S3_ENDPOINT_URL}/{settings.AWS_STORAGE_BUCKET_NAME}/{key}\"\n+                    )\n+\n                     defaults = {\n                         'size': obj['Size'],\n                         'last_modified': obj['LastModified'],\n                         'content_type': obj.get('ContentType'),\n                         'extension': key.split('.')[-1].lower() if '.' in key else '',\n                         'cache_control': obj.get('CacheControl'),\n                         'etag': obj.get('ETag'),\n-                        'is_deleted': False\n+                        'url': file_url,  # only works if 'url' field exists\n+                        'is_deleted': False,\n                     }\n-                    \n+\n                     bucket_file, created = BucketFile.objects.update_or_create(\n                         key=key,\n                         defaults=defaults\n                     )\n-                    \n+\n                     if created:\n                         sync_log.new_files += 1\n+                    elif any(getattr(bucket_file, k) != v for k, v in defaults.items() if hasattr(bucket_file, k)):\n+                        sync_log.updated_files += 1\n                     else:\n-                        sync_log.updated_files += 1\n-            \n\\ No newline at end of file\n-            # Mark any files not found in R2 as deleted\n-            deleted_count = BucketFile.objects.exclude(key__in=r2_keys)\\\n-                                            .update(is_deleted=True)\n+                        unchanged_files += 1\n+\n+            # Soft-delete records not found in R2\n+            deleted_count = BucketFile.objects.exclude(key__in=r2_keys).update(is_deleted=True)\n             sync_log.deleted_files = deleted_count\n-            \n             sync_log.total_files = len(r2_keys)\n             sync_log.success = True\n             sync_log.completed_at = timezone.now()\n             sync_log.save()\n-            \n+\n             self.stdout.write(self.style.SUCCESS(\n-                f\"Successfully synced {sync_log.total_files} files. \"\n-                f\"New: {sync_log.new_files}, Updated: {sync_log.updated_files}, \"\n-                f\"Deleted: {sync_log.deleted_files}\"\n+                f\"‚úÖ Synced {sync_log.total_files} files | \"\n+                f\"üÜï New: {sync_log.new_files}, \"\n+                f\"üõ†Ô∏è Updated: {sync_log.updated_files}, \"\n+                f\"üóëÔ∏è Deleted: {sync_log.deleted_files}, \"\n+                f\"‚úÖ Unchanged: {unchanged_files}\"\n             ))\n-            \n+\n         except Exception as e:\n             sync_log.error_message = str(e)\n             sync_log.completed_at = timezone.now()\n             sync_log.save()\n-            logger.error(f\"Failed to sync R2 bucket: {str(e)}\")\n-            self.stdout.write(self.style.ERROR(f\"Sync failed: {str(e)}\"))\n+            logger.error(f\"‚ùå Failed to sync R2 bucket: {str(e)}\")\n+            self.stderr.write(self.style.ERROR(f\"‚ùå Sync failed: {str(e)}\"))\n"
                },
                {
                    "date": 1746685478730,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,18 +41,18 @@\n                         else f\"{settings.AWS_S3_ENDPOINT_URL}/{settings.AWS_STORAGE_BUCKET_NAME}/{key}\"\n                     )\n \n                     defaults = {\n-                        'size': obj['Size'],\n-                        'last_modified': obj['LastModified'],\n-                        'content_type': obj.get('ContentType'),\n-                        'extension': key.split('.')[-1].lower() if '.' in key else '',\n-                        'cache_control': obj.get('CacheControl'),\n-                        'etag': obj.get('ETag'),\n-                        'url': file_url,  # only works if 'url' field exists\n-                        'is_deleted': False,\n-                    }\n+    'size': obj['Size'],\n+    'last_modified': obj['LastModified'],\n+    'content_type': obj.get('ContentType'),\n+    'extension': key.split('.')[-1].lower() if '.' in key else '',\n+    'cache_control': obj.get('CacheControl'),\n+    'etag': obj.get('ETag'),\n+    'is_deleted': False,\n+}\n \n+\n                     bucket_file, created = BucketFile.objects.update_or_create(\n                         key=key,\n                         defaults=defaults\n                     )\n"
                },
                {
                    "date": 1746686300029,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,8 +6,14 @@\n import logging\n \n logger = logging.getLogger(__name__)\n \n+EXCLUDED_PATHS = [\n+    'backup/',\n+    'database/backup/',\n+    'backup/database/',\n+]\n+\n class Command(BaseCommand):\n     help = 'Synchronizes the database with the R2 bucket contents'\n \n     def handle(self, *args, **options):\n@@ -27,32 +33,31 @@\n             page_iterator = paginator.paginate(Bucket=settings.AWS_STORAGE_BUCKET_NAME)\n \n             r2_keys = set()\n             unchanged_files = 0\n+            skipped_files = 0\n \n             for page in page_iterator:\n                 for obj in page.get('Contents', []):\n                     key = obj['Key']\n+\n+                    # ‚úÖ Skip excluded paths\n+                    if any(key.startswith(path) for path in EXCLUDED_PATHS):\n+                        skipped_files += 1\n+                        continue\n+\n                     r2_keys.add(key)\n \n-                    # Construct the URL (if you store it in DB)\n-                    file_url = (\n-                        f\"https://{settings.AWS_S3_CUSTOM_DOMAIN}/{key}\"\n-                        if getattr(settings, 'AWS_S3_CUSTOM_DOMAIN', None)\n-                        else f\"{settings.AWS_S3_ENDPOINT_URL}/{settings.AWS_STORAGE_BUCKET_NAME}/{key}\"\n-                    )\n-\n                     defaults = {\n-    'size': obj['Size'],\n-    'last_modified': obj['LastModified'],\n-    'content_type': obj.get('ContentType'),\n-    'extension': key.split('.')[-1].lower() if '.' in key else '',\n-    'cache_control': obj.get('CacheControl'),\n-    'etag': obj.get('ETag'),\n-    'is_deleted': False,\n-}\n+                        'size': obj['Size'],\n+                        'last_modified': obj['LastModified'],\n+                        'content_type': obj.get('ContentType'),\n+                        'extension': key.split('.')[-1].lower() if '.' in key else '',\n+                        'cache_control': obj.get('CacheControl'),\n+                        'etag': obj.get('ETag'),\n+                        'is_deleted': False,\n+                    }\n \n-\n                     bucket_file, created = BucketFile.objects.update_or_create(\n                         key=key,\n                         defaults=defaults\n                     )\n@@ -63,11 +68,19 @@\n                         sync_log.updated_files += 1\n                     else:\n                         unchanged_files += 1\n \n-            # Soft-delete records not found in R2\n+            # ‚úÖ Delete all entries from excluded paths in DB\n+            excluded_deleted = 0\n+            for path in EXCLUDED_PATHS:\n+                deleted_qs = BucketFile.objects.filter(key__startswith=path)\n+                count = deleted_qs.count()\n+                excluded_deleted += count\n+                deleted_qs.delete()\n+\n+            # Soft-delete remaining files not in R2\n             deleted_count = BucketFile.objects.exclude(key__in=r2_keys).update(is_deleted=True)\n-            sync_log.deleted_files = deleted_count\n+            sync_log.deleted_files = deleted_count + excluded_deleted\n             sync_log.total_files = len(r2_keys)\n             sync_log.success = True\n             sync_log.completed_at = timezone.now()\n             sync_log.save()\n@@ -75,9 +88,11 @@\n             self.stdout.write(self.style.SUCCESS(\n                 f\"‚úÖ Synced {sync_log.total_files} files | \"\n                 f\"üÜï New: {sync_log.new_files}, \"\n                 f\"üõ†Ô∏è Updated: {sync_log.updated_files}, \"\n-                f\"üóëÔ∏è Deleted: {sync_log.deleted_files}, \"\n+                f\"üóëÔ∏è Deleted (R2-missing): {deleted_count}, \"\n+                f\"üö´ Skipped (excluded): {skipped_files}, \"\n+                f\"üßπ Cleaned (excluded from DB): {excluded_deleted}, \"\n                 f\"‚úÖ Unchanged: {unchanged_files}\"\n             ))\n \n         except Exception as e:\n"
                },
                {
                    "date": 1746687239035,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,8 +51,9 @@\n                         'size': obj['Size'],\n                         'last_modified': obj['LastModified'],\n                         'content_type': obj.get('ContentType'),\n                         'extension': key.split('.')[-1].lower() if '.' in key else '',\n+                        'url'\n                         'cache_control': obj.get('CacheControl'),\n                         'etag': obj.get('ETag'),\n                         'is_deleted': False,\n                     }\n"
                },
                {
                    "date": 1746687247208,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,9 +51,9 @@\n                         'size': obj['Size'],\n                         'last_modified': obj['LastModified'],\n                         'content_type': obj.get('ContentType'),\n                         'extension': key.split('.')[-1].lower() if '.' in key else '',\n-                        'url'\n+                        'url': obj.get('WebsiteRedirectLocation'),\n                         'cache_control': obj.get('CacheControl'),\n                         'etag': obj.get('ETag'),\n                         'is_deleted': False,\n                     }\n"
                },
                {
                    "date": 1746687262452,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,8 +10,9 @@\n EXCLUDED_PATHS = [\n     'backup/',\n     'database/backup/',\n     'backup/database/',\n+    'static/',\n ]\n \n class Command(BaseCommand):\n     help = 'Synchronizes the database with the R2 bucket contents'\n"
                },
                {
                    "date": 1746689597645,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,105 +1,106 @@\n-from django.core.management.base import BaseCommand\n-from django.utils import timezone\n-from ...models import BucketFile, BucketSyncLog\n-from django.conf import settings\n-import boto3\n-import logging\n+# from django.core.management.base import BaseCommand\n+# from django.utils import timezone\n+# from ...models import BucketFile, BucketSyncLog\n+# from django.conf import settings\n+# import boto3\n+# import logging\n \n-logger = logging.getLogger(__name__)\n+# logger = logging.getLogger(__name__)\n \n-EXCLUDED_PATHS = [\n-    'backup/',\n-    'database/backup/',\n-    'backup/database/',\n-    'static/',\n-]\n+# EXCLUDED_PATHS = [\n+#     'backup/',\n+#     'database/backup/',\n+#     'backup/database/',\n+#     'static/',\n+#     'media/',\n+# ]\n \n-class Command(BaseCommand):\n-    help = 'Synchronizes the database with the R2 bucket contents'\n+# class Command(BaseCommand):\n+#     help = 'Synchronizes the database with the R2 bucket contents'\n \n-    def handle(self, *args, **options):\n-        sync_log = BucketSyncLog.objects.create(started_at=timezone.now())\n+#     def handle(self, *args, **options):\n+#         sync_log = BucketSyncLog.objects.create(started_at=timezone.now())\n \n-        try:\n-            # Setup S3 client\n-            s3_client = boto3.client(\n-                's3',\n-                endpoint_url=settings.AWS_S3_ENDPOINT_URL,\n-                aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n-                aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n-                region_name=settings.AWS_S3_REGION_NAME\n-            )\n+#         try:\n+#             # Setup S3 client\n+#             s3_client = boto3.client(\n+#                 's3',\n+#                 endpoint_url=settings.AWS_S3_ENDPOINT_URL,\n+#                 aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n+#                 aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n+#                 region_name=settings.AWS_S3_REGION_NAME\n+#             )\n \n-            paginator = s3_client.get_paginator('list_objects_v2')\n-            page_iterator = paginator.paginate(Bucket=settings.AWS_STORAGE_BUCKET_NAME)\n+#             paginator = s3_client.get_paginator('list_objects_v2')\n+#             page_iterator = paginator.paginate(Bucket=settings.AWS_STORAGE_BUCKET_NAME)\n \n-            r2_keys = set()\n-            unchanged_files = 0\n-            skipped_files = 0\n+#             r2_keys = set()\n+#             unchanged_files = 0\n+#             skipped_files = 0\n \n-            for page in page_iterator:\n-                for obj in page.get('Contents', []):\n-                    key = obj['Key']\n+#             for page in page_iterator:\n+#                 for obj in page.get('Contents', []):\n+#                     key = obj['Key']\n \n-                    # ‚úÖ Skip excluded paths\n-                    if any(key.startswith(path) for path in EXCLUDED_PATHS):\n-                        skipped_files += 1\n-                        continue\n+#                     # ‚úÖ Skip excluded paths\n+#                     if any(key.startswith(path) for path in EXCLUDED_PATHS):\n+#                         skipped_files += 1\n+#                         continue\n \n-                    r2_keys.add(key)\n+#                     r2_keys.add(key)\n \n-                    defaults = {\n-                        'size': obj['Size'],\n-                        'last_modified': obj['LastModified'],\n-                        'content_type': obj.get('ContentType'),\n-                        'extension': key.split('.')[-1].lower() if '.' in key else '',\n-                        'url': obj.get('WebsiteRedirectLocation'),\n-                        'cache_control': obj.get('CacheControl'),\n-                        'etag': obj.get('ETag'),\n-                        'is_deleted': False,\n-                    }\n+#                     defaults = {\n+#                         'size': obj['Size'],\n+#                         'last_modified': obj['LastModified'],\n+#                         'content_type': obj.get('ContentType'),\n+#                         'extension': key.split('.')[-1].lower() if '.' in key else '',\n+#                         'url': obj.get('WebsiteRedirectLocation'),\n+#                         'cache_control': obj.get('CacheControl'),\n+#                         'etag': obj.get('ETag'),\n+#                         'is_deleted': False,\n+#                     }\n \n-                    bucket_file, created = BucketFile.objects.update_or_create(\n-                        key=key,\n-                        defaults=defaults\n-                    )\n+#                     bucket_file, created = BucketFile.objects.update_or_create(\n+#                         key=key,\n+#                         defaults=defaults\n+#                     )\n \n-                    if created:\n-                        sync_log.new_files += 1\n-                    elif any(getattr(bucket_file, k) != v for k, v in defaults.items() if hasattr(bucket_file, k)):\n-                        sync_log.updated_files += 1\n-                    else:\n-                        unchanged_files += 1\n+#                     if created:\n+#                         sync_log.new_files += 1\n+#                     elif any(getattr(bucket_file, k) != v for k, v in defaults.items() if hasattr(bucket_file, k)):\n+#                         sync_log.updated_files += 1\n+#                     else:\n+#                         unchanged_files += 1\n \n-            # ‚úÖ Delete all entries from excluded paths in DB\n-            excluded_deleted = 0\n-            for path in EXCLUDED_PATHS:\n-                deleted_qs = BucketFile.objects.filter(key__startswith=path)\n-                count = deleted_qs.count()\n-                excluded_deleted += count\n-                deleted_qs.delete()\n+#             # ‚úÖ Delete all entries from excluded paths in DB\n+#             excluded_deleted = 0\n+#             for path in EXCLUDED_PATHS:\n+#                 deleted_qs = BucketFile.objects.filter(key__startswith=path)\n+#                 count = deleted_qs.count()\n+#                 excluded_deleted += count\n+#                 deleted_qs.delete()\n \n-            # Soft-delete remaining files not in R2\n-            deleted_count = BucketFile.objects.exclude(key__in=r2_keys).update(is_deleted=True)\n-            sync_log.deleted_files = deleted_count + excluded_deleted\n-            sync_log.total_files = len(r2_keys)\n-            sync_log.success = True\n-            sync_log.completed_at = timezone.now()\n-            sync_log.save()\n+#             # Soft-delete remaining files not in R2\n+#             deleted_count = BucketFile.objects.exclude(key__in=r2_keys).update(is_deleted=True)\n+#             sync_log.deleted_files = deleted_count + excluded_deleted\n+#             sync_log.total_files = len(r2_keys)\n+#             sync_log.success = True\n+#             sync_log.completed_at = timezone.now()\n+#             sync_log.save()\n \n-            self.stdout.write(self.style.SUCCESS(\n-                f\"‚úÖ Synced {sync_log.total_files} files | \"\n-                f\"üÜï New: {sync_log.new_files}, \"\n-                f\"üõ†Ô∏è Updated: {sync_log.updated_files}, \"\n-                f\"üóëÔ∏è Deleted (R2-missing): {deleted_count}, \"\n-                f\"üö´ Skipped (excluded): {skipped_files}, \"\n-                f\"üßπ Cleaned (excluded from DB): {excluded_deleted}, \"\n-                f\"‚úÖ Unchanged: {unchanged_files}\"\n-            ))\n+#             self.stdout.write(self.style.SUCCESS(\n+#                 f\"‚úÖ Synced {sync_log.total_files} files | \"\n+#                 f\"üÜï New: {sync_log.new_files}, \"\n+#                 f\"üõ†Ô∏è Updated: {sync_log.updated_files}, \"\n+#                 f\"üóëÔ∏è Deleted (R2-missing): {deleted_count}, \"\n+#                 f\"üö´ Skipped (excluded): {skipped_files}, \"\n+#                 f\"üßπ Cleaned (excluded from DB): {excluded_deleted}, \"\n+#                 f\"‚úÖ Unchanged: {unchanged_files}\"\n+#             ))\n \n-        except Exception as e:\n-            sync_log.error_message = str(e)\n-            sync_log.completed_at = timezone.now()\n-            sync_log.save()\n-            logger.error(f\"‚ùå Failed to sync R2 bucket: {str(e)}\")\n-            self.stderr.write(self.style.ERROR(f\"‚ùå Sync failed: {str(e)}\"))\n+#         except Exception as e:\n+#             sync_log.error_message = str(e)\n+#             sync_log.completed_at = timezone.now()\n+#             sync_log.save()\n+#             logger.error(f\"‚ùå Failed to sync R2 bucket: {str(e)}\")\n+#             self.stderr.write(self.style.ERROR(f\"‚ùå Sync failed: {str(e)}\"))\n"
                },
                {
                    "date": 1746689621112,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,4 +103,104 @@\n #             sync_log.completed_at = timezone.now()\n #             sync_log.save()\n #             logger.error(f\"‚ùå Failed to sync R2 bucket: {str(e)}\")\n #             self.stderr.write(self.style.ERROR(f\"‚ùå Sync failed: {str(e)}\"))\n+\n+\n+import os\n+from mimetypes import guess_type\n+from django.core.management.base import BaseCommand\n+from django.utils import timezone\n+from bucket.models import BucketFile  # change if BucketFile is in another app\n+\n+# Models\n+from accounts.models import CustomUser\n+from collegemanagement.models import College, CollegeGallery\n+from advertisement.models import Advertisement\n+from affiliation.models import Affiliation\n+from certification.models import Certification\n+from coursemanagement.models import Course, CourseCurriculumFile\n+from event.models import EventGallery, Event, EventOrganizer\n+from facilities.models import Facility, CollegeFacility\n+from gallery.models import Gallery\n+from informationmanagement.models import (\n+    InformationGallery, Information, InformationFiles, InformationCategory\n+)\n+from level.models import Level, SubLevel\n+from popup.models import Popup\n+from socialmedia.models import SocialMedia\n+\n+class Command(BaseCommand):\n+    help = \"Extracts file fields from models and stores them in BucketFile table\"\n+\n+    def add_arguments(self, parser):\n+        parser.add_argument(\n+            '--dry-run',\n+            action='store_true',\n+            help='Simulate the extraction without writing to the database',\n+        )\n+\n+    def handle(self, *args, **options):\n+        dry_run = options['dry_run']\n+        total_processed = 0\n+\n+        model_config = [\n+            {\"model\": CustomUser, \"fields\": [\"professional_image\"]},\n+            {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\"]},\n+            {\"model\": CollegeGallery, \"fields\": [\"image\"]},\n+            {\"model\": Advertisement, \"fields\": [\"image\"]},\n+            {\"model\": Affiliation, \"fields\": [\"logo\"]},\n+            {\"model\": Certification, \"fields\": [\"certificate_image\"]},\n+            {\"model\": Course, \"fields\": [\"thumbnail\"]},\n+            {\"model\": CourseCurriculumFile, \"fields\": [\"file\"]},\n+            {\"model\": EventGallery, \"fields\": [\"image\"]},\n+            {\"model\": Event, \"fields\": [\"main_image\"]},\n+            {\"model\": EventOrganizer, \"fields\": [\"logo\"]},\n+            {\"model\": Facility, \"fields\": [\"icon\"]},\n+            {\"model\": CollegeFacility, \"fields\": [\"image\"]},\n+            {\"model\": Gallery, \"fields\": [\"image\"]},\n+            {\"model\": InformationGallery, \"fields\": [\"file\"]},\n+            {\"model\": Information, \"fields\": [\"cover_image\"]},\n+            {\"model\": InformationFiles, \"fields\": [\"file\"]},\n+            {\"model\": InformationCategory, \"fields\": [\"icon\"]},\n+            {\"model\": Level, \"fields\": [\"icon\"]},\n+            {\"model\": SubLevel, \"fields\": [\"icon\"]},\n+            {\"model\": Popup, \"fields\": [\"image\"]},\n+            {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n+        ]\n+\n+        self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n+\n+        for config in model_config:\n+            model = config[\"model\"]\n+            fields = config[\"fields\"]\n+            model_name = f\"{model._meta.app_label}.{model.__name__}\"\n+            self.stdout.write(f\"üîç Processing model: {model_name}\")\n+\n+            for field_name in fields:\n+                count = 0\n+                for instance in model.objects.iterator():\n+                    file = getattr(instance, field_name, None)\n+                    if file and getattr(file, 'name', None):\n+                        key = file.name\n+                        extension = os.path.splitext(key)[1].lstrip(\".\")\n+                        content_type = guess_type(key)[0] or \"application/octet-stream\"\n+                        size = getattr(file, 'size', 0)\n+\n+                        if dry_run:\n+                            self.stdout.write(f\"    [DRY RUN] Would process file: {key}\")\n+                        else:\n+                            BucketFile.objects.update_or_create(\n+                                key=key,\n+                                defaults={\n+                                    'size': size,\n+                                    'last_modified': timezone.now(),\n+                                    'content_type': content_type,\n+                                    'extension': extension,\n+                                }\n+                            )\n+                            count += 1\n+                            total_processed += 1\n+\n+                self.stdout.write(f\"    ‚úÖ {count} files processed for field: {field_name}\")\n+\n+        self.stdout.write(f\"\\nüéâ File extraction {'simulated' if dry_run else 'completed'}. Total files processed: {total_processed}\")\n"
                },
                {
                    "date": 1746689627348,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,9 +143,9 @@\n         dry_run = options['dry_run']\n         total_processed = 0\n \n         model_config = [\n-            {\"model\": CustomUser, \"fields\": [\"professional_image\"]},\n+            {\"model\": CustomUser, \"fields\": [\"professional_image\",\"\"]},\n             {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\"]},\n             {\"model\": CollegeGallery, \"fields\": [\"image\"]},\n             {\"model\": Advertisement, \"fields\": [\"image\"]},\n             {\"model\": Affiliation, \"fields\": [\"logo\"]},\n"
                },
                {
                    "date": 1746689653820,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,10 +143,10 @@\n         dry_run = options['dry_run']\n         total_processed = 0\n \n         model_config = [\n-            {\"model\": CustomUser, \"fields\": [\"professional_image\",\"\"]},\n-            {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\"]},\n+            {\"model\": CustomUser, \"fields\": [\"professional_image\",\"avatar\"]},\n+            {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\",\"brochure\"]},\n             {\"model\": CollegeGallery, \"fields\": [\"image\"]},\n             {\"model\": Advertisement, \"fields\": [\"image\"]},\n             {\"model\": Affiliation, \"fields\": [\"logo\"]},\n             {\"model\": Certification, \"fields\": [\"certificate_image\"]},\n"
                },
                {
                    "date": 1746689683106,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,9 +147,9 @@\n             {\"model\": CustomUser, \"fields\": [\"professional_image\",\"avatar\"]},\n             {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\",\"brochure\"]},\n             {\"model\": CollegeGallery, \"fields\": [\"image\"]},\n             {\"model\": Advertisement, \"fields\": [\"image\"]},\n-            {\"model\": Affiliation, \"fields\": [\"logo\"]},\n+            {\"model\": Affiliation, \"fields\": [\"logo_image\"]},\n             {\"model\": Certification, \"fields\": [\"certificate_image\"]},\n             {\"model\": Course, \"fields\": [\"thumbnail\"]},\n             {\"model\": CourseCurriculumFile, \"fields\": [\"file\"]},\n             {\"model\": EventGallery, \"fields\": [\"image\"]},\n"
                },
                {
                    "date": 1746689697110,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -147,10 +147,10 @@\n             {\"model\": CustomUser, \"fields\": [\"professional_image\",\"avatar\"]},\n             {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\",\"brochure\"]},\n             {\"model\": CollegeGallery, \"fields\": [\"image\"]},\n             {\"model\": Advertisement, \"fields\": [\"image\"]},\n-            {\"model\": Affiliation, \"fields\": [\"logo_image\"]},\n-            {\"model\": Certification, \"fields\": [\"certificate_image\"]},\n+            {\"model\": Affiliation, \"fields\": [\"logo_image\",\"cover_image\"]},\n+            {\"model\": Certification, \"fields\": [\"image\"]},\n             {\"model\": Course, \"fields\": [\"thumbnail\"]},\n             {\"model\": CourseCurriculumFile, \"fields\": [\"file\"]},\n             {\"model\": EventGallery, \"fields\": [\"image\"]},\n             {\"model\": Event, \"fields\": [\"main_image\"]},\n"
                },
                {
                    "date": 1746689714797,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,9 +149,9 @@\n             {\"model\": CollegeGallery, \"fields\": [\"image\"]},\n             {\"model\": Advertisement, \"fields\": [\"image\"]},\n             {\"model\": Affiliation, \"fields\": [\"logo_image\",\"cover_image\"]},\n             {\"model\": Certification, \"fields\": [\"image\"]},\n-            {\"model\": Course, \"fields\": [\"thumbnail\"]},\n+            {\"model\": Course, \"fields\": [\"image\"]},\n             {\"model\": CourseCurriculumFile, \"fields\": [\"file\"]},\n             {\"model\": EventGallery, \"fields\": [\"image\"]},\n             {\"model\": Event, \"fields\": [\"main_image\"]},\n             {\"model\": EventOrganizer, \"fields\": [\"logo\"]},\n"
                },
                {
                    "date": 1746689725633,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,9 +150,9 @@\n             {\"model\": Advertisement, \"fields\": [\"image\"]},\n             {\"model\": Affiliation, \"fields\": [\"logo_image\",\"cover_image\"]},\n             {\"model\": Certification, \"fields\": [\"image\"]},\n             {\"model\": Course, \"fields\": [\"image\"]},\n-            {\"model\": CourseCurriculumFile, \"fields\": [\"file\"]},\n+            {\"model\": CourseCurriculumFile, \"fields\": [\"curriculum_file_upload\"]},\n             {\"model\": EventGallery, \"fields\": [\"image\"]},\n             {\"model\": Event, \"fields\": [\"main_image\"]},\n             {\"model\": EventOrganizer, \"fields\": [\"logo\"]},\n             {\"model\": Facility, \"fields\": [\"icon\"]},\n"
                },
                {
                    "date": 1746689752976,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -152,9 +152,9 @@\n             {\"model\": Certification, \"fields\": [\"image\"]},\n             {\"model\": Course, \"fields\": [\"image\"]},\n             {\"model\": CourseCurriculumFile, \"fields\": [\"curriculum_file_upload\"]},\n             {\"model\": EventGallery, \"fields\": [\"image\"]},\n-            {\"model\": Event, \"fields\": [\"main_image\"]},\n+            {\"model\": Event, \"fields\": [\"featured_image\"]},\n             {\"model\": EventOrganizer, \"fields\": [\"logo\"]},\n             {\"model\": Facility, \"fields\": [\"icon\"]},\n             {\"model\": CollegeFacility, \"fields\": [\"image\"]},\n             {\"model\": Gallery, \"fields\": [\"image\"]},\n"
                },
                {
                    "date": 1746689758368,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,206 @@\n+# from django.core.management.base import BaseCommand\n+# from django.utils import timezone\n+# from ...models import BucketFile, BucketSyncLog\n+# from django.conf import settings\n+# import boto3\n+# import logging\n+\n+# logger = logging.getLogger(__name__)\n+\n+# EXCLUDED_PATHS = [\n+#     'backup/',\n+#     'database/backup/',\n+#     'backup/database/',\n+#     'static/',\n+#     'media/',\n+# ]\n+\n+# class Command(BaseCommand):\n+#     help = 'Synchronizes the database with the R2 bucket contents'\n+\n+#     def handle(self, *args, **options):\n+#         sync_log = BucketSyncLog.objects.create(started_at=timezone.now())\n+\n+#         try:\n+#             # Setup S3 client\n+#             s3_client = boto3.client(\n+#                 's3',\n+#                 endpoint_url=settings.AWS_S3_ENDPOINT_URL,\n+#                 aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n+#                 aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n+#                 region_name=settings.AWS_S3_REGION_NAME\n+#             )\n+\n+#             paginator = s3_client.get_paginator('list_objects_v2')\n+#             page_iterator = paginator.paginate(Bucket=settings.AWS_STORAGE_BUCKET_NAME)\n+\n+#             r2_keys = set()\n+#             unchanged_files = 0\n+#             skipped_files = 0\n+\n+#             for page in page_iterator:\n+#                 for obj in page.get('Contents', []):\n+#                     key = obj['Key']\n+\n+#                     # ‚úÖ Skip excluded paths\n+#                     if any(key.startswith(path) for path in EXCLUDED_PATHS):\n+#                         skipped_files += 1\n+#                         continue\n+\n+#                     r2_keys.add(key)\n+\n+#                     defaults = {\n+#                         'size': obj['Size'],\n+#                         'last_modified': obj['LastModified'],\n+#                         'content_type': obj.get('ContentType'),\n+#                         'extension': key.split('.')[-1].lower() if '.' in key else '',\n+#                         'url': obj.get('WebsiteRedirectLocation'),\n+#                         'cache_control': obj.get('CacheControl'),\n+#                         'etag': obj.get('ETag'),\n+#                         'is_deleted': False,\n+#                     }\n+\n+#                     bucket_file, created = BucketFile.objects.update_or_create(\n+#                         key=key,\n+#                         defaults=defaults\n+#                     )\n+\n+#                     if created:\n+#                         sync_log.new_files += 1\n+#                     elif any(getattr(bucket_file, k) != v for k, v in defaults.items() if hasattr(bucket_file, k)):\n+#                         sync_log.updated_files += 1\n+#                     else:\n+#                         unchanged_files += 1\n+\n+#             # ‚úÖ Delete all entries from excluded paths in DB\n+#             excluded_deleted = 0\n+#             for path in EXCLUDED_PATHS:\n+#                 deleted_qs = BucketFile.objects.filter(key__startswith=path)\n+#                 count = deleted_qs.count()\n+#                 excluded_deleted += count\n+#                 deleted_qs.delete()\n+\n+#             # Soft-delete remaining files not in R2\n+#             deleted_count = BucketFile.objects.exclude(key__in=r2_keys).update(is_deleted=True)\n+#             sync_log.deleted_files = deleted_count + excluded_deleted\n+#             sync_log.total_files = len(r2_keys)\n+#             sync_log.success = True\n+#             sync_log.completed_at = timezone.now()\n+#             sync_log.save()\n+\n+#             self.stdout.write(self.style.SUCCESS(\n+#                 f\"‚úÖ Synced {sync_log.total_files} files | \"\n+#                 f\"üÜï New: {sync_log.new_files}, \"\n+#                 f\"üõ†Ô∏è Updated: {sync_log.updated_files}, \"\n+#                 f\"üóëÔ∏è Deleted (R2-missing): {deleted_count}, \"\n+#                 f\"üö´ Skipped (excluded): {skipped_files}, \"\n+#                 f\"üßπ Cleaned (excluded from DB): {excluded_deleted}, \"\n+#                 f\"‚úÖ Unchanged: {unchanged_files}\"\n+#             ))\n+\n+#         except Exception as e:\n+#             sync_log.error_message = str(e)\n+#             sync_log.completed_at = timezone.now()\n+#             sync_log.save()\n+#             logger.error(f\"‚ùå Failed to sync R2 bucket: {str(e)}\")\n+#             self.stderr.write(self.style.ERROR(f\"‚ùå Sync failed: {str(e)}\"))\n+\n+\n+import os\n+from mimetypes import guess_type\n+from django.core.management.base import BaseCommand\n+from django.utils import timezone\n+from bucket.models import BucketFile  # change if BucketFile is in another app\n+\n+# Models\n+from accounts.models import CustomUser\n+from collegemanagement.models import College, CollegeGallery\n+from advertisement.models import Advertisement\n+from affiliation.models import Affiliation\n+from certification.models import Certification\n+from coursemanagement.models import Course, CourseCurriculumFile\n+from event.models import EventGallery, Event, EventOrganizer\n+from facilities.models import Facility, CollegeFacility\n+from gallery.models import Gallery\n+from informationmanagement.models import (\n+    InformationGallery, Information, InformationFiles, InformationCategory\n+)\n+from level.models import Level, SubLevel\n+from popup.models import Popup\n+from socialmedia.models import SocialMedia\n+\n+class Command(BaseCommand):\n+    help = \"Extracts file fields from models and stores them in BucketFile table\"\n+\n+    def add_arguments(self, parser):\n+        parser.add_argument(\n+            '--dry-run',\n+            action='store_true',\n+            help='Simulate the extraction without writing to the database',\n+        )\n+\n+    def handle(self, *args, **options):\n+        dry_run = options['dry_run']\n+        total_processed = 0\n+\n+        model_config = [\n+            {\"model\": CustomUser, \"fields\": [\"professional_image\",\"avatar\"]},\n+            {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\",\"brochure\"]},\n+            {\"model\": CollegeGallery, \"fields\": [\"image\"]},\n+            {\"model\": Advertisement, \"fields\": [\"image\"]},\n+            {\"model\": Affiliation, \"fields\": [\"logo_image\",\"cover_image\"]},\n+            {\"model\": Certification, \"fields\": [\"image\"]},\n+            {\"model\": Course, \"fields\": [\"image\"]},\n+            {\"model\": CourseCurriculumFile, \"fields\": [\"curriculum_file_upload\"]},\n+            {\"model\": EventGallery, \"fields\": [\"image\"]},\n+            {\"model\": Event, \"fields\": [\"featured_image\"]},\n+            {\"model\": EventOrganizer, \"fields\": [\"image\"]},\n+            {\"model\": Facility, \"fields\": [\"icon\"]},\n+            {\"model\": CollegeFacility, \"fields\": [\"image\"]},\n+            {\"model\": Gallery, \"fields\": [\"image\"]},\n+            {\"model\": InformationGallery, \"fields\": [\"file\"]},\n+            {\"model\": Information, \"fields\": [\"cover_image\"]},\n+            {\"model\": InformationFiles, \"fields\": [\"file\"]},\n+            {\"model\": InformationCategory, \"fields\": [\"icon\"]},\n+            {\"model\": Level, \"fields\": [\"icon\"]},\n+            {\"model\": SubLevel, \"fields\": [\"icon\"]},\n+            {\"model\": Popup, \"fields\": [\"image\"]},\n+            {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n+        ]\n+\n+        self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n+\n+        for config in model_config:\n+            model = config[\"model\"]\n+            fields = config[\"fields\"]\n+            model_name = f\"{model._meta.app_label}.{model.__name__}\"\n+            self.stdout.write(f\"üîç Processing model: {model_name}\")\n+\n+            for field_name in fields:\n+                count = 0\n+                for instance in model.objects.iterator():\n+                    file = getattr(instance, field_name, None)\n+                    if file and getattr(file, 'name', None):\n+                        key = file.name\n+                        extension = os.path.splitext(key)[1].lstrip(\".\")\n+                        content_type = guess_type(key)[0] or \"application/octet-stream\"\n+                        size = getattr(file, 'size', 0)\n+\n+                        if dry_run:\n+                            self.stdout.write(f\"    [DRY RUN] Would process file: {key}\")\n+                        else:\n+                            BucketFile.objects.update_or_create(\n+                                key=key,\n+                                defaults={\n+                                    'size': size,\n+                                    'last_modified': timezone.now(),\n+                                    'content_type': content_type,\n+                                    'extension': extension,\n+                                }\n+                            )\n+                            count += 1\n+                            total_processed += 1\n+\n+                self.stdout.write(f\"    ‚úÖ {count} files processed for field: {field_name}\")\n+\n+        self.stdout.write(f\"\\nüéâ File extraction {'simulated' if dry_run else 'completed'}. Total files processed: {total_processed}\")\n"
                },
                {
                    "date": 1746689763738,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -154,9 +154,9 @@\n             {\"model\": CourseCurriculumFile, \"fields\": [\"curriculum_file_upload\"]},\n             {\"model\": EventGallery, \"fields\": [\"image\"]},\n             {\"model\": Event, \"fields\": [\"featured_image\"]},\n             {\"model\": EventOrganizer, \"fields\": [\"image\"]},\n-            {\"model\": Facility, \"fields\": [\"icon\"]},\n+            {\"model\": Facility, \"fields\": [\"image\"]},\n             {\"model\": CollegeFacility, \"fields\": [\"image\"]},\n             {\"model\": Gallery, \"fields\": [\"image\"]},\n             {\"model\": InformationGallery, \"fields\": [\"file\"]},\n             {\"model\": Information, \"fields\": [\"cover_image\"]},\n@@ -203,210 +203,4 @@\n \n                 self.stdout.write(f\"    ‚úÖ {count} files processed for field: {field_name}\")\n \n         self.stdout.write(f\"\\nüéâ File extraction {'simulated' if dry_run else 'completed'}. Total files processed: {total_processed}\")\n-# from django.core.management.base import BaseCommand\n-# from django.utils import timezone\n-# from ...models import BucketFile, BucketSyncLog\n-# from django.conf import settings\n-# import boto3\n-# import logging\n-\n-# logger = logging.getLogger(__name__)\n-\n-# EXCLUDED_PATHS = [\n-#     'backup/',\n-#     'database/backup/',\n-#     'backup/database/',\n-#     'static/',\n-#     'media/',\n-# ]\n-\n-# class Command(BaseCommand):\n-#     help = 'Synchronizes the database with the R2 bucket contents'\n-\n-#     def handle(self, *args, **options):\n-#         sync_log = BucketSyncLog.objects.create(started_at=timezone.now())\n-\n-#         try:\n-#             # Setup S3 client\n-#             s3_client = boto3.client(\n-#                 's3',\n-#                 endpoint_url=settings.AWS_S3_ENDPOINT_URL,\n-#                 aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n-#                 aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n-#                 region_name=settings.AWS_S3_REGION_NAME\n-#             )\n-\n-#             paginator = s3_client.get_paginator('list_objects_v2')\n-#             page_iterator = paginator.paginate(Bucket=settings.AWS_STORAGE_BUCKET_NAME)\n-\n-#             r2_keys = set()\n-#             unchanged_files = 0\n-#             skipped_files = 0\n-\n-#             for page in page_iterator:\n-#                 for obj in page.get('Contents', []):\n-#                     key = obj['Key']\n-\n-#                     # ‚úÖ Skip excluded paths\n-#                     if any(key.startswith(path) for path in EXCLUDED_PATHS):\n-#                         skipped_files += 1\n-#                         continue\n-\n-#                     r2_keys.add(key)\n-\n-#                     defaults = {\n-#                         'size': obj['Size'],\n-#                         'last_modified': obj['LastModified'],\n-#                         'content_type': obj.get('ContentType'),\n-#                         'extension': key.split('.')[-1].lower() if '.' in key else '',\n-#                         'url': obj.get('WebsiteRedirectLocation'),\n-#                         'cache_control': obj.get('CacheControl'),\n-#                         'etag': obj.get('ETag'),\n-#                         'is_deleted': False,\n-#                     }\n-\n-#                     bucket_file, created = BucketFile.objects.update_or_create(\n-#                         key=key,\n-#                         defaults=defaults\n-#                     )\n-\n-#                     if created:\n-#                         sync_log.new_files += 1\n-#                     elif any(getattr(bucket_file, k) != v for k, v in defaults.items() if hasattr(bucket_file, k)):\n-#                         sync_log.updated_files += 1\n-#                     else:\n-#                         unchanged_files += 1\n-\n-#             # ‚úÖ Delete all entries from excluded paths in DB\n-#             excluded_deleted = 0\n-#             for path in EXCLUDED_PATHS:\n-#                 deleted_qs = BucketFile.objects.filter(key__startswith=path)\n-#                 count = deleted_qs.count()\n-#                 excluded_deleted += count\n-#                 deleted_qs.delete()\n-\n-#             # Soft-delete remaining files not in R2\n-#             deleted_count = BucketFile.objects.exclude(key__in=r2_keys).update(is_deleted=True)\n-#             sync_log.deleted_files = deleted_count + excluded_deleted\n-#             sync_log.total_files = len(r2_keys)\n-#             sync_log.success = True\n-#             sync_log.completed_at = timezone.now()\n-#             sync_log.save()\n-\n-#             self.stdout.write(self.style.SUCCESS(\n-#                 f\"‚úÖ Synced {sync_log.total_files} files | \"\n-#                 f\"üÜï New: {sync_log.new_files}, \"\n-#                 f\"üõ†Ô∏è Updated: {sync_log.updated_files}, \"\n-#                 f\"üóëÔ∏è Deleted (R2-missing): {deleted_count}, \"\n-#                 f\"üö´ Skipped (excluded): {skipped_files}, \"\n-#                 f\"üßπ Cleaned (excluded from DB): {excluded_deleted}, \"\n-#                 f\"‚úÖ Unchanged: {unchanged_files}\"\n-#             ))\n-\n-#         except Exception as e:\n-#             sync_log.error_message = str(e)\n-#             sync_log.completed_at = timezone.now()\n-#             sync_log.save()\n-#             logger.error(f\"‚ùå Failed to sync R2 bucket: {str(e)}\")\n-#             self.stderr.write(self.style.ERROR(f\"‚ùå Sync failed: {str(e)}\"))\n-\n-\n-import os\n-from mimetypes import guess_type\n-from django.core.management.base import BaseCommand\n-from django.utils import timezone\n-from bucket.models import BucketFile  # change if BucketFile is in another app\n-\n-# Models\n-from accounts.models import CustomUser\n-from collegemanagement.models import College, CollegeGallery\n-from advertisement.models import Advertisement\n-from affiliation.models import Affiliation\n-from certification.models import Certification\n-from coursemanagement.models import Course, CourseCurriculumFile\n-from event.models import EventGallery, Event, EventOrganizer\n-from facilities.models import Facility, CollegeFacility\n-from gallery.models import Gallery\n-from informationmanagement.models import (\n-    InformationGallery, Information, InformationFiles, InformationCategory\n-)\n-from level.models import Level, SubLevel\n-from popup.models import Popup\n-from socialmedia.models import SocialMedia\n-\n-class Command(BaseCommand):\n-    help = \"Extracts file fields from models and stores them in BucketFile table\"\n-\n-    def add_arguments(self, parser):\n-        parser.add_argument(\n-            '--dry-run',\n-            action='store_true',\n-            help='Simulate the extraction without writing to the database',\n-        )\n-\n-    def handle(self, *args, **options):\n-        dry_run = options['dry_run']\n-        total_processed = 0\n-\n-        model_config = [\n-            {\"model\": CustomUser, \"fields\": [\"professional_image\",\"avatar\"]},\n-            {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\",\"brochure\"]},\n-            {\"model\": CollegeGallery, \"fields\": [\"image\"]},\n-            {\"model\": Advertisement, \"fields\": [\"image\"]},\n-            {\"model\": Affiliation, \"fields\": [\"logo_image\",\"cover_image\"]},\n-            {\"model\": Certification, \"fields\": [\"image\"]},\n-            {\"model\": Course, \"fields\": [\"image\"]},\n-            {\"model\": CourseCurriculumFile, \"fields\": [\"curriculum_file_upload\"]},\n-            {\"model\": EventGallery, \"fields\": [\"image\"]},\n-            {\"model\": Event, \"fields\": [\"featured_image\"]},\n-            {\"model\": EventOrganizer, \"fields\": [\"logo\"]},\n-            {\"model\": Facility, \"fields\": [\"icon\"]},\n-            {\"model\": CollegeFacility, \"fields\": [\"image\"]},\n-            {\"model\": Gallery, \"fields\": [\"image\"]},\n-            {\"model\": InformationGallery, \"fields\": [\"file\"]},\n-            {\"model\": Information, \"fields\": [\"cover_image\"]},\n-            {\"model\": InformationFiles, \"fields\": [\"file\"]},\n-            {\"model\": InformationCategory, \"fields\": [\"icon\"]},\n-            {\"model\": Level, \"fields\": [\"icon\"]},\n-            {\"model\": SubLevel, \"fields\": [\"icon\"]},\n-            {\"model\": Popup, \"fields\": [\"image\"]},\n-            {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n-        ]\n-\n-        self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n-\n-        for config in model_config:\n-            model = config[\"model\"]\n-            fields = config[\"fields\"]\n-            model_name = f\"{model._meta.app_label}.{model.__name__}\"\n-            self.stdout.write(f\"üîç Processing model: {model_name}\")\n-\n-            for field_name in fields:\n-                count = 0\n-                for instance in model.objects.iterator():\n-                    file = getattr(instance, field_name, None)\n-                    if file and getattr(file, 'name', None):\n-                        key = file.name\n-                        extension = os.path.splitext(key)[1].lstrip(\".\")\n-                        content_type = guess_type(key)[0] or \"application/octet-stream\"\n-                        size = getattr(file, 'size', 0)\n-\n-                        if dry_run:\n-                            self.stdout.write(f\"    [DRY RUN] Would process file: {key}\")\n-                        else:\n-                            BucketFile.objects.update_or_create(\n-                                key=key,\n-                                defaults={\n-                                    'size': size,\n-                                    'last_modified': timezone.now(),\n-                                    'content_type': content_type,\n-                                    'extension': extension,\n-                                }\n-                            )\n-                            count += 1\n-                            total_processed += 1\n-\n-                self.stdout.write(f\"    ‚úÖ {count} files processed for field: {field_name}\")\n-\n-        self.stdout.write(f\"\\nüéâ File extraction {'simulated' if dry_run else 'completed'}. Total files processed: {total_processed}\")\n"
                },
                {
                    "date": 1746689781910,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,11 +155,10 @@\n             {\"model\": EventGallery, \"fields\": [\"image\"]},\n             {\"model\": Event, \"fields\": [\"featured_image\"]},\n             {\"model\": EventOrganizer, \"fields\": [\"image\"]},\n             {\"model\": Facility, \"fields\": [\"image\"]},\n-            {\"model\": CollegeFacility, \"fields\": [\"image\"]},\n             {\"model\": Gallery, \"fields\": [\"image\"]},\n-            {\"model\": InformationGallery, \"fields\": [\"file\"]},\n+            {\"model\": InformationGallery, \"fields\": [\"image\"]},\n             {\"model\": Information, \"fields\": [\"cover_image\"]},\n             {\"model\": InformationFiles, \"fields\": [\"file\"]},\n             {\"model\": InformationCategory, \"fields\": [\"icon\"]},\n             {\"model\": Level, \"fields\": [\"icon\"]},\n"
                },
                {
                    "date": 1746689808148,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -157,9 +157,9 @@\n             {\"model\": EventOrganizer, \"fields\": [\"image\"]},\n             {\"model\": Facility, \"fields\": [\"image\"]},\n             {\"model\": Gallery, \"fields\": [\"image\"]},\n             {\"model\": InformationGallery, \"fields\": [\"image\"]},\n-            {\"model\": Information, \"fields\": [\"cover_image\"]},\n+            {\"model\": Information, \"fields\": [\"fe\"]},\n             {\"model\": InformationFiles, \"fields\": [\"file\"]},\n             {\"model\": InformationCategory, \"fields\": [\"icon\"]},\n             {\"model\": Level, \"fields\": [\"icon\"]},\n             {\"model\": SubLevel, \"fields\": [\"icon\"]},\n"
                },
                {
                    "date": 1746689829266,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -157,9 +157,9 @@\n             {\"model\": EventOrganizer, \"fields\": [\"image\"]},\n             {\"model\": Facility, \"fields\": [\"image\"]},\n             {\"model\": Gallery, \"fields\": [\"image\"]},\n             {\"model\": InformationGallery, \"fields\": [\"image\"]},\n-            {\"model\": Information, \"fields\": [\"fe\"]},\n+            {\"model\": Information, \"fields\": [\"file\"]},\n             {\"model\": InformationFiles, \"fields\": [\"file\"]},\n             {\"model\": InformationCategory, \"fields\": [\"icon\"]},\n             {\"model\": Level, \"fields\": [\"icon\"]},\n             {\"model\": SubLevel, \"fields\": [\"icon\"]},\n"
                },
                {
                    "date": 1746689834930,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -157,9 +157,9 @@\n             {\"model\": EventOrganizer, \"fields\": [\"image\"]},\n             {\"model\": Facility, \"fields\": [\"image\"]},\n             {\"model\": Gallery, \"fields\": [\"image\"]},\n             {\"model\": InformationGallery, \"fields\": [\"image\"]},\n-            {\"model\": Information, \"fields\": [\"file\"]},\n+            {\"model\": Information, \"fields\": [\"featured_image\"]},\n             {\"model\": InformationFiles, \"fields\": [\"file\"]},\n             {\"model\": InformationCategory, \"fields\": [\"icon\"]},\n             {\"model\": Level, \"fields\": [\"icon\"]},\n             {\"model\": SubLevel, \"fields\": [\"icon\"]},\n"
                },
                {
                    "date": 1746689841616,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -159,9 +159,9 @@\n             {\"model\": Gallery, \"fields\": [\"image\"]},\n             {\"model\": InformationGallery, \"fields\": [\"image\"]},\n             {\"model\": Information, \"fields\": [\"featured_image\"]},\n             {\"model\": InformationFiles, \"fields\": [\"file\"]},\n-            {\"model\": InformationCategory, \"fields\": [\"icon\"]},\n+            {\"model\": InformationCategory, \"fields\": [\"image\"]},\n             {\"model\": Level, \"fields\": [\"icon\"]},\n             {\"model\": SubLevel, \"fields\": [\"icon\"]},\n             {\"model\": Popup, \"fields\": [\"image\"]},\n             {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n"
                },
                {
                    "date": 1746689847557,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,9 @@\n             {\"model\": InformationGallery, \"fields\": [\"image\"]},\n             {\"model\": Information, \"fields\": [\"featured_image\"]},\n             {\"model\": InformationFiles, \"fields\": [\"file\"]},\n             {\"model\": InformationCategory, \"fields\": [\"image\"]},\n-            {\"model\": Level, \"fields\": [\"icon\"]},\n+            {\"model\": Level, \"fields\": [\"image\"]},\n             {\"model\": SubLevel, \"fields\": [\"icon\"]},\n             {\"model\": Popup, \"fields\": [\"image\"]},\n             {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n         ]\n"
                },
                {
                    "date": 1746689864419,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -161,9 +161,9 @@\n             {\"model\": Information, \"fields\": [\"featured_image\"]},\n             {\"model\": InformationFiles, \"fields\": [\"file\"]},\n             {\"model\": InformationCategory, \"fields\": [\"image\"]},\n             {\"model\": Level, \"fields\": [\"image\"]},\n-            {\"model\": SubLevel, \"fields\": [\"icon\"]},\n+            {\"model\": SubLevel, \"fields\": [\"image\"]},\n             {\"model\": Popup, \"fields\": [\"image\"]},\n             {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n         ]\n \n"
                },
                {
                    "date": 1746689873250,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -164,8 +164,10 @@\n             {\"model\": Level, \"fields\": [\"image\"]},\n             {\"model\": SubLevel, \"fields\": [\"image\"]},\n             {\"model\": Popup, \"fields\": [\"image\"]},\n             {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n+            {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n+            \n         ]\n \n         self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n \n"
                },
                {
                    "date": 1746689880312,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -164,9 +164,9 @@\n             {\"model\": Level, \"fields\": [\"image\"]},\n             {\"model\": SubLevel, \"fields\": [\"image\"]},\n             {\"model\": Popup, \"fields\": [\"image\"]},\n             {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n-            {\"model\": SocialMedia, \"fields\": [\"icon\"]},\n+            {\"model\": CollegeSocialMedia, \"fields\": [\"icon\"]},\n             \n         ]\n \n         self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n"
                },
                {
                    "date": 1746689886912,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,9 +126,9 @@\n     InformationGallery, Information, InformationFiles, InformationCategory\n )\n from level.models import Level, SubLevel\n from popup.models import Popup\n-from socialmedia.models import SocialMedia\n+from socialmedia.models import SocialMedia,CollegeSocialMedia\n \n class Command(BaseCommand):\n     help = \"Extracts file fields from models and stores them in BucketFile table\"\n \n"
                },
                {
                    "date": 1746690130791,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -109,9 +109,9 @@\n import os\n from mimetypes import guess_type\n from django.core.management.base import BaseCommand\n from django.utils import timezone\n-from bucket.models import BucketFile  # change if BucketFile is in another app\n+from bucketimages.models import BucketFile  # change if BucketFile is in another app\n \n # Models\n from accounts.models import CustomUser\n from collegemanagement.models import College, CollegeGallery\n"
                },
                {
                    "date": 1746690226139,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -109,10 +109,9 @@\n import os\n from mimetypes import guess_type\n from django.core.management.base import BaseCommand\n from django.utils import timezone\n-from bucketimages.models import BucketFile  # change if BucketFile is in another app\n-\n+from bucketimages.models import BucketFile  \n # Models\n from accounts.models import CustomUser\n from collegemanagement.models import College, CollegeGallery\n from advertisement.models import Advertisement\n@@ -181,27 +180,25 @@\n                 count = 0\n                 for instance in model.objects.iterator():\n                     file = getattr(instance, field_name, None)\n                     if file and getattr(file, 'name', None):\n-                        key = file.name\n-                        extension = os.path.splitext(key)[1].lstrip(\".\")\n-                        content_type = guess_type(key)[0] or \"application/octet-stream\"\n-                        size = getattr(file, 'size', 0)\n+                        content_type = guess_type(file.name)[0] or \"application/octet-stream\"\n \n                         if dry_run:\n-                            self.stdout.write(f\"    [DRY RUN] Would process file: {key}\")\n+                            self.stdout.write(f\"    [DRY RUN] Would process: {file.name}\")\n                         else:\n                             BucketFile.objects.update_or_create(\n-                                key=key,\n+                                file=file,  # ‚úÖ Use `file` instead of `key`\n                                 defaults={\n-                                    'size': size,\n-                                    'last_modified': timezone.now(),\n-                                    'content_type': content_type,\n-                                    'extension': extension,\n+                                    \"content_type\": content_type,\n+                                    \"object_id\": instance.id,\n+                                    # `size` and `url` will be populated by `save()` method\n                                 }\n                             )\n                             count += 1\n                             total_processed += 1\n \n                 self.stdout.write(f\"    ‚úÖ {count} files processed for field: {field_name}\")\n \n-        self.stdout.write(f\"\\nüéâ File extraction {'simulated' if dry_run else 'completed'}. Total files processed: {total_processed}\")\n+        self.stdout.write(\n+            f\"\\nüéâ File extraction {'simulated' if dry_run else 'completed'}. Total files processed: {total_processed}\"\n+        )\n\\ No newline at end of file\n"
                },
                {
                    "date": 1746694338300,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,36 +169,68 @@\n         ]\n \n         self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n \n-        for config in model_config:\n-            model = config[\"model\"]\n-            fields = config[\"fields\"]\n-            model_name = f\"{model._meta.app_label}.{model.__name__}\"\n-            self.stdout.write(f\"üîç Processing model: {model_name}\")\n-\n-            for field_name in fields:\n-                count = 0\n-                for instance in model.objects.iterator():\n-                    file = getattr(instance, field_name, None)\n-                    if file and getattr(file, 'name', None):\n-                        content_type = guess_type(file.name)[0] or \"application/octet-stream\"\n-\n-                        if dry_run:\n-                            self.stdout.write(f\"    [DRY RUN] Would process: {file.name}\")\n-                        else:\n-                            BucketFile.objects.update_or_create(\n-                                file=file,  # ‚úÖ Use `file` instead of `key`\n-                                defaults={\n-                                    \"content_type\": content_type,\n-                                    \"object_id\": instance.id,\n-                                    # `size` and `url` will be populated by `save()` method\n-                                }\n-                            )\n-                            count += 1\n-                            total_processed += 1\n-\n-                self.stdout.write(f\"    ‚úÖ {count} files processed for field: {field_name}\")\n-\n-        self.stdout.write(\n-            f\"\\nüéâ File extraction {'simulated' if dry_run else 'completed'}. Total files processed: {total_processed}\"\n-        )\n\\ No newline at end of file\n+        processed_files = 0\n+    skipped_files = 0\n+    \n+    print(\"Starting file extraction process...\")\n+    \n+    for model_key, field_names in model_config.items():\n+        try:\n+            app_label, model_name = model_key.split('.')\n+            model = apps.get_model(app_label, model_name)\n+            print(f\"\\nProcessing model: {model_key}\")\n+            \n+            if not field_names:\n+                print(f\"Skipping {model_key} - no fields specified\")\n+                continue\n+                \n+            for field_name in field_names:\n+                print(f\"  Checking field: {field_name}\")\n+                field_count = 0\n+                field_skipped = 0\n+                \n+                for instance in model.objects.all():\n+                    try:\n+                        file_field = getattr(instance, field_name, None)\n+                        if not file_field or not file_field.name:\n+                            continue\n+                            \n+                        # Verify file exists before processing\n+                        if not file_field.storage.exists(file_field.name):\n+                            print(f\"    File not found, skipping: {file_field.name}\")\n+                            field_skipped += 1\n+                            skipped_files += 1\n+                            continue\n+                            \n+                        BucketFile.objects.update_or_create(\n+                            file=file_field.name,\n+                            defaults={\n+                                'file': file_field,\n+                                'content_type': getattr(file_field, 'content_type', None),\n+                                'object_id': instance.id,\n+                                'object_type': f\"{app_label}.{model_name}\",\n+                                'field_name': field_name,\n+                            }\n+                        )\n+                        field_count += 1\n+                        processed_files += 1\n+                        \n+                    except Exception as field_error:\n+                        print(f\"    Error processing {field_name} for {model_key} ID {instance.id}: {str(field_error)}\")\n+                        skipped_files += 1\n+                        continue\n+                \n+                print(f\"    Processed {field_count} files, skipped {field_skipped} files for field {field_name}\")\n+                \n+        except Exception as e:\n+            print(f\"Error processing {model_key}: {str(e)}\")\n+            continue\n+    \n+    print(f\"\\nFile extraction complete.\")\n+    print(f\"Total files processed: {processed_files}\")\n+    print(f\"Total files skipped: {skipped_files}\")\n+    return {\n+        'processed': processed_files,\n+        'skipped': skipped_files\n+    }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1746694385657,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -174,9 +174,9 @@\n     skipped_files = 0\n     \n     print(\"Starting file extraction process...\")\n     \n-    for model_key, field_names in model_config.items():\n+    for model_key, field_names in self.model_config.items():\n         try:\n             app_label, model_name = model_key.split('.')\n             model = apps.get_model(app_label, model_name)\n             print(f\"\\nProcessing model: {model_key}\")\n"
                },
                {
                    "date": 1746694403999,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -174,9 +174,9 @@\n     skipped_files = 0\n     \n     print(\"Starting file extraction process...\")\n     \n-    for model_key, field_names in self.model_config.items():\n+    for model_key, field_names in model_config.items():\n         try:\n             app_label, model_name = model_key.split('.')\n             model = apps.get_model(app_label, model_name)\n             print(f\"\\nProcessing model: {model_key}\")\n"
                },
                {
                    "date": 1746694468846,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,9 +169,9 @@\n         ]\n \n         self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n \n-        processed_files = 0\n+    processed_files = 0\n     skipped_files = 0\n     \n     print(\"Starting file extraction process...\")\n     \n"
                },
                {
                    "date": 1746694474052,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,9 +169,9 @@\n         ]\n \n         self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n \n-    processed_files = 0\n+     processed_files = 0\n     skipped_files = 0\n     \n     print(\"Starting file extraction process...\")\n     \n"
                },
                {
                    "date": 1746694495473,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -170,67 +170,67 @@\n \n         self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n \n      processed_files = 0\n-    skipped_files = 0\n-    \n-    print(\"Starting file extraction process...\")\n-    \n-    for model_key, field_names in model_config.items():\n-        try:\n-            app_label, model_name = model_key.split('.')\n-            model = apps.get_model(app_label, model_name)\n-            print(f\"\\nProcessing model: {model_key}\")\n-            \n-            if not field_names:\n-                print(f\"Skipping {model_key} - no fields specified\")\n-                continue\n+        skipped_files = 0\n+        \n+        print(\"Starting file extraction process...\")\n+        \n+        for model_key, field_names in model_config.items():\n+            try:\n+                app_label, model_name = model_key.split('.')\n+                model = apps.get_model(app_label, model_name)\n+                print(f\"\\nProcessing model: {model_key}\")\n                 \n-            for field_name in field_names:\n-                print(f\"  Checking field: {field_name}\")\n-                field_count = 0\n-                field_skipped = 0\n-                \n-                for instance in model.objects.all():\n-                    try:\n-                        file_field = getattr(instance, field_name, None)\n-                        if not file_field or not file_field.name:\n-                            continue\n+                if not field_names:\n+                    print(f\"Skipping {model_key} - no fields specified\")\n+                    continue\n+                    \n+                for field_name in field_names:\n+                    print(f\"  Checking field: {field_name}\")\n+                    field_count = 0\n+                    field_skipped = 0\n+                    \n+                    for instance in model.objects.all():\n+                        try:\n+                            file_field = getattr(instance, field_name, None)\n+                            if not file_field or not file_field.name:\n+                                continue\n+                                \n+                            # Verify file exists before processing\n+                            if not file_field.storage.exists(file_field.name):\n+                                print(f\"    File not found, skipping: {file_field.name}\")\n+                                field_skipped += 1\n+                                skipped_files += 1\n+                                continue\n+                                \n+                            BucketFile.objects.update_or_create(\n+                                file=file_field.name,\n+                                defaults={\n+                                    'file': file_field,\n+                                    'content_type': getattr(file_field, 'content_type', None),\n+                                    'object_id': instance.id,\n+                                    'object_type': f\"{app_label}.{model_name}\",\n+                                    'field_name': field_name,\n\\ No newline at end of file\n+                                }\n+                            )\n+                            field_count += 1\n+                            processed_files += 1\n                             \n-                        # Verify file exists before processing\n-                        if not file_field.storage.exists(file_field.name):\n-                            print(f\"    File not found, skipping: {file_field.name}\")\n-                            field_skipped += 1\n+                        except Exception as field_error:\n+                            print(f\"    Error processing {field_name} for {model_key} ID {instance.id}: {str(field_error)}\")\n                             skipped_files += 1\n                             continue\n-                            \n-                        BucketFile.objects.update_or_create(\n-                            file=file_field.name,\n-                            defaults={\n-                                'file': file_field,\n-                                'content_type': getattr(file_field, 'content_type', None),\n-                                'object_id': instance.id,\n-                                'object_type': f\"{app_label}.{model_name}\",\n-                                'field_name': field_name,\n-                            }\n-                        )\n-                        field_count += 1\n-                        processed_files += 1\n-                        \n-                    except Exception as field_error:\n-                        print(f\"    Error processing {field_name} for {model_key} ID {instance.id}: {str(field_error)}\")\n-                        skipped_files += 1\n-                        continue\n-                \n-                print(f\"    Processed {field_count} files, skipped {field_skipped} files for field {field_name}\")\n-                \n-        except Exception as e:\n-            print(f\"Error processing {model_key}: {str(e)}\")\n-            continue\n-    \n-    print(f\"\\nFile extraction complete.\")\n-    print(f\"Total files processed: {processed_files}\")\n-    print(f\"Total files skipped: {skipped_files}\")\n-    return {\n-        'processed': processed_files,\n-        'skipped': skipped_files\n-    }\n+                    \n+                    print(f\"    Processed {field_count} files, skipped {field_skipped} files for field {field_name}\")\n+                    \n+            except Exception as e:\n+                print(f\"Error processing {model_key}: {str(e)}\")\n+                continue\n+        \n+        print(f\"\\nFile extraction complete.\")\n+        print(f\"Total files processed: {processed_files}\")\n+        print(f\"Total files skipped: {skipped_files}\")\n+        return {\n+            'processed': processed_files,\n+            'skipped': skipped_files\n+        }\n\\ No newline at end of file\n"
                },
                {
                    "date": 1746694500865,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -169,9 +169,9 @@\n         ]\n \n         self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n \n-     processed_files = 0\n+         processed_files = 0\n         skipped_files = 0\n         \n         print(\"Starting file extraction process...\")\n         \n"
                },
                {
                    "date": 1746694564065,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -127,8 +127,12 @@\n from level.models import Level, SubLevel\n from popup.models import Popup\n from socialmedia.models import SocialMedia,CollegeSocialMedia\n \n+from django.db.models import FileField, ImageField\n+from django.apps import apps\n+from bucketimages.models import BucketFile \n+\n class Command(BaseCommand):\n     help = \"Extracts file fields from models and stores them in BucketFile table\"\n \n     def add_arguments(self, parser):\n@@ -169,9 +173,9 @@\n         ]\n \n         self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n \n-         processed_files = 0\n+        processed_files = 0\n         skipped_files = 0\n         \n         print(\"Starting file extraction process...\")\n         \n"
                },
                {
                    "date": 1746694876206,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -178,63 +178,50 @@\n         skipped_files = 0\n         \n         print(\"Starting file extraction process...\")\n         \n-        for model_key, field_names in model_config.items():\n+        for config in model_config:\n+    model = config[\"model\"]\n+    field_names = config[\"fields\"]\n+    model_key = f\"{model._meta.app_label}.{model.__name__}\"\n+\n+    print(f\"\\nüîç Processing model: {model_key}\")\n+\n+    if not field_names:\n+        print(f\"‚ö†Ô∏è Skipping {model_key} - no fields specified\")\n+        continue\n+\n+    for field_name in field_names:\n+        field_count = 0\n+        field_skipped = 0\n+\n+        for instance in model.objects.iterator():  # Use .iterator() for performance\n             try:\n-                app_label, model_name = model_key.split('.')\n-                model = apps.get_model(app_label, model_name)\n-                print(f\"\\nProcessing model: {model_key}\")\n-                \n-                if not field_names:\n-                    print(f\"Skipping {model_key} - no fields specified\")\n+                file_field = getattr(instance, field_name, None)\n+                if not file_field or not file_field.name:\n                     continue\n-                    \n-                for field_name in field_names:\n-                    print(f\"  Checking field: {field_name}\")\n-                    field_count = 0\n-                    field_skipped = 0\n-                    \n-                    for instance in model.objects.all():\n-                        try:\n-                            file_field = getattr(instance, field_name, None)\n-                            if not file_field or not file_field.name:\n-                                continue\n-                                \n-                            # Verify file exists before processing\n-                            if not file_field.storage.exists(file_field.name):\n-                                print(f\"    File not found, skipping: {file_field.name}\")\n-                                field_skipped += 1\n-                                skipped_files += 1\n-                                continue\n-                                \n-                            BucketFile.objects.update_or_create(\n-                                file=file_field.name,\n-                                defaults={\n-                                    'file': file_field,\n-                                    'content_type': getattr(file_field, 'content_type', None),\n-                                    'object_id': instance.id,\n-                                    'object_type': f\"{app_label}.{model_name}\",\n-                                    'field_name': field_name,\n-                                }\n-                            )\n-                            field_count += 1\n-                            processed_files += 1\n-                            \n\\ No newline at end of file\n-                        except Exception as field_error:\n-                            print(f\"    Error processing {field_name} for {model_key} ID {instance.id}: {str(field_error)}\")\n-                            skipped_files += 1\n-                            continue\n-                    \n-                    print(f\"    Processed {field_count} files, skipped {field_skipped} files for field {field_name}\")\n-                    \n-            except Exception as e:\n-                print(f\"Error processing {model_key}: {str(e)}\")\n+\n+                if not file_field.storage.exists(file_field.name):\n+                    print(f\"    ‚ùå File not found: {file_field.name}\")\n+                    field_skipped += 1\n+                    skipped_files += 1\n+                    continue\n+\n+                if not dry_run:\n+                    BucketFile.objects.update_or_create(\n+                        file=file_field.name,\n+                        defaults={\n+                            'file': file_field,\n+                            'content_type': getattr(file_field, 'content_type', None),\n+                            'object_id': instance.id,\n+                        }\n+                    )\n+\n+                field_count += 1\n+                processed_files += 1\n+\n+            except Exception as field_error:\n+                print(f\"    ‚ö†Ô∏è Error in {field_name} for {model_key} ID {instance.id}: {str(field_error)}\")\n+                skipped_files += 1\n                 continue\n-        \n-        print(f\"\\nFile extraction complete.\")\n-        print(f\"Total files processed: {processed_files}\")\n-        print(f\"Total files skipped: {skipped_files}\")\n-        return {\n-            'processed': processed_files,\n-            'skipped': skipped_files\n-        }\n+\n+        print(f\"    ‚úÖ {field_count} files processed, üö´ {field_skipped} skipped for field '{field_name}'\")\n"
                },
                {
                    "date": 1746694882404,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -179,49 +179,49 @@\n         \n         print(\"Starting file extraction process...\")\n         \n         for config in model_config:\n-    model = config[\"model\"]\n-    field_names = config[\"fields\"]\n-    model_key = f\"{model._meta.app_label}.{model.__name__}\"\n+            model = config[\"model\"]\n+            field_names = config[\"fields\"]\n+            model_key = f\"{model._meta.app_label}.{model.__name__}\"\n \n-    print(f\"\\nüîç Processing model: {model_key}\")\n+            print(f\"\\nüîç Processing model: {model_key}\")\n \n-    if not field_names:\n-        print(f\"‚ö†Ô∏è Skipping {model_key} - no fields specified\")\n-        continue\n+            if not field_names:\n+                print(f\"‚ö†Ô∏è Skipping {model_key} - no fields specified\")\n+                continue\n \n-    for field_name in field_names:\n-        field_count = 0\n-        field_skipped = 0\n+            for field_name in field_names:\n+                field_count = 0\n+                field_skipped = 0\n \n-        for instance in model.objects.iterator():  # Use .iterator() for performance\n-            try:\n-                file_field = getattr(instance, field_name, None)\n-                if not file_field or not file_field.name:\n-                    continue\n+                for instance in model.objects.iterator():  # Use .iterator() for performance\n+                    try:\n+                        file_field = getattr(instance, field_name, None)\n+                        if not file_field or not file_field.name:\n+                            continue\n \n-                if not file_field.storage.exists(file_field.name):\n-                    print(f\"    ‚ùå File not found: {file_field.name}\")\n-                    field_skipped += 1\n-                    skipped_files += 1\n-                    continue\n+                        if not file_field.storage.exists(file_field.name):\n+                            print(f\"    ‚ùå File not found: {file_field.name}\")\n+                            field_skipped += 1\n+                            skipped_files += 1\n+                            continue\n \n-                if not dry_run:\n-                    BucketFile.objects.update_or_create(\n-                        file=file_field.name,\n-                        defaults={\n-                            'file': file_field,\n-                            'content_type': getattr(file_field, 'content_type', None),\n-                            'object_id': instance.id,\n-                        }\n-                    )\n+                        if not dry_run:\n+                            BucketFile.objects.update_or_create(\n+                                file=file_field.name,\n+                                defaults={\n+                                    'file': file_field,\n+                                    'content_type': getattr(file_field, 'content_type', None),\n+                                    'object_id': instance.id,\n+                                }\n+                            )\n \n-                field_count += 1\n-                processed_files += 1\n+                        field_count += 1\n+                        processed_files += 1\n \n-            except Exception as field_error:\n-                print(f\"    ‚ö†Ô∏è Error in {field_name} for {model_key} ID {instance.id}: {str(field_error)}\")\n-                skipped_files += 1\n-                continue\n+                    except Exception as field_error:\n+                        print(f\"    ‚ö†Ô∏è Error in {field_name} for {model_key} ID {instance.id}: {str(field_error)}\")\n+                        skipped_files += 1\n+                        continue\n \n-        print(f\"    ‚úÖ {field_count} files processed, üö´ {field_skipped} skipped for field '{field_name}'\")\n+                print(f\"    ‚úÖ {field_count} files processed, üö´ {field_skipped} skipped for field '{field_name}'\")\n"
                },
                {
                    "date": 1746695625863,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -171,57 +171,65 @@\n             {\"model\": CollegeSocialMedia, \"fields\": [\"icon\"]},\n             \n         ]\n \n-        self.stdout.write(\"üöÄ Starting file extraction...\\n\")\n+        self.stdout.write(self.style.SUCCESS(\"üöÄ Starting file extraction process...\"))\n \n-        processed_files = 0\n-        skipped_files = 0\n-        \n-        print(\"Starting file extraction process...\")\n-        \n         for config in model_config:\n             model = config[\"model\"]\n             field_names = config[\"fields\"]\n             model_key = f\"{model._meta.app_label}.{model.__name__}\"\n \n-            print(f\"\\nüîç Processing model: {model_key}\")\n+            self.stdout.write(f\"\\nüîç Processing model: {model_key}\")\n \n-            if not field_names:\n-                print(f\"‚ö†Ô∏è Skipping {model_key} - no fields specified\")\n-                continue\n-\n             for field_name in field_names:\n                 field_count = 0\n                 field_skipped = 0\n+                field_existing = 0\n \n-                for instance in model.objects.iterator():  # Use .iterator() for performance\n+                for instance in model.objects.iterator():\n                     try:\n                         file_field = getattr(instance, field_name, None)\n                         if not file_field or not file_field.name:\n                             continue\n \n                         if not file_field.storage.exists(file_field.name):\n-                            print(f\"    ‚ùå File not found: {file_field.name}\")\n+                            self.stdout.write(f\"    ‚ùå File not found: {file_field.name}\")\n+                            skipped_files += 1\n                             field_skipped += 1\n-                            skipped_files += 1\n                             continue\n \n+                        if BucketFile.objects.filter(file=file_field.name).exists():\n+                            self.stdout.write(f\"    ‚è© Already exists: {file_field.name}\")\n+                            already_existing_files += 1\n+                            field_existing += 1\n+                            continue\n+\n                         if not dry_run:\n-                            BucketFile.objects.update_or_create(\n-                                file=file_field.name,\n-                                defaults={\n-                                    'file': file_field,\n-                                    'content_type': getattr(file_field, 'content_type', None),\n-                                    'object_id': instance.id,\n-                                }\n+                            BucketFile.objects.create(\n+                                file=file_field,\n+                                name=os.path.basename(file_field.name),\n+                                content_type=getattr(file_field, 'content_type', None),\n+                                object_id=instance.id,\n                             )\n \n+                        processed_files += 1\n                         field_count += 1\n-                        processed_files += 1\n \n-                    except Exception as field_error:\n-                        print(f\"    ‚ö†Ô∏è Error in {field_name} for {model_key} ID {instance.id}: {str(field_error)}\")\n+                    except Exception as e:\n+                        self.stdout.write(self.style.WARNING(\n+                            f\"    ‚ö†Ô∏è Error in {field_name} for {model_key} (ID {instance.id}): {e}\"\n+                        ))\n                         skipped_files += 1\n-                        continue\n+                        field_skipped += 1\n \n-                print(f\"    ‚úÖ {field_count} files processed, üö´ {field_skipped} skipped for field '{field_name}'\")\n+                self.stdout.write(\n+                    f\"    ‚úÖ {field_count} processed | \"\n+                    f\"‚è© {field_existing} already existed | \"\n+                    f\"üö´ {field_skipped} skipped for field '{field_name}'\"\n+                )\n+\n+        self.stdout.write(self.style.SUCCESS(\"\\nüéâ Extraction summary:\"))\n+        self.stdout.write(f\"   üÜï New files added     : {processed_files}\")\n+        self.stdout.write(f\"   ‚è© Already existing    : {already_existing_files}\")\n+        self.stdout.write(f\"   üö´ Skipped (missing/failed): {skipped_files}\")\n+        self.stdout.write(f\"   Mode                  : {'DRY RUN' if dry_run else 'LIVE'}\")\n\\ No newline at end of file\n"
                },
                {
                    "date": 1746695951942,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,9 +126,9 @@\n )\n from level.models import Level, SubLevel\n from popup.models import Popup\n from socialmedia.models import SocialMedia,CollegeSocialMedia\n-\n+from mimetypes import guess_type\n from django.db.models import FileField, ImageField\n from django.apps import apps\n from bucketimages.models import BucketFile \n \n"
                },
                {
                    "date": 1746696017161,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -204,15 +204,17 @@\n                             field_existing += 1\n                             continue\n \n                         if not dry_run:\n-                            BucketFile.objects.create(\n-                                file=file_field,\n-                                name=os.path.basename(file_field.name),\n-                                content_type=getattr(file_field, 'content_type', None),\n-                                object_id=instance.id,\n-                            )\n+    content_type = guess_type(file_field.name)[0] or \"application/octet-stream\"\n+    BucketFile.objects.create(\n+        file=file_field,\n+        name=os.path.basename(file_field.name),\n+        content_type=content_type,\n+        object_id=instance.id,\n+    )\n \n+\n                         processed_files += 1\n                         field_count += 1\n \n                     except Exception as e:\n"
                },
                {
                    "date": 1746696024777,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -204,15 +204,15 @@\n                             field_existing += 1\n                             continue\n \n                         if not dry_run:\n-    content_type = guess_type(file_field.name)[0] or \"application/octet-stream\"\n-    BucketFile.objects.create(\n-        file=file_field,\n-        name=os.path.basename(file_field.name),\n-        content_type=content_type,\n-        object_id=instance.id,\n-    )\n+                            content_type = guess_type(file_field.name)[0] or \"application/octet-stream\"\n+                            BucketFile.objects.create(\n+                                file=file_field,\n+                                name=os.path.basename(file_field.name),\n+                                content_type=content_type,\n+                                object_id=instance.id,\n+                            )\n \n \n                         processed_files += 1\n                         field_count += 1\n"
                },
                {
                    "date": 1746696097887,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -144,8 +144,12 @@\n \n     def handle(self, *args, **options):\n         dry_run = options['dry_run']\n         total_processed = 0\n+        # ‚úÖ Initialize counters\n+    processed_files = 0\n+    skipped_files = 0\n+    already_existing_files = 0\n \n         model_config = [\n             {\"model\": CustomUser, \"fields\": [\"professional_image\",\"avatar\"]},\n             {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\",\"brochure\"]},\n"
                },
                {
                    "date": 1746696104817,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,13 +143,12 @@\n         )\n \n     def handle(self, *args, **options):\n         dry_run = options['dry_run']\n-        total_processed = 0\n         # ‚úÖ Initialize counters\n-    processed_files = 0\n-    skipped_files = 0\n-    already_existing_files = 0\n+        processed_files = 0\n+        skipped_files = 0\n+        already_existing_files = 0\n \n         model_config = [\n             {\"model\": CustomUser, \"fields\": [\"professional_image\",\"avatar\"]},\n             {\"model\": College, \"fields\": [\"dp_image\", \"banner_image\",\"brochure\"]},\n"
                },
                {
                    "date": 1746697415931,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -163,9 +163,8 @@\n             {\"model\": EventOrganizer, \"fields\": [\"image\"]},\n             {\"model\": Facility, \"fields\": [\"image\"]},\n             {\"model\": Gallery, \"fields\": [\"image\"]},\n             {\"model\": InformationGallery, \"fields\": [\"image\"]},\n-            {\"model\": Information, \"fields\": [\"featured_image\"]},\n             {\"model\": InformationFiles, \"fields\": [\"file\"]},\n             {\"model\": InformationCategory, \"fields\": [\"image\"]},\n             {\"model\": Level, \"fields\": [\"image\"]},\n             {\"model\": SubLevel, \"fields\": [\"image\"]},\n"
                },
                {
                    "date": 1746697613701,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -158,9 +158,8 @@\n             {\"model\": Certification, \"fields\": [\"image\"]},\n             {\"model\": Course, \"fields\": [\"image\"]},\n             {\"model\": CourseCurriculumFile, \"fields\": [\"curriculum_file_upload\"]},\n             {\"model\": EventGallery, \"fields\": [\"image\"]},\n-            {\"model\": Event, \"fields\": [\"featured_image\"]},\n             {\"model\": EventOrganizer, \"fields\": [\"image\"]},\n             {\"model\": Facility, \"fields\": [\"image\"]},\n             {\"model\": Gallery, \"fields\": [\"image\"]},\n             {\"model\": InformationGallery, \"fields\": [\"image\"]},\n"
                }
            ],
            "date": 1746683926792,
            "name": "Commit-0",
            "content": "# management/commands/sync_r2_bucket.py\nfrom django.core.management.base import BaseCommand\nfrom django.utils import timezone\nfrom ...models import BucketFile, BucketSyncLog\nimport boto3\nfrom django.conf import settings\nfrom datetime import datetime\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass Command(BaseCommand):\n    help = 'Synchronizes the database with the R2 bucket contents'\n    \n    def handle(self, *args, **options):\n        sync_log = BucketSyncLog.objects.create(started_at=timezone.now())\n        \n        try:\n            # Initialize R2 client\n            s3_client = boto3.client(\n                's3',\n                endpoint_url=settings.AWS_S3_ENDPOINT_URL,\n                aws_access_key_id=settings.AWS_ACCESS_KEY_ID,\n                aws_secret_access_key=settings.AWS_SECRET_ACCESS_KEY,\n                region_name=settings.AWS_S3_REGION_NAME\n            )\n            \n            # Get all files from R2\n            paginator = s3_client.get_paginator('list_objects_v2')\n            page_iterator = paginator.paginate(Bucket=settings.AWS_STORAGE_BUCKET_NAME)\n            \n            # Track all keys we find in R2\n            r2_keys = set()\n            \n            # Process each page of results\n            for page in page_iterator:\n                for obj in page.get('Contents', []):\n                    key = obj['Key']\n                    r2_keys.add(key)\n                    \n                    # Update or create the BucketFile record\n                    defaults = {\n                        'size': obj['Size'],\n                        'last_modified': obj['LastModified'],\n                        'content_type': obj.get('ContentType'),\n                        'extension': key.split('.')[-1].lower() if '.' in key else '',\n                        'cache_control': obj.get('CacheControl'),\n                        'etag': obj.get('ETag'),\n                        'is_deleted': False\n                    }\n                    \n                    bucket_file, created = BucketFile.objects.update_or_create(\n                        key=key,\n                        defaults=defaults\n                    )\n                    \n                    if created:\n                        sync_log.new_files += 1\n                    else:\n                        sync_log.updated_files += 1\n            \n            # Mark any files not found in R2 as deleted\n            deleted_count = BucketFile.objects.exclude(key__in=r2_keys)\\\n                                            .update(is_deleted=True)\n            sync_log.deleted_files = deleted_count\n            \n            sync_log.total_files = len(r2_keys)\n            sync_log.success = True\n            sync_log.completed_at = timezone.now()\n            sync_log.save()\n            \n            self.stdout.write(self.style.SUCCESS(\n                f\"Successfully synced {sync_log.total_files} files. \"\n                f\"New: {sync_log.new_files}, Updated: {sync_log.updated_files}, \"\n                f\"Deleted: {sync_log.deleted_files}\"\n            ))\n            \n        except Exception as e:\n            sync_log.error_message = str(e)\n            sync_log.completed_at = timezone.now()\n            sync_log.save()\n            logger.error(f\"Failed to sync R2 bucket: {str(e)}\")\n            self.stdout.write(self.style.ERROR(f\"Sync failed: {str(e)}\"))"
        }
    ]
}